[
  {
    "objectID": "08_qualcoder.html",
    "href": "08_qualcoder.html",
    "title": "QualCoder",
    "section": "",
    "text": "QualCoder is client-based open-source and free multi-platform tool designed to streamline qualitative data analysis. With its user-friendly interface and comprehensive features, QualCoder empowers researchers and analysts to efficiently organize, code, and analyze qualitative data, ranging from text documents to multimedia files. By providing tools for coding, categorizing, and retrieving data, QualCoder facilitates in-depth exploration and interpretation, enabling researchers to uncover meaningful insights and patterns, and extract rich and nuanced information from their qualitative data.\nWith QualCoder you can code text, images, audio and video, write journal notes and memos. You can categorize codes into a tree-like hierarchical categorization scheme. Coding for audio and video requires the VLC media player. Coder comparison reports can be generated for text coding using the Cohen’s Kappa statistic for interrater/intercoder reliability (FIXME: mchugh2012interrater) measuring the level of agreement between coders. A graph displaying codes and categories can be generated to visualize the coding hierarchy. Most reports can be exported as html, open document text (ODT) or as plain text files.\nDocumentation:\n\nWiki - https://github.com/ccbogel/QualCoder/wiki\nGitHub Repository - https://github.com/ccbogel/QualCoder\nGitHub Repository (QualCoder AI - beta) - https://github.com/kaixxx/QualCoder/tree/ai_integration\n\nYou are welcome to follow installation and running instructions for your operating system. Please note, however, this is a multistep process that requires additional components and the use of the command line/terminal. We are available to consult with individuals or groups that require assistance in that process.\nTo save us time, for this workshop we will skip the installation process and we will be supplying a virtual machine to attendees. We will first use the latest available version on the software (currently 3.5) for the exercises, and then, give a sneak peak of the beta version with AI capabilities to test how ChatGPT 4.0 can assist with horizontal coding. On this second half, we will discuss the implications for privacy and also the expected costs associated with the use of AI.\n\n\nTo run QualCoder from the command prompt or terminal use the following\nMac From any directory: qualcoder From the QualCoder-Master directory: python3 -m qualcoder or python3 qualcoder/__main__.py\nWindows From the command prompt: py -m qualcoder\n\n\n\nWe will cover some basic settings, terms (attributes, including how to create a project and how to engage with coding. If you are following this content asynchronously, we strongly advise you to watch the following video for additional features:\n\nOther helpful tutorials can be found here: https://github.com/ccbogel/QualCoder/wiki/15-QualCoder-videos\n\n\nIn QualCoder, projects serve as containers for organizing multiple files, cases, and attributes. Each project allows you to manage and analyze a specific set of qualitative data, keeping everything organized. You can create different projects for distinct research questions or datasets, enabling a streamlined workflow. This structure ultimately helps you delve deeper into your analysis without mixing different data sets.\nUnderstanding the roles of files, attributes and cases and their relationships is key to most QDA software. In QualCoder, files refer to the individual documents or data sources you import for analysis. You may have more than one file associate with a unique case. Cases are specific units of analysis, often linked to files, that allow you to organize and explore your data in a more detailed manner. Together, they create a structured environment for qualitative analysis, allowing you to delve into themes and patterns effectively. In our project example, the cases will consist of individual interviewees, but we could also focus on the niche they belong to if we were to analyze at that level.\nFor example, let’s imagine that Sarah also collected social media video posts from content creators and digital influencers to analyze behaviors beyond participants’ self-reports, by also checking how they have been engaging with their followers. In this case, she can add video clips to QualCoder and associate them with the same research participant. This structure will depend on the nature of your project, allowing for flexibility in how data is organized and analyzed to best capture the insights you seek.\nAttributes are metadata or characteristics that can be assigned to files and cases, helping to categorize or describe them. Examples related to our project example could be demographic pieces of information (e.g., gender, age group), years active, niche, total followers assigned to individuals represented as cases, or characteristics attributed to a file such as the type of source (e.g., interview, social media), total numbers of views, shares and comments, platform (e.g., IG, Youtube, TikTok). You may take advantage of memos to record more detailed information about attributes. Think of it an essential aid to documentation that will provide better contextualization so researchers, including your future self, can better interpret and understand decisions made during the research process.\nAs a heads up, QualCoder was not designed having statistical analysis of attributes in mind, for that, you can export attributes as csv files and use R or other open dedicated application. Also, the only current options for attributes are numeric and character, so dates, types or integers are not supported. To export attributes, use the icon below or\n\nYou can utilize Journals to practice data journaling anddocument ideas and reflections throughout the coding process, making them searchable with regex expressions—a topic we’ll explore later. Journals are particularly useful for a bottom-up approaches, requiring multiple iterations to refine your themes and coding scheme. Plus, you can enhance your journaling content with markdown formatting!\n\n\n\nNow that we are more familiar with QualCoder’s interface, some key functions and the mechanics of setting up a project, let’s bring Sarah’s data into QualCoder and create a project. We will then, walk her through the process of coding her interview transcripts using some recommended practices.\n\n\nProject &gt; Create New Project\nSelect the directory you want to save your qda project and name it Influencers.\n\n\n\n\nWe’ve created a project, but we don’t have any files yet, so let’s load them now.\n\nRecommended/Cited Sources:",
    "crumbs": [
      "Analyzing & Documenting",
      "Qualcoder Software"
    ]
  },
  {
    "objectID": "08_qualcoder.html#what-is-qualcoder",
    "href": "08_qualcoder.html#what-is-qualcoder",
    "title": "QualCoder",
    "section": "",
    "text": "QualCoder is client-based open-source and free multi-platform tool designed to streamline qualitative data analysis. With its user-friendly interface and comprehensive features, QualCoder empowers researchers and analysts to efficiently organize, code, and analyze qualitative data, ranging from text documents to multimedia files. By providing tools for coding, categorizing, and retrieving data, QualCoder facilitates in-depth exploration and interpretation, enabling researchers to uncover meaningful insights and patterns, and extract rich and nuanced information from their qualitative data.\nWith QualCoder you can code text, images, audio and video, write journal notes and memos. You can categorize codes into a tree-like hierarchical categorization scheme. Coding for audio and video requires the VLC media player. Coder comparison reports can be generated for text coding using the Cohen’s Kappa statistic for interrater/intercoder reliability (FIXME: mchugh2012interrater) measuring the level of agreement between coders. A graph displaying codes and categories can be generated to visualize the coding hierarchy. Most reports can be exported as html, open document text (ODT) or as plain text files.\nDocumentation:\n\nWiki - https://github.com/ccbogel/QualCoder/wiki\nGitHub Repository - https://github.com/ccbogel/QualCoder\nGitHub Repository (QualCoder AI - beta) - https://github.com/kaixxx/QualCoder/tree/ai_integration\n\nYou are welcome to follow installation and running instructions for your operating system. Please note, however, this is a multistep process that requires additional components and the use of the command line/terminal. We are available to consult with individuals or groups that require assistance in that process.\nTo save us time, for this workshop we will skip the installation process and we will be supplying a virtual machine to attendees. We will first use the latest available version on the software (currently 3.5) for the exercises, and then, give a sneak peak of the beta version with AI capabilities to test how ChatGPT 4.0 can assist with horizontal coding. On this second half, we will discuss the implications for privacy and also the expected costs associated with the use of AI.\n\n\nTo run QualCoder from the command prompt or terminal use the following\nMac From any directory: qualcoder From the QualCoder-Master directory: python3 -m qualcoder or python3 qualcoder/__main__.py\nWindows From the command prompt: py -m qualcoder\n\n\n\nWe will cover some basic settings, terms (attributes, including how to create a project and how to engage with coding. If you are following this content asynchronously, we strongly advise you to watch the following video for additional features:\n\nOther helpful tutorials can be found here: https://github.com/ccbogel/QualCoder/wiki/15-QualCoder-videos\n\n\nIn QualCoder, projects serve as containers for organizing multiple files, cases, and attributes. Each project allows you to manage and analyze a specific set of qualitative data, keeping everything organized. You can create different projects for distinct research questions or datasets, enabling a streamlined workflow. This structure ultimately helps you delve deeper into your analysis without mixing different data sets.\nUnderstanding the roles of files, attributes and cases and their relationships is key to most QDA software. In QualCoder, files refer to the individual documents or data sources you import for analysis. You may have more than one file associate with a unique case. Cases are specific units of analysis, often linked to files, that allow you to organize and explore your data in a more detailed manner. Together, they create a structured environment for qualitative analysis, allowing you to delve into themes and patterns effectively. In our project example, the cases will consist of individual interviewees, but we could also focus on the niche they belong to if we were to analyze at that level.\nFor example, let’s imagine that Sarah also collected social media video posts from content creators and digital influencers to analyze behaviors beyond participants’ self-reports, by also checking how they have been engaging with their followers. In this case, she can add video clips to QualCoder and associate them with the same research participant. This structure will depend on the nature of your project, allowing for flexibility in how data is organized and analyzed to best capture the insights you seek.\nAttributes are metadata or characteristics that can be assigned to files and cases, helping to categorize or describe them. Examples related to our project example could be demographic pieces of information (e.g., gender, age group), years active, niche, total followers assigned to individuals represented as cases, or characteristics attributed to a file such as the type of source (e.g., interview, social media), total numbers of views, shares and comments, platform (e.g., IG, Youtube, TikTok). You may take advantage of memos to record more detailed information about attributes. Think of it an essential aid to documentation that will provide better contextualization so researchers, including your future self, can better interpret and understand decisions made during the research process.\nAs a heads up, QualCoder was not designed having statistical analysis of attributes in mind, for that, you can export attributes as csv files and use R or other open dedicated application. Also, the only current options for attributes are numeric and character, so dates, types or integers are not supported. To export attributes, use the icon below or\n\nYou can utilize Journals to practice data journaling anddocument ideas and reflections throughout the coding process, making them searchable with regex expressions—a topic we’ll explore later. Journals are particularly useful for a bottom-up approaches, requiring multiple iterations to refine your themes and coding scheme. Plus, you can enhance your journaling content with markdown formatting!\n\n\n\nNow that we are more familiar with QualCoder’s interface, some key functions and the mechanics of setting up a project, let’s bring Sarah’s data into QualCoder and create a project. We will then, walk her through the process of coding her interview transcripts using some recommended practices.\n\n\nProject &gt; Create New Project\nSelect the directory you want to save your qda project and name it Influencers.\n\n\n\n\nWe’ve created a project, but we don’t have any files yet, so let’s load them now.\n\nRecommended/Cited Sources:",
    "crumbs": [
      "Analyzing & Documenting",
      "Qualcoder Software"
    ]
  },
  {
    "objectID": "01_qual-data.html",
    "href": "01_qual-data.html",
    "title": "Qualitative Data",
    "section": "",
    "text": "Quantiative vs. Qualitative Data\nQualitative data refers to any non-numeric forms of data that are typically textual, audio, images, or video recordings, which can be transcribed or described. This type of data includes field notes, interviews, social media content (e.g., posts, comments), archival documents, and more.\nQualitative data delves into the “why” and “how” of research questions, aiming to understand underlying motivations, behaviors, and experiences. Considering their It can complement quantitative data in mixed methods studies by providing deeper insights, context, and understanding of complex research phenomena.\nDespite of its advantages, handling and analyzing qualitative data requires specialized skills, tools, and techniques to overcome the following challenges:\nQualitative researchers should be aware of the above limitations and plan accordingly in order to identify ways to improve the transparency and reusability of their work.",
    "crumbs": [
      "Planning & Collecting",
      "Qualitative Data"
    ]
  },
  {
    "objectID": "01_qual-data.html#scientific-rigor-in-qualitative-research",
    "href": "01_qual-data.html#scientific-rigor-in-qualitative-research",
    "title": "Qualitative Data",
    "section": "Scientific Rigor in Qualitative Research",
    "text": "Scientific Rigor in Qualitative Research\nScientific rigor manifests differently in qualitative research. Unlike quantitative research, which produces numerical results, qualitative research yields findings. These findings consist of insights, themes, and patterns that emerge from analyzing non-numerical. They offer a descriptive and interpretive understanding of the studied phenomenon and play a crucial role in theory-building and contextual understanding, laying the groundwork for exploring phenomena in depth and inform practical applications.\nA key challenge in qualitative research is establishing the credibility of these findings, as they lack the objective metrics used in quantitative research, such as validity and reliability checks. Also, instead of aiming for generalizability to a broader population, qualitative research provides in-depth insights into specific contexts or groups, enhancing our understanding of the subject matter within those contexts.\nQualitative researchers use the concept of “trustworthiness” as a foundational scientific rigor principle, which is built on four key pillars:\n\nCredibility: the confidence in the truthfulness of the findings.\nTransferability: the extent to which the findings are applicable to other contexts.\nDependability: the consistency of the findings and their potential for replication.\nConfirmability: the degree to which the findings are shaped by the respondents rather than influenced by researcher bias or personal interests.\n\nLater in this course, we’ll cover some practical recommendations to increase the trustworthiness of your qualitative research findings.",
    "crumbs": [
      "Planning & Collecting",
      "Qualitative Data"
    ]
  },
  {
    "objectID": "01_qual-data.html#data-types-in-qualitative-research",
    "href": "01_qual-data.html#data-types-in-qualitative-research",
    "title": "Qualitative Data",
    "section": "Data Types in Qualitative Research",
    "text": "Data Types in Qualitative Research\n\n\n\n\n\nQualitative data includes a diverse array of sources, ranging from those obtained through direct interaction with human participants to those generated independently of a research protocol. Although not originally intended for research, some data can still be analyzed and explored through a research lens. Think about a sociologist or communication researcher who analyzes social media posts to study the public discourse on a particular topic.\nQualitative data types include written text, recordings and their transcriptions, and imagery found in diaries, personal notes, letters, emails, news articles, social media platforms, official documents, policies, medical records, meeting minutes, and other organizational documents and much more. These sources are analyzed to gain insights into attitudes, behaviors, and cultural norms in a variety of topics.\nQualitative data also includes the analysis of literature, film, music, and artifacts—whether produced, valued, accumulated, or possessed by research participants or their communities. These artifacts, such as products, remains, artwork, photographs, or prototypes, can reveal both explicit and implicit (or overt and covert) values, attitudes, beliefs, and experiences that are crucial for understanding the phenomenon under study.\n\n\n\n\n\n\n💭 Discussion: Can you think of any examples where an everyday artifact can be used in research to provide valuable insights about a specific group?\n\n\n\n\n\nA collection of protest signs from a social movement could be one. These signs might initially appear to be simple expressions of demands or slogans. However, analyzing the language, imagery, and symbolism used can uncover underlying values, emotions, and the collective identity of the movement. For instance, the choice of words, the use of humor or anger, and the symbols depicted can reveal insights into the group’s attitudes towards authority, their sense of justice, and the cultural or references that have historically resonate with them. This deeper analysis helps in understanding the broader social and political context in which the movement operates.\n\n\n\nQualitative data can be collected through various research methods. Defining the best approach to obtain qualitative data depends on several factors, including the research question(s), the dispersion of research subjects, the nature of the data needed, and the context of the study. Below, are the most common methods:\n\nDocument analyses: Examination of written or recorded materials to gain insights into past events, attitudes, and experiences.\nOpen-ended digital surveys: Online questionnaires with open-ended questions to collect qualitative data at scale to capture participants’ opinions, experiences, and suggestions.\nFocus Groups: Moderated discussions among a small group of participants representing the targeted population or the community of interest, allowing researchers to observe interactions and gather collective insights on a specific topic.\nObservations: Close monitoring and recording of behaviors, interactions, and phenomena in natural physical or online setting and the context in which those are generated. Observations might be naturalistic - when researchers observe people in their natural setting without interference -, or participatory - when the investigator becomes an active member of the group being observed.\nField Work: Researchers produce annotations based on their observations, reflections, and impressions during fieldwork, capturing nuances and contextual details that may not be captured through other methods.\nCase Studies: In-depth examination of a single individual, group, or organization, using multiple data sources to explore complex phenomena within real-life contexts.\nEthnography: Immersive, long-term engagement with a particular community or culture, aiming to understand its social dynamics, rituals, and practices through participant observation and interaction. It may also be conducted online to explore communities and cultures formed and active through computer-mediated social interaction.\nOne-to-one Interviews: Direct conversations between the researcher and participants, allowing for in-depth exploration of thoughts, experiences, and perspectives. Interviews can be structured, semi-structured or unstructured and be conducted utilizing various means (phone, online, in-person).\n\nThis course will be using qualitative research data obtained from direct interactions between researchers and human participants through interviews.\nWhen dealing with human subjects data, ethical considerations become significantly more complex due to the direct impact on individuals’ privacy, rights, and well-being. In the next episode, we will cover ethical considerations and additional steps researchers should take when conducting research with human participants.\n\nRecommended/Cited Sources:\nCreswell, J. W., & Creswell, J. W. (2007). Qualitative inquiry & research design: choosing among five approaches (Second edition.). Sage Publications. (UCSB Library Catalog)",
    "crumbs": [
      "Planning & Collecting",
      "Qualitative Data"
    ]
  },
  {
    "objectID": "01_dmp.html",
    "href": "01_dmp.html",
    "title": "Data Management Planning",
    "section": "",
    "text": "A Data Management Plan (DMP) is a strategic document that outlines how you will handle data throughout the life cycle of a project. It serves as a roadmap for managing, storing, and sharing data effectively, ensuring that your data is well-organized, accessible, and secure. By documenting your data management strategies, a DMP helps streamline data workflows, facilitates compliance with institutional and funding agency requirements, and enhances the efficiency and integrity of research.\nThe importance of a DMP can be highlighted in several ways:\n\nOrganization and efficiency: A well-structured DMP ensures that data is collected, stored, and analyzed in a consistent and organized manner. This reduces the risk of data loss, duplication, or mismanagement, and helps team members quickly find and use the data they need.\nResource allocation: The plan helps allocate resources effectively, including budgeting for data management tools and personnel. This foresight can prevent unexpected costs and ensure that data management tasks are adequately resourced.\nData security and privacy: The DMP outlines measures for safeguarding sensitive or confidential data, ensuring compliance with privacy regulations and ethical guidelines. This includes specifying access controls, encryption methods, and protocols for data anonymization.\nData sharing and reproducibility: By detailing how data will be shared and made available, the DMP supports transparency and collaboration. This includes specifying data formats, metadata standards, and repositories for public or restricted access, which enhances the reproducibility of research findings.\nLong-Term preservation: The DMP addresses strategies for long-term data preservation, including backup procedures, archival formats, and storage solutions. This ensures that valuable data remains accessible and usable beyond the immediate project duration.\n\nMost funders require researchers to submit a Data Management Plan (DMP) along with their research proposal as a prerequisite for consideration. However, developing a DMP offers substantial benefits beyond meeting this requirement. It serves as a crucial tool for all researchers, aiding them in anticipating resource needs, exploring available services, and strategically planning for data management throughout the project’s life cycle. For example, a researcher planning to conduct interviews can use the DMP to identify and utilize institutionally-supported transcription services and secure storage solutions.\nBy anticipating these needs early on, the researcher can allocate appropriate budget and resources, ensuring that these essential services are available when required. Additionally, the DMP helps the researcher create a structured plan for managing qualitative data, such as organizing interview transcripts, coding data, and maintaining a detailed audit trail of data analysis decisions.\nBroadly, the DMP should cover the following topics:\n\nThe types of data you expect to collect,\nHow those data will be documented and organized,\nHow the data will be stored and kept secure, and\nHow the data will be shared (or why not) and stored for the long term.\n\nThe ultimate goal is that researchers will make more informed decisions on how to produce and share data satisfying the FAIR principles:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/CT8P-D5MK).\nResearchers studying indigenous communities should pay special attention to an additional set of principles, named CARE, an acronym for a set of purpose-oriented principles for Indigenous Data Governance, which aims to help advancing Indigenous innovation, sovereignty, and self-determination.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/3ZHR-6JAG).",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "01_dmp.html#what-are-data-management-plans-dmps-and-why-should-you-care",
    "href": "01_dmp.html#what-are-data-management-plans-dmps-and-why-should-you-care",
    "title": "Data Management Planning",
    "section": "",
    "text": "A Data Management Plan (DMP) is a strategic document that outlines how you will handle data throughout the life cycle of a project. It serves as a roadmap for managing, storing, and sharing data effectively, ensuring that your data is well-organized, accessible, and secure. By documenting your data management strategies, a DMP helps streamline data workflows, facilitates compliance with institutional and funding agency requirements, and enhances the efficiency and integrity of research.\nThe importance of a DMP can be highlighted in several ways:\n\nOrganization and efficiency: A well-structured DMP ensures that data is collected, stored, and analyzed in a consistent and organized manner. This reduces the risk of data loss, duplication, or mismanagement, and helps team members quickly find and use the data they need.\nResource allocation: The plan helps allocate resources effectively, including budgeting for data management tools and personnel. This foresight can prevent unexpected costs and ensure that data management tasks are adequately resourced.\nData security and privacy: The DMP outlines measures for safeguarding sensitive or confidential data, ensuring compliance with privacy regulations and ethical guidelines. This includes specifying access controls, encryption methods, and protocols for data anonymization.\nData sharing and reproducibility: By detailing how data will be shared and made available, the DMP supports transparency and collaboration. This includes specifying data formats, metadata standards, and repositories for public or restricted access, which enhances the reproducibility of research findings.\nLong-Term preservation: The DMP addresses strategies for long-term data preservation, including backup procedures, archival formats, and storage solutions. This ensures that valuable data remains accessible and usable beyond the immediate project duration.\n\nMost funders require researchers to submit a Data Management Plan (DMP) along with their research proposal as a prerequisite for consideration. However, developing a DMP offers substantial benefits beyond meeting this requirement. It serves as a crucial tool for all researchers, aiding them in anticipating resource needs, exploring available services, and strategically planning for data management throughout the project’s life cycle. For example, a researcher planning to conduct interviews can use the DMP to identify and utilize institutionally-supported transcription services and secure storage solutions.\nBy anticipating these needs early on, the researcher can allocate appropriate budget and resources, ensuring that these essential services are available when required. Additionally, the DMP helps the researcher create a structured plan for managing qualitative data, such as organizing interview transcripts, coding data, and maintaining a detailed audit trail of data analysis decisions.\nBroadly, the DMP should cover the following topics:\n\nThe types of data you expect to collect,\nHow those data will be documented and organized,\nHow the data will be stored and kept secure, and\nHow the data will be shared (or why not) and stored for the long term.\n\nThe ultimate goal is that researchers will make more informed decisions on how to produce and share data satisfying the FAIR principles:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/CT8P-D5MK).\nResearchers studying indigenous communities should pay special attention to an additional set of principles, named CARE, an acronym for a set of purpose-oriented principles for Indigenous Data Governance, which aims to help advancing Indigenous innovation, sovereignty, and self-determination.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/3ZHR-6JAG).",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "01_dmp.html#crafting-a-dmp-for-qualitative-research",
    "href": "01_dmp.html#crafting-a-dmp-for-qualitative-research",
    "title": "Data Management Planning",
    "section": "Crafting a DMP for Qualitative Research",
    "text": "Crafting a DMP for Qualitative Research\nFortunately, creating a DMP is simplified with the help of appropriate templates.The DMPTool is an excellent resource for researchers to develop customized data management plans tailored to specific disciplines and funding agencies. The handout below outlines is more information about it:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/3HFE-6X7U).\nTo get started with the DMPTool, follow these steps:\n\nVisit DMPTool in your web browser.\nOn the DMPTool homepage, enter your institutional email address on the right side and click “Continue.”\nThe tool will automatically recognize your affiliation. Click the “Sign in with Institution (SSO)” button to be redirected to the EID sign-in page and complete the sign-in process.\n\nThe DMPTool offers a wide range of templates tailored to specific funder requirements and assists you in structuring your Data Management Plan (DMP) according to these templates. It guides you through the process with targeted questions, ensuring that all necessary components are addressed. Once completed, the tool generates a well-organized and professionally formatted DMP.\nThe Qualitative Data Repository (QDR) has developed a specialized data management checklist for qualitative researchers to ensure that their Data Management Plans (DMPs) address all critical aspects. The checklist includes topic-specific examples from qualitative research where applicable, and offers practical tips based on our extensive experience advising researchers on their DMPs. You can use this checklist proactively as a guide while drafting your DMP, or as a tool to review and confirm that an existing DMP comprehensively covers all necessary elements. Using the DMPTool in tandem with QDR’s checklist can provide additional clarity and help you navigate the various topics effectively. This combination ensures that your DMP is comprehensive, meets all requirements, and is easy to understand.\nFor future reference, you may find valuable insights by consulting the list of the 11 winning DMPs from the DMPTool Qualitative Data Competition. These exemplary DMPs showcase best practices and innovative approaches in managing qualitative data, offering a useful benchmark for developing your own plan.",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "01_dmp.html#update-as-you-progress",
    "href": "01_dmp.html#update-as-you-progress",
    "title": "Data Management Planning",
    "section": "Update as You Progress",
    "text": "Update as You Progress\nDMPs aren’t meant to collect dust; they should be treated as living documents that evolve alongside your project. To ensure they stay relevant and accurate, it’s crucial to implement version control. Regularly update the DMP to reflect any changes in your project, and track these revisions to maintain a clear history of modifications. This way, you can easily reference past versions, understand the evolution of your project, and ensure that everyone involved is working with the most current information.",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "01_dmp.html#ethical-considerations-are-paramount-in-any-research-but-they-take-on-added-significance-in-qualitative-research-where-interactions-and-data-collection-are-often-more-personal-and-complex.-its-vital-to-address-these-considerations-early-in-the-research-process-and-incorporate-them-into-the-data-management-plan-dmp.-for-qualitative-research-the-dmp-should-be-particularly-detailed-in-outlining-these-ethical-aspects.-this-includes-obtaining-informed-consent-from-participants-ensuring-confidentiality-and-privacy-managing-potential-biases-and-being-transparent-about-how-data-will-be-used.-by-embedding-these-considerations-into-a-well-defined-dmp-you-create-a-framework-that-not-only-supports-responsible-and-respectful-research-practices-but-also-anticipates-the-resources-and-services-needed-for-compliance.-in-the-next-episode-we-will-explore-the-ethical-dimensions-of-qualitative-research-in-greater-detail.",
    "href": "01_dmp.html#ethical-considerations-are-paramount-in-any-research-but-they-take-on-added-significance-in-qualitative-research-where-interactions-and-data-collection-are-often-more-personal-and-complex.-its-vital-to-address-these-considerations-early-in-the-research-process-and-incorporate-them-into-the-data-management-plan-dmp.-for-qualitative-research-the-dmp-should-be-particularly-detailed-in-outlining-these-ethical-aspects.-this-includes-obtaining-informed-consent-from-participants-ensuring-confidentiality-and-privacy-managing-potential-biases-and-being-transparent-about-how-data-will-be-used.-by-embedding-these-considerations-into-a-well-defined-dmp-you-create-a-framework-that-not-only-supports-responsible-and-respectful-research-practices-but-also-anticipates-the-resources-and-services-needed-for-compliance.-in-the-next-episode-we-will-explore-the-ethical-dimensions-of-qualitative-research-in-greater-detail.",
    "title": "Data Management Planning",
    "section": "Ethical considerations are paramount in any research, but they take on added significance in qualitative research, where interactions and data collection are often more personal and complex. It’s vital to address these considerations early in the research process and incorporate them into the Data Management Plan (DMP). For qualitative research, the DMP should be particularly detailed in outlining these ethical aspects. This includes obtaining informed consent from participants, ensuring confidentiality and privacy, managing potential biases, and being transparent about how data will be used. By embedding these considerations into a well-defined DMP, you create a framework that not only supports responsible and respectful research practices but also anticipates the resources and services needed for compliance. In the next episode, we will explore the ethical dimensions of qualitative research in greater detail.",
    "text": "Ethical considerations are paramount in any research, but they take on added significance in qualitative research, where interactions and data collection are often more personal and complex. It’s vital to address these considerations early in the research process and incorporate them into the Data Management Plan (DMP). For qualitative research, the DMP should be particularly detailed in outlining these ethical aspects. This includes obtaining informed consent from participants, ensuring confidentiality and privacy, managing potential biases, and being transparent about how data will be used. By embedding these considerations into a well-defined DMP, you create a framework that not only supports responsible and respectful research practices but also anticipates the resources and services needed for compliance. In the next episode, we will explore the ethical dimensions of qualitative research in greater detail.\nRecommended/Cited Sources:\nQualitative Data Repository (2017). Data Management Checklist. https://qdr.syr.edu/drupal_data/public/QDR%20-%20Data%20Management%20Checklist.pdf",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "05_deidentification.html",
    "href": "05_deidentification.html",
    "title": "Data De-identification",
    "section": "",
    "text": "Disclosing sensitive information without proper safeguards can lead to significant harm and jeopardize not only the well-being of participants but also the integrity and trustworthiness of the research process.\nThe richness of the data obtained through interviews presents additional ethical and analytical challenges, particularly regarding the need for more nuanced and intricate analysis, concerning the protection of participant’ identity, and the mitigation of the risk of re-identification.\nQualitative data can be more difficult to de-identify compared to quantitative data due to its typically unstructured nature. Another challenge is that qualitative data includes unique personal narratives and contextual details that can inadvertently reveal identities, requiring careful and nuanced de-identification strategies to protect participants’ privacy effectively. The process of de-identification or anonymization can also alter or diminish data value, especially when the significance lies in capturing personal experiences or narratives. Researchers should consider using de-identification alongside other methods, such as limiting access to the data or obtaining explicit consent from participants to share some or all of their personal information. Because extensive de-identification might sometimes obscure important details needed for deep analysis, striking a balance between removing identifiers and preserving the essential context of the qualitative data is key.\nIn a nutshell, data de-identification entails the process of removing direct and indirect identifiers from a dataset, while maintaining enough information to preserve its value and usability to future research. In this episode, we cover common strategies for de-identifying interview data and recommendations for handling sensitive information.",
    "crumbs": [
      "Analyzing & Documenting",
      "Data De-identification"
    ]
  },
  {
    "objectID": "05_deidentification.html#direct-vs-indirect-identifiers",
    "href": "05_deidentification.html#direct-vs-indirect-identifiers",
    "title": "Data De-identification",
    "section": "Direct vs Indirect Identifiers",
    "text": "Direct vs Indirect Identifiers\nA person’s identity can be disclosed from direct identifiers, unique to an individual or indirect identifiers which, when linked with other available information, could identify someone.\nDirect identifiers are pieces of information that can immediately identify an individual on their own, such as a social security number, full name, email address, home address, or phone number. In contrast, indirect identifiers are data points that do not uniquely identify someone by themselves but can do so when combined with other information. Examples of indirect identifiers include job title, gender, and ethnicity. The risk associated with indirect identifiers can vary depending on the context and the availability of additional data. Therefore, it is crucial to understand which data points are relevant within the context of the study and to consider both direct and indirect identifiers to effectively protect privacy and minimize the risk of identification.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/LM2L-K8DN).\nInitially, researchers might change names and disguise locations, but effectively managing identifying details in qualitative data requires a nuanced approach. Anonymity and privacy exist on a spectrum, so researchers must balance the risk of identification with the needs of their research.\nWhen data isn’t fully anonymised, including cases of potential indirect identification, obtaining explicit consent is essential before public release. This consent should be secured either at the time of data collection or prior to publication.\nOne significant challenge arises with indirect identification, particularly in small groups where participants know each other. This issue can be exacerbated by existing power dynamics, such as those between a team and its leader. Therefore, researchers must transparently communicate the risks of identity disclosure to participants and respect their wishes.\nTo mitigate these risks, techniques such as using multiple pseudonyms, rephrasing quotes, and breaking known links can be effective. These methods help protect participant identities while allowing for restricted sharing of the dataset.\nHowever, there may be situations where these strategies are not feasible. In such cases, researchers need to carefully consider how to obtain consent for disclosing an individual’s identity without inadvertently revealing the identities of others. Ultimately, the outcomes of these processes should be documented in the data management plan and ethics application, ensuring that all considerations are addressed.\n\n\n\n\n\n\n💭 Discussion: What could be potential direct and indirect identifiers in the context of Sarah’s research?\n\n\n\n\n\nAnswer: Social media usernames or handles are considered direct identifiers, while patterns in use of custom tags and symbols, bios and profile descriptions and catchphrases combined might direct identify users, especially in niche activities.\nConsider the infamous Baby Reindeer Netflix series lawsuit, in which a simple keyword search on Twitter swiftly uncovered the identity of the real person behind the pseudonym used in the series.",
    "crumbs": [
      "Analyzing & Documenting",
      "Data De-identification"
    ]
  },
  {
    "objectID": "05_deidentification.html#data-de-identification-essentials",
    "href": "05_deidentification.html#data-de-identification-essentials",
    "title": "Data De-identification",
    "section": "Data De-identification Essentials",
    "text": "Data De-identification Essentials\nThe existing literature provides limited guidance on de-identifying qualitative data, with most researchers relying on ad hoc strategies. Perhaps the most robust and up-to-date methodological framework for handling sensitive narrative data is found in Campbell and co-authors (2023) multiphased process inspired by common qualitative coding techniques.\nIn the first phase, the process involves consultations with a range of stakeholders and subject-matter experts to identify risks related to re-identifiability and concerns about data sharing. The second phase outlines an iterative approach to identifying potentially identifiable information and developing tailored remediation strategies through group review and consensus. The final phase includes multiple methods for evaluating the effectiveness of the de-identification efforts, ensuring that the remediated transcripts adequately protect participants’ privacy. If your project involves working with vulnerable or protected groups, or handling sensitive data, we strongly recommend reviewing and applying this framework to your research.\n\n\n\nQualitative Data De-identification Overview. Source: Campbell et al. (2023)\n\n\nThese three phases can be broken into tasks, actions and examples:\n\n\n\n\n\n\n\n\n\nPhase goal\nTasks\nActions\nExamples\n\n\n\n\nPhase 1:\nDevelop a process to distinguish potentially identifiable data\nCreate a coding framework\nConsult with stakeholders and look for strategies followed by similar projects\nRegulatory guidance (HIPPAA, IRB, relevant professional and research associations)\n\n\n\n\n\nSubject-matter experts familiar with the population studied\n\n\n\n\n\nPublicly available records that may contain same/similar information as the research-interview transcripts\n\n\n\n\nDraft a codebook\nScan for named entities (e.g., names, places, dates)\n\n\n\n\n\nList potential identifiable topics\n\n\n\n\n\nGuidance for evaluating ambiguous information others or public records might have and that combined could jeopardize privacy and confidentiality\n\n\nPhase 2:\nRemediate potentially identifiable data\nEstablish a coding team*\nHire and train coders\nInclude coders with varying levels of familiarity with the data\n\n\n\nReview transcripts and propose re mediation plans\nHighlight each data point and create an audit trail containing proposed edits\nDraft blurred text\n\n\n\n\n\nBracket redacted text\n\n\n\n\n\nReview proposed remediation plans and discuss as a team\n\n\n\nImplement remediation plans\nEdit and redact text\nInsert blurred text, remove redacted text, and insert summaries in the event of important long redactions\n\n\n\nProvide support to coding team if there will be repeated exposure to traumatic con tent*\nCheck in with team members regarding their experiences of vicarious trauma\nProvide information and support regarding the emotional impact of repeated exposure to traumatic content and offer supportive resources\n\n\nPhase 3:\nAssess the validity of the de-identification analyses\nSelect validity standards\nAssess credibility (i.e., confidence in the accuracy of the findings)\nUse of prolonged engagement, persistent engagement, and member checks to assess accuracy of the findings\n\n\n\n\nAssess dependability (i.e., the findings are consistent and could be repeated)\nUse of codebooks, memos, and audit trails to document analyses\n\n\n\n\nAssess transferability (i.e., the findings have applicability in other con texts)\nProvide audiences with sufficient detail about the project so they can assess whether conclusions are transferable to other settings\n\n\n\n\nAssess confirmability (i.e., the findings reflect the participants’ views, not the researchers’ biases)\nConsider how re searchers’ positionalities affected the processes and findings and when necessary, recenter the participants’ views\n\n\n\n*Not applicable to small-scale studies. Source: Adapted from: Campbell et al. (2023).\nNow, let’s focus on some practical ways to remediate potentially identifiable data in interview transcripts. General recommendations include:\n\nEstablish uniform de-identification rules at the start of your project and apply them consistently throughout, especially when working with a team.\nThoroughly review data to pinpoint any details that could lead to individual identification.\nAssign an unique identifier to each participant to replace their name.\nReplace real names (people, companies) with pseudonyms.\nGeneralize location details and dates (e.g., North Carolina → [Southeast], 1977 → [1975-1980])\nGeneralize meaning of detailed variables (e.g., specific professional position → occupation or area of expertise) \nRemove or redact sensitive text or entire sections as needed.\nAvoid blanking out or replacing items without any indication. The use of brackets indicates that something has been changed, modified, or deleted from its original form.\nMaintain a master log of all replacements, aggregations, or removals made and keep it in a secure location separate from the de-identified data files.\n\nIn a single excerpt, you can integrate multiple de-identification techniques. The example below illustrates how an excerpt has been de-identified, with the modifications clearly indicated, while still retaining essential information for future analysis.\n\n\n\n\n\n\n\n🏋️‍♀️ Exercise: Handling identifiable information\n\n\n\n\n\n\n\n\n\n\n\nHow many potential identifiers can you spot? How would you mitigate re-identification risk?\n\n\n\n\n\n*Identifiable Excerpt:** I collaborated with EcoFashion Co., a company based in North Carolina, to launch a sustainable clothing line bearing my name, Chole Adams. Given that this line was closely associated with my personal brand, I insisted on strict terms for fair compensation and transparency in sourcing. Unfortunately, the CEO at the time, Mitchel, did not adhere to these commitments. It was later exposed in a Netflix documentary named Harmful Fashion, that some of the items were produced in Cambodia under very unfair labor practices, a fact Mitchel had failed to disclose to me.*\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are seven potential identifiers that if combined with public information could potentially give away the interviewee’s identity.\nDe-identified Excerpt: I collaborated with [Company A] based in [the Southeastern U.S.] to launch a sustainable clothing line bearing my name [name redacted]. Given that this line was closely associated with my personal brand, I insisted on strict terms for fair compensation and transparency in sourcing. Unfortunately, the CEO at the time, [CEO Name], did not adhere to these commitments. It was later exposed in a [Streaming Service Name] documentary named [Documentary Title], that some items were produced in [Southeast Asia] under very unfair labor practices, which the CEO had failed to disclose to me.\n\n\n\n\n\n\n\n\n\nThis handout provides a compilation with some helpful tips:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/LM2L-K8DN).\nHaving promised to keep participants’ identities confidential, Sarah have assigned pseudonyms to interviewees. However, she is uncertain how to handle indirect identifiers. We will help her to address these issues confidently.\n\n\n\n\n\n\n🏋️‍♀Exercise: Helping Sarah De-identifying Transcripts\n\n\n\n\n\n\n\n\n\n\n\nLet’s open the files for interviewee_01 and interviewee_03 (Sara-Project/Data). To make things easier, we have highlighted some potential sensitive pieces of information. How we would mitigate those issues?\nThe Data-Qualcoder folder contains a possible solution. (FIXME-ADD SOLLUTION FILE)\n\n\n\n\n\n\nAdopting best practices for de-identifying responses from your human participants can significantly enhance both the ease and reliability of this process. It’s crucial to incorporate de-identification considerations early in your planning phase as part of your overall data management strategy. Decisions on which data to collect, what to exclude, and how to inform participants about de-identification will profoundly impact how you can use and share the data in the future as we will explore in future episodes.\n\nRecommended/Cited Sources:\nCampbell R, Javorka M, Engleton J, Fishwick K, Gregory K, Goodman-Williams R. Open-Science Guidance for Qualitative Research: An Empirically Validated Approach for De-Identifying Sensitive Narrative Data. Advances in Methods and Practices in Psychological Science. 2023;6(4). doi:10.1177/25152459231205832\nMyers CA, Long SE, Polasek FO. Protecting participant privacy while maintaining content and context: Challenges in qualitative data De-identification and sharing. ProcAssoc Inf Sci Technol. 2020;57:e415. https://doi.org/10.1002/pra2.415",
    "crumbs": [
      "Analyzing & Documenting",
      "Data De-identification"
    ]
  },
  {
    "objectID": "09_documenting.html",
    "href": "09_documenting.html",
    "title": "Documenting Data",
    "section": "",
    "text": "Required:\n\nOriginal source data, when possible, and as long as it is de-identified\nProcessed data (e.g., transcripts)\nCodebook\nReadme\nInstrument for data collection (e.g., survey, interview, etc), if applicable\nDocumentation generated in the Qualitative Data Analysis Software (QDAS) application (e.g., Nvivo, atlas.TI, etc.): memos, notes, networks, classifications\n\nDesired:\n\nInformed consent statement(s) or assent (see the Human Subjects Data Essentials Primer)\nIRB protocol (see the Human Subjects Data Essentials Primer)\nStudy protocol or procedures manual\nData Management Plan –To indicate how long the data needs to be preserved/accessible, and any other stipulations on sharing/management.\n\nQUALCODER - https://www.qdasoftware.org/refi-qda-codebook",
    "crumbs": [
      "Analyzing & Documenting",
      "Data Documentation"
    ]
  },
  {
    "objectID": "09_documenting.html#what-should-you-document",
    "href": "09_documenting.html#what-should-you-document",
    "title": "Documenting Data",
    "section": "",
    "text": "Required:\n\nOriginal source data, when possible, and as long as it is de-identified\nProcessed data (e.g., transcripts)\nCodebook\nReadme\nInstrument for data collection (e.g., survey, interview, etc), if applicable\nDocumentation generated in the Qualitative Data Analysis Software (QDAS) application (e.g., Nvivo, atlas.TI, etc.): memos, notes, networks, classifications\n\nDesired:\n\nInformed consent statement(s) or assent (see the Human Subjects Data Essentials Primer)\nIRB protocol (see the Human Subjects Data Essentials Primer)\nStudy protocol or procedures manual\nData Management Plan –To indicate how long the data needs to be preserved/accessible, and any other stipulations on sharing/management.\n\nQUALCODER - https://www.qdasoftware.org/refi-qda-codebook",
    "crumbs": [
      "Analyzing & Documenting",
      "Data Documentation"
    ]
  },
  {
    "objectID": "07_tools.html",
    "href": "07_tools.html",
    "title": "Computer-aided Qualitative Data Analysis (CAQDA) tools",
    "section": "",
    "text": "Qualitative data analysis tools vary in terms of features, usability, and costs and researchers should choose the tool that best fits their needs and preferences. It is essential for researchers to have a good understanding of qualitative research principles and methodologies, to then choose the most appropriate tool for their projects.\nThese tools may range from analog methods, such as post-its notes and manual annotation techniques for categorization and color-coding or highlighting, to more advanced computer-aided qualitative data analysis (CAQDA) software that allows for similar strategies in a more automated way and with the aid of advanced features.\n\n\n\nPost-it Coding - Source: Musings (2013)\n\n\nNot rarely, researchers use MS Word or Google docs to annotate and analyze qualitative data. While this process may satisfy some small-scale projects’ needs, there are some disadvantages associated with this process. Can you guess what are those?\n\n\n\nMS Word Coding Example - Source: hdl.handle.net/2139/39482\n\n\n\n\n\n\n\n\nDiscussion: What are the pitfalls of these approaches to coding?\n\n\n\n\n\nWhile the above mentioned approaches may suffice for small-scale projects or initial exploratory tasks, we caution against their use in more research-intensive endeavors. Why? They tend to be considerably more time-consuming and labor-intensive, particularly when managing extensive datasets or coordinating multiple coders. Implementing changes can prove challenging, especially post-coding completion and categorization of data into themes or categories. Revising codes or themes often requires extensive data re-analysis and re-organization, resulting in inefficiencies and project delays. Additionally, ensuring coding consistency and reliability across diverse coders can pose significant challenges, potentially leading to conflicts or discrepancies in interpretation. Collaboration may inadvertently lead to overlapping efforts or disagreements. Furthermore, these methods lack the capability for advanced analysis and visualizations, restricting the depth of insights that can be gleaned from the data.\n\n\n\nWe vouch for the use of Computer-aided Qualitative Data Analysis (CAQDA) Tools given their capability to reduce ambiguities and provide visibility into all instances of a code. Such tools facilitate the seamless updating, merging, and splitting of codes, categories, and themes, ensuring these changes reflect across the board and allow multiple collaborators to concurrently engage in analysis, fostering efficient teamwork. Below we describe some advantages:\n\nCoding and annotation: these tools offer features to code data segments, identifying themes and patterns, with options for flexible coding schemes and annotations as you progress.\nCollaboration and teamwork: these tools facilitate collaborative research efforts by enabling multiple users to work on projects simultaneously, promoting communication and data sharing.\nData management: most CAQDA tools have the ability to import, structure, and manage different types of qualitative data (text, image, videos) in a variety of formats, simplifying data access and retrieval. They also support the export of codebooks and other reports that are important to allow for interpretation and transparency of results.\nData retrieval and querying: such tools allow for search for specific data segments based on coding or other criteria, aiding in the retrieval of pertinent information and a more efficient identification of specific relevant excerpts and quotes to be included in your reports.\nData visualization: most CAQDA tools provide embedded features to generate charts, diagrams, and graphs to assist the visual exploration and interpretation of the data.\nIntegration with other software: such tools integrate with various qualitative research applications, such as transcription or survey tools, to enhance the research workflow.\n\nChoosing a tool will depend on the specific needs of your project, including the types of data you are analyzing, the complexity of the analysis, and whether collaboration is a key requirement.\nThe UCSB Library offers limited NVivo and MaxQDA licenses to campus affiliates via the DREAMLab. These two programs together with Atlas.TI are the leading and most widely recognized proprietary solutions in the market. They are well-known for their comprehensive features, flexibility, and ability to handle complex qualitative research projects.\nSo why not focuses on those well-established proprietary solutions? 1. These those already have comprehensive documentation and online tutorials available. 2. UCSB can only support a limited number of seats per quarter due to the high licensing costs. Additionally, some advanced collaboration and synchronization are not supported for our institutional license. 3. The learning curve to use these tools can be steep for some researchers, especially since some features may be less relevant and underutilized. 4. While we can help you navigate and get started with UCSB-licensed proprietary tools if you choose so, we want promote of more democratic, open-source and free alternatives to help researchers organize, code and analyze qualitative data.\nBelow we compare the top most popular open-source free tool: Qualcoder and Taguette.\nComparison of Qualcoder vs. Taguette for Qualitative Data Analysis\n\n\n\n\n\n\n\n\nFeature\nQualcoder\nTaguette\n\n\n\n\nAccess\nDesktop-based qualitative data analysis tool\nWeb-based qualitative data analysis tool\n\n\nData Formats\nText, audio, video, images, PDFs\nPrimarily text (e.g., plain text, PDFs, Word)\n\n\nCoding\nManual coding, automatic coding, hierarchical coding, AI-based functionality*\nManual coding, tagging\n\n\nMemoing\nCreate and link memos to codes and data segments\nCreate and attach notes to codes and text segments\n\n\nAnalysis\nCode frequency tables, text search, code co-occurrence analysis\nBasic query and search functions\n\n\nVisualizations\nCode clouds, coding matrices\nLimited visualization options\n\n\nCollaboration\nDesigned for single-user projects and. manual file sharing. Some options for working together in a team.\nReal-time sharing and editing capabilities\n\n\nStrengths\nComprehensive coding and memoing tools, supports various data formats, active development community\nEasy to use, excellent for collaborative work, accessible from any device\n\n\nLimitations\nLimited collaboration features, less intuitive for new users, limited advanced visualizations\nLimited to text-based data, basic analysis tools, fewer advanced features\n\n\n\n(*) QualCoder AI (beta) experimental version of QualCoder with AI-enhanced functionality (using GPT-4). See: https://github.com/kaixxx/QualCoder/tree/ai_integration\nDespite of its limited collaboration capabilities (as it will be further described), for this offer of the course we will be using QualCoder. This choice was motivated by the fact that this tool offer more robust coding features than Taguette, and that are more similar to advanced options offered by commercial QDA tools. If you are interested in learning more about Taguette, here is a hands-on workshop to help you get started: Open Qualitative Research.",
    "crumbs": [
      "Analyzing & Documenting",
      "Computer-Aided Analysis Tools"
    ]
  },
  {
    "objectID": "07_tools.html#qualitative-data-analysis-tools",
    "href": "07_tools.html#qualitative-data-analysis-tools",
    "title": "Computer-aided Qualitative Data Analysis (CAQDA) tools",
    "section": "",
    "text": "Qualitative data analysis tools vary in terms of features, usability, and costs and researchers should choose the tool that best fits their needs and preferences. It is essential for researchers to have a good understanding of qualitative research principles and methodologies, to then choose the most appropriate tool for their projects.\nThese tools may range from analog methods, such as post-its notes and manual annotation techniques for categorization and color-coding or highlighting, to more advanced computer-aided qualitative data analysis (CAQDA) software that allows for similar strategies in a more automated way and with the aid of advanced features.\n\n\n\nPost-it Coding - Source: Musings (2013)\n\n\nNot rarely, researchers use MS Word or Google docs to annotate and analyze qualitative data. While this process may satisfy some small-scale projects’ needs, there are some disadvantages associated with this process. Can you guess what are those?\n\n\n\nMS Word Coding Example - Source: hdl.handle.net/2139/39482\n\n\n\n\n\n\n\n\nDiscussion: What are the pitfalls of these approaches to coding?\n\n\n\n\n\nWhile the above mentioned approaches may suffice for small-scale projects or initial exploratory tasks, we caution against their use in more research-intensive endeavors. Why? They tend to be considerably more time-consuming and labor-intensive, particularly when managing extensive datasets or coordinating multiple coders. Implementing changes can prove challenging, especially post-coding completion and categorization of data into themes or categories. Revising codes or themes often requires extensive data re-analysis and re-organization, resulting in inefficiencies and project delays. Additionally, ensuring coding consistency and reliability across diverse coders can pose significant challenges, potentially leading to conflicts or discrepancies in interpretation. Collaboration may inadvertently lead to overlapping efforts or disagreements. Furthermore, these methods lack the capability for advanced analysis and visualizations, restricting the depth of insights that can be gleaned from the data.\n\n\n\nWe vouch for the use of Computer-aided Qualitative Data Analysis (CAQDA) Tools given their capability to reduce ambiguities and provide visibility into all instances of a code. Such tools facilitate the seamless updating, merging, and splitting of codes, categories, and themes, ensuring these changes reflect across the board and allow multiple collaborators to concurrently engage in analysis, fostering efficient teamwork. Below we describe some advantages:\n\nCoding and annotation: these tools offer features to code data segments, identifying themes and patterns, with options for flexible coding schemes and annotations as you progress.\nCollaboration and teamwork: these tools facilitate collaborative research efforts by enabling multiple users to work on projects simultaneously, promoting communication and data sharing.\nData management: most CAQDA tools have the ability to import, structure, and manage different types of qualitative data (text, image, videos) in a variety of formats, simplifying data access and retrieval. They also support the export of codebooks and other reports that are important to allow for interpretation and transparency of results.\nData retrieval and querying: such tools allow for search for specific data segments based on coding or other criteria, aiding in the retrieval of pertinent information and a more efficient identification of specific relevant excerpts and quotes to be included in your reports.\nData visualization: most CAQDA tools provide embedded features to generate charts, diagrams, and graphs to assist the visual exploration and interpretation of the data.\nIntegration with other software: such tools integrate with various qualitative research applications, such as transcription or survey tools, to enhance the research workflow.\n\nChoosing a tool will depend on the specific needs of your project, including the types of data you are analyzing, the complexity of the analysis, and whether collaboration is a key requirement.\nThe UCSB Library offers limited NVivo and MaxQDA licenses to campus affiliates via the DREAMLab. These two programs together with Atlas.TI are the leading and most widely recognized proprietary solutions in the market. They are well-known for their comprehensive features, flexibility, and ability to handle complex qualitative research projects.\nSo why not focuses on those well-established proprietary solutions? 1. These those already have comprehensive documentation and online tutorials available. 2. UCSB can only support a limited number of seats per quarter due to the high licensing costs. Additionally, some advanced collaboration and synchronization are not supported for our institutional license. 3. The learning curve to use these tools can be steep for some researchers, especially since some features may be less relevant and underutilized. 4. While we can help you navigate and get started with UCSB-licensed proprietary tools if you choose so, we want promote of more democratic, open-source and free alternatives to help researchers organize, code and analyze qualitative data.\nBelow we compare the top most popular open-source free tool: Qualcoder and Taguette.\nComparison of Qualcoder vs. Taguette for Qualitative Data Analysis\n\n\n\n\n\n\n\n\nFeature\nQualcoder\nTaguette\n\n\n\n\nAccess\nDesktop-based qualitative data analysis tool\nWeb-based qualitative data analysis tool\n\n\nData Formats\nText, audio, video, images, PDFs\nPrimarily text (e.g., plain text, PDFs, Word)\n\n\nCoding\nManual coding, automatic coding, hierarchical coding, AI-based functionality*\nManual coding, tagging\n\n\nMemoing\nCreate and link memos to codes and data segments\nCreate and attach notes to codes and text segments\n\n\nAnalysis\nCode frequency tables, text search, code co-occurrence analysis\nBasic query and search functions\n\n\nVisualizations\nCode clouds, coding matrices\nLimited visualization options\n\n\nCollaboration\nDesigned for single-user projects and. manual file sharing. Some options for working together in a team.\nReal-time sharing and editing capabilities\n\n\nStrengths\nComprehensive coding and memoing tools, supports various data formats, active development community\nEasy to use, excellent for collaborative work, accessible from any device\n\n\nLimitations\nLimited collaboration features, less intuitive for new users, limited advanced visualizations\nLimited to text-based data, basic analysis tools, fewer advanced features\n\n\n\n(*) QualCoder AI (beta) experimental version of QualCoder with AI-enhanced functionality (using GPT-4). See: https://github.com/kaixxx/QualCoder/tree/ai_integration\nDespite of its limited collaboration capabilities (as it will be further described), for this offer of the course we will be using QualCoder. This choice was motivated by the fact that this tool offer more robust coding features than Taguette, and that are more similar to advanced options offered by commercial QDA tools. If you are interested in learning more about Taguette, here is a hands-on workshop to help you get started: Open Qualitative Research.",
    "crumbs": [
      "Analyzing & Documenting",
      "Computer-Aided Analysis Tools"
    ]
  },
  {
    "objectID": "03_data-interviews.html",
    "href": "03_data-interviews.html",
    "title": "Interview Data",
    "section": "",
    "text": "Interviews entail direct exchanges between investigators and participants, facilitating a thorough exploration of subjects’ experiences, perspectives, and opinions. Through the use of open-ended questions and probing techniques, researchers can uncover richer and more nuanced insights by delving deeply into firsthand narratives that illuminate the research phenomenon. This depth of understanding is often unattainable through other research methodologies.",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "03_data-interviews.html#interview-techniques",
    "href": "03_data-interviews.html#interview-techniques",
    "title": "Interview Data",
    "section": "Interview Techniques",
    "text": "Interview Techniques\nThere are three primary types of interview techniques:\n\nStructured: In this approach, the interviewer follows a set of predefined questions in a specific order. Although this format offers less flexibility, it ensures that all respondents receive the same questions in the same manner and order, leading to more consistent data collection and easier comparison of responses.\nSemi-structured: This method focuses on exploring a few issues in moderate detail, allowing researchers to expand their understanding to some extent. Semi-structured interviews offer the advantage of maintaining objectivity while enabling participants to share their perspectives and opinions. Researchers typically develop an interview guide with targeted open questions to steer the conversation.\nIn-depth/unstructured: This approach aims to delve into a person’s subjective experiences and feelings regarding a specific topic. Such interviews are commonly employed to explore emotive subjects. Researchers craft an interview guide with selected open questions, but participants have more influence over the direction of the conversation compared to semi-structured interviews. In-depth interviews prioritize participants’ lived experiences and are frequently utilized in phenomenology studies.\n\nIt is beyond the scope of this course to cover interview techniques and the actual process of data collection, best approaches to choose one technique over another or strategies for developing interview protocols and conducting interviews. For that we recommend Gubrium & Holstein’s (2001) handbook listed below.",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "03_data-interviews.html#transcription-considerations",
    "href": "03_data-interviews.html#transcription-considerations",
    "title": "Interview Data",
    "section": "Transcription Considerations",
    "text": "Transcription Considerations\nTranscription is an essential process that converts spoken data into written text, capturing the nuances of participants’ responses during interviews or discussions. This detailed record is crucial for researchers as it provides a complete and accurate account of the data, enabling thorough analysis. In academic research, two most common types of transcription are verbatim and intelligent:\n\nVerbatim Transcription: This method includes every utterance, such as filler words or what some call response tokens (“um,” “uh,” “you know”), involuntary vocalizations such as coughing, sneezing, burping, sniffing, laughing, and even non-verbal cues (gestures and face expression) when the interview is film recorded, ensuring that every detail of the conversation is translated into text. This approach preserves the exact way participants express themselves, which can be important for analyzing language use and speech patterns.\nIntelligent Transcription: This approach focuses on the core content of the conversation by omitting filler words and redundant elements. It presents the dialogue in a more readable format while retaining the essential meaning, making it easier to interpret and analyze the main points of the discussion.\n\nTo assist this process researchers can leverage dedicated transcription programs such as Express Scribe that allows easy control of playback speed and navigation through audio segments or even use automatic transcription services such as Otter.ai. Regardless of the chosen tool, we advise researchers to be cautious about technologies that automatically upload recordings for transcription. It is very likely these recordings will contain sensitive and personal information that could be compromised if not handled properly. Therefore, we suggest you:\n\nVerify if the transcription company encrypts your data both during transmission and while at rest. Most services do, but if you find one that doesn’t, it’s wise to consider another provider.\nPrioritize security and privacy when evaluating new technologies. Check the privacy policy and terms of use for the transcription service to understand who can access your data, including any potential access by employees or third parties.\nNot let your project data linger online. After downloading your transcripts and storing them in a secure location, be sure to delete them from the transcription service’s website. This helps protect your information from unauthorized access in case the service experiences a breach.\n\nTranscription is a representation of verbal narratives into written text which can affect how data are conceptualized. Instead of being viewed as a behind-the-scenes task, this process allows in-depth reflection and honoring both the research process and participant’s voice. (Oliver, Serovich & Mason, 2005).\nWhile automated transcription and third-party services can expedite data processing, researchers might prefer to transcribe interviews themselves. Depending on the scope of the project, manual transcription can be an effective way to ensure confidentiality and privacy, maintain better control over data quality, and become more familiar with the content. This hands-on approach allows researchers to observe subtleties in language, tone, and expression that may be missed in automated transcriptions, leading to deeper insights into participants’ thoughts and feelings.\n\nWe won’t have time to cover the transcription process in detail, but here is good guide with things to remember when you are transcribing audio recording.",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "03_data-interviews.html#what-to-do-with-recordings-after-transcription",
    "href": "03_data-interviews.html#what-to-do-with-recordings-after-transcription",
    "title": "Interview Data",
    "section": "What to Do with Recordings After Transcription?",
    "text": "What to Do with Recordings After Transcription?\nOnce your transcriptions are complete and the data is in text form, what’s the next step? Should you hang onto those audio recordings or dispose of them? While some research teams opt to delete the recordings right away, others suggest keeping them for potential future use or inspectability.\nAudio recordings are considered highly identifiable private information. Accents, speech patterns, and vocal signatures can potentially reveal participants’ identities. Therefore, in addition to obtaining formal consent from participants before recording, it’s crucial to disclose how the recordings will be handled, who will have access to them, and the procedures for their eventual destruction.\nThe question of whether to destroy audio or video recordings remains debated. Some advocate for retaining these recordings to ensure that the original data can be reviewed in case of allegations of misconduct and for historical value. Still, special considerations must be given to studies involving vulnerable populations, and ethical guidelines should be followed to balance transparency with privacy. Also, considerations should be made about costs and technical requirements for retaining data indefinitely and maintain its persistence. While there is no one-size-fits-all approach to data retention, researchers should consult with local IRB for recommended practices and follow promises made to research participants in agreeded protocols and informed consents.\n\nRecommended/Cited Sources:\nBailey, J. (2008). First steps in qualitative data analysis: transcribing, Family Practice, Volume 25, Issue 2, April 2008, Pages 127–131, https://doi.org/10.1093/fampra/cmn003\nGubrium, J. F., & Holstein, J. A. (Eds.) (2001). Handbook of interview research. SAGE Publications, Inc., https://doi.org/10.4135/9781412973588\nMcCrae, N., & Murray, J. (2008). When to delete recorded qualitative research data. Research Ethics, 4(2), 76-77. https://doi.org/10.1177/17470161080040021\nOliver, D. G., Serovich, J. M., & Mason, T. L. (2005). Constraints and Opportunities with Interview Transcription: Towards Reflection in Qualitative Research. Social forces; a scientific medium of social study and interpretation, 84(2), 1273–1289. https://doi.org/10.1353/sof.2006.0023\nResnik, D. B., Antes, A., & Mozersky, J. (2024). Should Researchers Destroy Audio or Video Recordings?. Ethics & Human Research, 46(2), 30-35, https://doi.org/10.1002/eahr.500205",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "00_about.html",
    "href": "00_about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "00_about.html#ways-we-can-help-you",
    "href": "00_about.html#ways-we-can-help-you",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handling and Sharing Qualitative Data Responsibly and Effectively",
    "section": "",
    "text": "Qualitative data is vital in social sciences research as it delves deep into human behavior, attitudes, and interactions, offering a comprehensive perspective beyond what quantitative data can provide. It can offer valuable insights into the intricacies of social relationships and as means to develop hypotheses, theories, and models, guiding further inquiry and contributing to knowledge advancement. Often times qualitative data derives from researchers recruitment and engagement with human subjects posing additional challenges and considerations for proper data management.\nThis website aims to assist researchers to better and ethically manage qualitative data from studies involving human subjects throughout its life cycle, since planning to sharing and reusing. Episodes were developed in such way to help researchers:\n\nUnderstand the nuances and challenges of working with qualitative data;\nLearn about the ethical aspects around working with human subjects data and how to stay compliant with IRB regulations;\nCraft better consent forms to allow for data sharing and reuse;\nSelect and apply data de-identification techniques;\nGet started with tools to manage and analyze qualitative data;\nIdentify best approaches to share and receive credit for qualitative data.\n\nThis course is grounded in the research data life cycle depicted in the graphical handout below which focuses on approaches for researchers to handle human subject qualitative data more effectively & responsibly. We will use this framework to explore recommended data management practices applied to human subjects qualitative data throughout the following steps: 1) planning and collecting, 2) analyzing and documenting, 3) sharing/archiving, and 4) reusing.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/D95L-54W8).\nResearchers and students are welcome to take advantage of this learning resource asynchronously at their own pace. We also encourage faculty to reuse or repurpose content to their research methods classes. If you’d like to us to host a hands-on in-person or remote workshop for your team, reach out to RDS for scheduling: rds@library.ucsb.edu.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Handling and Sharing Qualitative Data Responsibly and Effectively",
    "section": "",
    "text": "Qualitative data is vital in social sciences research as it delves deep into human behavior, attitudes, and interactions, offering a comprehensive perspective beyond what quantitative data can provide. It can offer valuable insights into the intricacies of social relationships and as means to develop hypotheses, theories, and models, guiding further inquiry and contributing to knowledge advancement. Often times qualitative data derives from researchers recruitment and engagement with human subjects posing additional challenges and considerations for proper data management.\nThis website aims to assist researchers to better and ethically manage qualitative data from studies involving human subjects throughout its life cycle, since planning to sharing and reusing. Episodes were developed in such way to help researchers:\n\nUnderstand the nuances and challenges of working with qualitative data;\nLearn about the ethical aspects around working with human subjects data and how to stay compliant with IRB regulations;\nCraft better consent forms to allow for data sharing and reuse;\nSelect and apply data de-identification techniques;\nGet started with tools to manage and analyze qualitative data;\nIdentify best approaches to share and receive credit for qualitative data.\n\nThis course is grounded in the research data life cycle depicted in the graphical handout below which focuses on approaches for researchers to handle human subject qualitative data more effectively & responsibly. We will use this framework to explore recommended data management practices applied to human subjects qualitative data throughout the following steps: 1) planning and collecting, 2) analyzing and documenting, 3) sharing/archiving, and 4) reusing.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/D95L-54W8).\nResearchers and students are welcome to take advantage of this learning resource asynchronously at their own pace. We also encourage faculty to reuse or repurpose content to their research methods classes. If you’d like to us to host a hands-on in-person or remote workshop for your team, reach out to RDS for scheduling: rds@library.ucsb.edu.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "04_example.html",
    "href": "04_example.html",
    "title": "Running Example",
    "section": "",
    "text": "From this point onward, we will be working with a fictional research project and a toy dataset containing interview transcripts generated with the assistance of ChatGPT 4.0 mini.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "04_example.html#meet-sarah",
    "href": "04_example.html#meet-sarah",
    "title": "Running Example",
    "section": "Meet Sarah!",
    "text": "Meet Sarah!\n\nImage Generated using openart.ai\nIn the remaining episodes, we’ll be guiding Sarah, a graduate student in the Communication department, as she embarks on a pilot study with content creators/digital influencers for her dissertation research. Sarah’s goal is to understand how content creators/influencers, with verified profiles and a large community of followers, perceive their impact on consumerism, their role in shaping their audience shopping behaviors, and their ethical responsibilities when endorsing products and services.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "04_example.html#example-research-project",
    "href": "04_example.html#example-research-project",
    "title": "Running Example",
    "section": "Example Research Project",
    "text": "Example Research Project\nHer main research question is: How do content creators/digital influencers view their role in shaping their followers’ consumer behavior, and what ethical dilemmas do they face when promoting products?\nGiven the novelty of this research topic and the limited academic literature available, Sarah hopes that the insights gained from this small-scale qualitative exploratory study will help identify key variables for a larger survey study with a representative sample of content creators/digital influencers across the U.S.\nSarah has previous experience with quantitative methods but is very new to qualitative research and could use our help for better handling the data. Having already conducted six short structured with subjects from top revenue niches (i.e., Home Decor and DYI, Travel & Adventure, Fashion & Style, Health & Wellness, Finance & Investment, Beauty & Skincare) and planning to conduct a dozen more, Sarah is eager to begin engaging with the data she has collected so far and deciding how to best organize and interpret it. We’ll be walking her through this process, providing the necessary guidance and support for effective and responsible data management.\nInterviews were conducted over Zoom and audio recorded with participants’ consent. The interview included four main questions, which were consistent across all interviews:\nQ1. Please tell me a little about your work as a content creator/digital influencer how it started, and how you have established yourself in your current niche.\nQ2. In what ways do you believe content creators/digital influencers shape consumer behavior? Could you share any examples?\nQ3. What strategies would you say content creators/digital influencers typically use to increase sales of sponsored products and services? Which ones have you used? What worked and what did not work for you? Why?\nQ4. What do you see as key ethical responsibilities of content creators/digital influencers? In your opinion, how should these ethics apply to endorsements and recommendations? Are there any personal examples you could share?\nEach interview generated approximately 15 minutes of audio recording and were manually transcribed by Sarah. Sarah decided to keep the transcription true to the recordings and seek assistance to mitigate any risk of identification later, as we will see in the next episode.\nNow, let’s make sure we all have the project files and that we take a quick pick on what is included there: Link to Project Folder. Make sure to also save this file in your computer.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "04_example.html#reviewing-documents",
    "href": "04_example.html#reviewing-documents",
    "title": "Running Example",
    "section": "Reviewing Documents",
    "text": "Reviewing Documents\nOpen the Sarah’s Project folder. Before we look at the transcripts, skim through the consent form and identify any opportunities for improvement. What details would you advise Sarah to add?\n\n\n\n\n\n\n💭 Discussion\n\n\n\n\n\n\nDo pilot studies, such as Sarah’s, require IRB review?\n\nAnswer: Yes! A pilot study is a preliminary investigation, often involving 10 or fewer participants, aimed at testing the feasibility of a research design and refining data collection methods or instruments. Despite its exploratory nature and potential lack of inclusion in final results or publications, a pilot study with human subjects requires IRB review as it constitutes human subjects research. If the pilot study refines tools or procedures for a larger study, it remains an integral part of the research process. It’s crucial to inform participants that they are part of a pilot study and clarify whether their data will be used for refinement purposes or included in the main study.\n\nWhat if Sarah accepts our advise and make some modifications to the informed consent? What about the participants who have already signed the original consent form?\n\nAnswer: All amendments (i.e., changes/modifications) to approved research must be submitted to the IRB for review and approval before they are implemented. If the amendments involve changes to how participants’ data will be used or how their involvement is described, inform the participants of these changes. This might involve contacting them to obtain updated consent or simply providing them with new information depending on the nature of the amendments.\n\n\n\nNow, let’s open some transcripts. Sarah did well by not listing actual interviewees’ names. She organized all transcripts in a dedicated data folder and adopted a file convention name. Way to go, Sarah! When working on your own projects, ensure to do the same.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "06_coding.html",
    "href": "06_coding.html",
    "title": "Analyzing Data",
    "section": "",
    "text": "Before we engage with data analysis, we have to create a project in QualCoder.\nLook at this! https://community.csusm.edu/pluginfile.php/21112/mod_resource/content/1/CresswellJWAndPlanoClarkVLPrinciples_of_QualitativeResearchDesigningQualitativeStudyPPT.pdf\nRefer back to trustworthiness and practical ways to accomplish that Intercode reliability Saturation Triangulation",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "06_coding.html#the-role-of-contextuality",
    "href": "06_coding.html#the-role-of-contextuality",
    "title": "Analyzing Data",
    "section": "The Role of Contextuality",
    "text": "The Role of Contextuality\nQualitative data can be explored and re-analyzed to uncover hidden meanings and deeply unfold historical, social and cultural contexts and their entanglement with human subjects’ attitudes, behaviors and opinions. An important contextual issue regarding qualitative data concerns the ‘traces’ left by different perspectives on the material. Texts, whether from interviews, social media, or embedded in an artifact, are not just produced under certain material conditions embedded within socio-cultural contexts, but they are also produced to do something and becomes part of the context to be understood [@silva2007].",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "06_coding.html#a-few-words-about-secondary-analysis",
    "href": "06_coding.html#a-few-words-about-secondary-analysis",
    "title": "Analyzing Data",
    "section": "A Few Words about Secondary Analysis",
    "text": "A Few Words about Secondary Analysis\nArchived qualitative data offers opportunities for reanalysis, reinterpretation, and comparison with both existing and newly collected data sources. While many steps in secondary analysis parallel those of primary data analysis—such as data processing, analysis, and quality control—unique challenges arise, particularly in aligning the archived data with the specific objectives of the new study and assessing the data’s value in that context.\nThe success of secondary analysis is also heavily dependent on the quality and comprehensiveness of the accompanying documentation, which must provide detailed context about the original data collection, including the methodology, sampling, and any potential biases. Without this, the data may be misinterpreted, leading to inaccurate conclusions or limiting its applicability to new research questions. During the hands on component of this course, we will reuse pre-existing data and assess the associated documentation provided. Later, we will also explore recommendations on what type of metadata and documentation should be preserved and archive along with the data.",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "06_coding.html#understanding-qualitative-data-analysis-methods",
    "href": "06_coding.html#understanding-qualitative-data-analysis-methods",
    "title": "Analyzing Data",
    "section": "Understanding Qualitative Data Analysis Methods",
    "text": "Understanding Qualitative Data Analysis Methods\nPicture this: a treasure trove of data brimming with stories, emotions, and insights waiting to be uncovered. That’s the realm of qualitative data analysis – a journey where we navigate through the complexities of human attitudes, behaviors, and perceptions to unearth the gems that lie within.\nQualitative analysis is about finding patterns, unfold both explicit and implicit attitudes, behaviors, and beliefs, understanding meanings, and making sense of findings in relation to the research questions at hand.\nResearchers use various methods, such as thematic analysis, content analysis, comparative analysis, and discourse analysis, to unearth and understand the complexities of human experiences, attitudes, behaviors, and perceptions captured in qualitative data. The goal is to generate rich and nuanced understandings of phenomena rather than producing numerical summaries or generalizable findings. These methods have been used to inform theory development, policy, and practice across disciplines and research domains and applied to various types of data sources, including written documents, social media posts, news articles, advertisements, photographs, videos, and interviews.\n\nThematic analysis: perhaps the most common method for qualitative data analysis, it allows researchers to spot overarching topics and main themes in the data that uncover recurring ideas, topics, or concepts.\nContent analysis: digs deeper into themes to see how often they come up. It’s like zooming in to see the details. Content analysis involves systematically analyzing and interpreting the content of textual, visual, or audio materials to uncover patterns, trends, or meanings. It focuses on the specific elements within the content, such as words, phrases, images, or themes, rather than the broader themes or concepts.\nComparative analysis: delves into the intricate web of causal relationships between events and outcomes across diverse cases. By scrutinizing the nuances and variations it focuses on causal relationships between events and outcomes in different cases.\nDiscourse analysis: helps us understand how language reflects different ideas and cultures. Focuses on spoken or written conversational language.\nSentiment analysis: a branch of discourse of content analysis particularly interested in determining the emotional tone of the message or discourse (speech or written) is positive, negative, or neutral, or even exploring a broader spectrum of sentiments. It can be also heavily computational and quantitative-oriented, depending on the corpus of interest.",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "06_coding.html#thematic-analysis",
    "href": "06_coding.html#thematic-analysis",
    "title": "Analyzing Data",
    "section": "Thematic Analysis",
    "text": "Thematic Analysis\nFor our practical exercises we will focuses on the most common approach; thematic analysis.\n\nSo, here is a recommended workflow we suggest you follow for this method:\n\n\n\nSource: Sendze (2019) adapted from Braun, V., & Clarke, V. (2006).\n\n\nWhether you’re new to research or a pro, qualitative analysis is an active process of reflexivity in which your subjective experience and pre-knowledge about the phenomenon of interest inform your process of making sense of the data and finding patterns and relationships. It is is all about close familiarization with the data, categorizing, discovering, reviewing and iterating until you find meaningful insights to inform your research questions, before you can drawn conclusions and articulated your findings.\nData can be arranged and coded following a deductive (top-down) approach, where researchers apply pre-existing theoretical frameworks or concepts to the data, more inductively (bottom-up), where insights, themes emerge directly from the data without prior assumptions, or even use a combination of both as they engage with and learn more details from the data.\n(FIXME: ADD QUIZ & RECREATE IMAGES)\n\n\nImages from: https://delvetool.com/blog/deductiveinductive\n\nImage source: https://www.nngroup.com/articles/thematic-analysis\nThe top-down approach begins with a specific theoretical framework or research question. Data collection and analysis are guided by preconceived hypotheses, focusing on confirming or refuting these hypotheses. While it provides more direction to the research processs, it may limit alternative perspectives or emergent themes not accounted for in the initial framework.In contrast, the bottom-up approach is exploratory and higly flexible as they are no predetermined categories or hypothesis, it has its basis on grounded theory, meaning that themes emerge directly from the data through open coding and analysis, allowing a wide range of topics and potential unexpected insights to be extracted from the data.\n\n\n\n\n\n\nTime to Practice! 💪🏼\n\n\n\nNow that we’ve gained some knowledge about coding, let’s explore how we can put these concepts into practice. In pairs, access the worksheet (add link) and follow the instructions.\nhttps://docs.google.com/document/d/1Kxjx_Wp0PQ29e3Xhs1GZXTW2K3Da4LxnVd8zq1N2afY/edit\nExercise Sheet (add link) - publish on Zenodo\n\n\nWe will get more into this in later episodes where we will further explore different strategies to perform coding assisted by a QDA open and free software.\nConsider a note about salience and saturation: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7200005/\n\nRecommended & Cited Sources:",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "10_sharing.html",
    "href": "10_sharing.html",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "",
    "text": "Sharing qualitative data benefits both the scholarly community and researchers in several ways:\n\nFostering Public Trust: Transparency enhances public confidence in research outcomes, vital for securing funding and support for future projects. It allows for verification of claims, reinforcing trust in the research.\nDynamic Research Environment: While qualitative research invites diverse interpretations, sharing data fosters improved research quality through collaborative critique and examination.\nEnabling New Research: Access to shared data inspires innovative analyses, maximizing the scientific value of existing studies.\nMore Effective Use of Resources: Data sharing reduces costs related to new data collection, promoting efficient resource utilization and minimizing the burden on frequently targeted communities.\nSkill Development for Trainees: It offers students valuable opportunities to learn coding and analysis techniques, enhancing their educational experience.\nReceiving Credit: Sharing data ensures proper attribution, allowing researchers to gain recognition for their work.\nOpportunities for Collaboration: Open data fosters partnerships among researchers, leading to new insights and advancements.",
    "crumbs": [
      "Sharing & Archiving",
      "Sharing and Archiving"
    ]
  },
  {
    "objectID": "10_sharing.html#sharing-with-caring",
    "href": "10_sharing.html#sharing-with-caring",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "Sharing with Caring",
    "text": "Sharing with Caring\nWhen sharing data, researchers should make their best effort to provide complete and good quality documentation to support reuse.\nBefore we dive into what researchers should share and where. Let’s explore something together.\n\n\n\n\n\n\n💭 Discussion: Comparing Data Deposits\n\n\n\n\n\nPlease open the links to the two data deposits below:\nTaherzadeh, O., 2016, “Interview Transcripts”, Interview Transcripts, https://doi.org/10.7910/DVN/4C9KFK/XRREIY, Harvard Dataverse, V1\nKlein, M., 2022. Interview transcripts of addiction therapists and recovering drug service users. Bath: University of Bath Research Data Archive. Available from: https://doi.org/10.15125/BATH-01096.\nCan you spot any differences? Supposing those were both topics related to your research, how likely would you be to reuse one dataset versus another? Why?\n\nContext and Documentation\n\nTaherzadeh (2016): This deposit lacks detailed contextual information about the study, such as the sample, interview questions, study goals, or informed consent details. It is a standalone collection of transcripts.\nKlein (2022): This deposit provides clearer context, the objectives of the research and questions asked, and links to the associated dissertation.\n\nReuse Value\nTaherzadeh (2016) Dataset: Low\n\nThe absence of context and supporting documentation makes it challenging to assess the dataset’s validity, reliability, and relevance to other research. Without knowing the background or how the data was collected, it’s difficult to justify its use in further studies.\n\nKlein (2022): Higher\n\nThe dataset seems to come with comprehensive documentation, including context about the participants and the study goals. This information facilitates a better understanding of how to apply the data effectively in new research, making it much more reusable.",
    "crumbs": [
      "Sharing & Archiving",
      "Sharing and Archiving"
    ]
  },
  {
    "objectID": "10_sharing.html#considerations-on-what-to-share",
    "href": "10_sharing.html#considerations-on-what-to-share",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "Considerations on What to Share",
    "text": "Considerations on What to Share\nRemember when we discussed the importance of outlining data-sharing plans in Data Management Plans (DMPs)? At this stage, Sarah could greatly benefit from having a clear strategy for archiving and storing her data. As we mentioned earlier, understanding the available options and having at least a rough plan for what will be shared, along with strategies to facilitate the process, is crucial. We provided Sarah with recommendations on what to document, and we hope this guidance will empower her to share her research deliverables confidently while adhering to key principles of open practices.\nAlso it is important to recap the importance of balancing the value of open sharing against the risks of harms associated with the identification of participants, communities and research sites. The good news is that there are more options in between data being closed and open!\n\n\n\n\n\nDepending on your project needs and what was agreed in the informed consent, we recommend you to consider evaluating access control options, they will help you determine which data repository will be most suitable for storing and preserving your project data.\n\nAccess Control Questions\nAccess controls fall into three main categories:\n\nWho can access your data? Access may be limited to qualified researchers, often requiring proof of interest through a research proposal, or it may require pre-approval from an Institutional Review Board (IRB) for general requests.\nHow can others access your data? Secure internet connections might be required for downloading data, along with agreements regarding data storage and destruction. In some cases, researchers may need to access data in person on a secure, offline computer. Hybrid solutions, like ICPSR’s “virtual enclave,” allow remote viewing without data leaving the server.\nWhen can others access your data? Embargoes can temporarily restrict access to protect human participants, often allowing researchers to publish findings before wider access. These embargoes can also facilitate long-term data availability, with set dates for lifting restrictions, as seen in historical archives.\n\n\n\nSharing Levels\n\nOpenly available: data (typically de-identified) shared with no restrictions.\n\nExample: Cunningham, Una; De Brún, Aoife; Mayumi, Willgerodt et al. (2021). Appendices interview formats [Dataset]. Dryad. https://doi.org/10.5061/dryad.q83bk3jg8\n\nSubject to Embargo: a temporary restriction on sharing or publishing data. It means that the data can’t be made public for a set period, usually to protect sensitive information, allow for further analysis, or wait for a specific event such as a formal publication before releasing it.\n\nExample: Ibitoye, Mobolaji; OlaOlorun, Funmilola; Casterline, John B.. 2025. “Demand for Modern Contraception in Sub-Saharan Africa: New Methods, New Evidence”. Qualitative Data Repository. https://doi.org/10.5064/F600CMLO. QDR Main Collection. V1\n\nClosed Access/Metadata Record Only (sensitive data/no consent): a summary and description of a dataset without containing the actual data itself that provides essential information about the dataset’s provenance, structure, and context.\n\nDepending on the research case, access can be provided through a Data Use Agreement (DUA) and involve a data enclave for safe access. These requirements will be also dependable on IRB and consent form agreements.\n\nData Use Agreement (DUA) required: a contract that outlines the terms and conditions for a recipient to use data from a data owner. It’s specific to a project or study and can include limitations on use, data safeguarding obligations, and privacy rights. Some supplementary files (i.e., codebooks, data collection instrument, selected processed data to reproduce specific figures or support some findings).\n\nExample: Steeves, Vicky; Peltzman, Shira; Kim, Julia; Griesinger, Peggy; Blumenthal, Karl-Rainer. 2020. “Data for:”What’s Wrong with Digital Stewardship: Evaluating the Organization of Digital Preservation Programs from Practitioners’ Perspectives”“. Qualitative Data Repository. https://doi.org/10.5064/F6DJRPLK.\n\n\n\n\n\n\n💭 Discussion: What is the value of sharing a metadata record?\n\n\n\n\n\nA metadata-only record for research data that isn’t openly available enables readers to quickly evaluate whether they want to request access. While a well-crafted Data Availability Statement in journal papers serves a similar purpose, a metadata-only record in a suitable repository offers the benefit of being discoverable through data-focused searches, along with the ability to provide more detailed descriptions through rich, linked, and interoperable metadata.\n\n\n\n\n\nA Note About DAS\nData Availability Statements (DAS) are crucial for the credibility of manuscripts and other published research. They provide interested readers—and sometimes automated algorithms—with access to the underlying data supporting your claims, allowing them to verify those assertions or use the data for further research. We suggest following some best practices for crafting statements that are both effective and clear, while also complying with funders’ and journal policies’ requirements.\n&lt;iframe width=“50%” height=“800” src=“https://rcd.ucsb.edu/sites/default/files/2024-02/DLS-202402-DataAvailability_navy.pdf”&gt; ”\n\nSource: UCSB Library Data Literacy Series ([perma.cc/3ZHR-6JAG](https://perma.cc/3ZHR-6JAG)).\n\n\nApplying Access Controls\nImplementing access controls involves a trade-off: while stricter controls reduce misuse risk, they can hinder beneficial access. Though powerful, they should not unnecessarily complicate access to low-risk data. As the principal steward of your data, you ultimately decide on access controls. However, it’s advisable to involve repository staff in this process, as they can highlight potential challenges, ensuring that your data remains accessible and ethically shared in the long run.\n\nSharing de-identified transcripts openly, while placing recordings under more stringent access controls.\nDo keep a list of de-identification rules, both for yourself, or for your team should you collaborate. This list serves as important documentation when you share your data. See for example the protocol used by Thad Dunning and Edward Camp to de-identify data deposited with the Qualitative Data Repository. This document is separate from the key that links de-identified entries to the actual individuals or entities interviewed, which should not be included when you share your data.\nDo check the document properties of files, which may contain identifiers such as original file names identifying interview respondents.\nFinally, do try to strike a balance between keeping your participants’ information confidential and unnecessarily reducing the analytic value of the data by removing too much information. If you are having difficulties striking that balance, you could ask another subject-matter expert for assistance; some repository personnel or data librarians can also provide abstract rules that you can follow.",
    "crumbs": [
      "Sharing & Archiving",
      "Sharing and Archiving"
    ]
  },
  {
    "objectID": "10_sharing.html#what-data",
    "href": "10_sharing.html#what-data",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "What data?",
    "text": "What data?\nICPSR’s Guide for Sharing Qualitative Data outlines examples of of qualitative data sources that may be archived for secondary analysis:\n• Interview methods, including those captured through notes, audio, and video\n\nIn-depth and/or unstructured interviews\nSemi-structured interviews\nFocus group interviews\n\n• Diary studies that are unstructured or use semi-structured writing prompts\n• Observational studies that generate field notes and other text and information\n\nNaturalistic observation of real-world environments (e.g., classrooms, workplaces, healthcare facilities, courtrooms, public spaces)\nParticipant observation where the researcher becomes an active part of the setting to collect information (e.g., online gaming, community policing, nightclub culture)\nStructured observation is where the research has predefined objectives and a systemic approach to collecting information. This would include case studies.\n\n• Text from available sources\n\nMeeting minutes\nOfficial records Medical records\nNews sources and social media\nExcerpts of copyrighted materials (e.g., literature, film, music)\n\n• Survey methods or questionnaires with substantial open-ended comments\n\nWhat format?\n\n\nWhere should you share your project Data?\n\n\nChoosing a Data Repository\nSpecial considerations for Qualitative Data need to be taken into account.\n\n\nPreparing your data for Submission\nRequired:\n\nOriginal source data, whenever possible, and as long as it is de-identified\nProcessed data (e.g., transcripts)\nCodebook\nCoded Data\nReadme\nInstrument for data collection (e.g., survey, interview, etc), if applicable\nDocumentation generated in the Qualitative Data Analysis Software (QDAS) application: memos, notes, networks, classifications\n\nRecommended:\n\nInformed consent statement(s) or assent (see the Human Subjects Data Essentials Primer)\nIRB protocol (see the Human Subjects Data Essentials Primer)\nStudy protocol or procedures manual\nData Management Plan –To indicate how long the data needs to be preserved/accessible, and any other stipulations on sharing/management.\n\n\n\nSource: UCSB Library Data Literacy Series ([perma.cc/E7BA-BBYE](https://perma.cc/E7BA-BBYE)).).)\n\n\nLicensing Your Data\nLicensing\n&lt;iframe width=“25%” height=“450” src=“ ”\nSource: UCSB Library Data Literacy Series ([perma.cc/ET6F-N84X](https://perma.cc/ET6F-N84X))\n\nRecommended/Cited Sources:\nCampbell R, Javorka M, Engleton J, Fishwick K, Gregory K, Goodman-Williams R. Open-Science Guidance for Qualitative Research: An Empirically Validated Approach for De-Identifying Sensitive Narrative Data. Advances in Methods and Practices in Psychological Science. 2023;6(4). doi:10.1177/25152459231205832\nMyers CA, Long SE, Polasek FO. Protecting participant privacy while maintaining content and context: Challenges in qualitative data De-identification and sharing. ProcAssoc Inf Sci Technol. 2020;57:e415. https://doi.org/10.1002/pra2.415\nDuBois, J. M., Strait, M., & Walsh, H. (2018). Is it time to share qualitative research data?Qualitative Psychology, 5(3), 380–393. https://doi.org/10.1037/qup0000076",
    "crumbs": [
      "Sharing & Archiving",
      "Sharing and Archiving"
    ]
  },
  {
    "objectID": "02_ethics.html",
    "href": "02_ethics.html",
    "title": "Ethical Considerations for HS Research",
    "section": "",
    "text": "Ethical practices are central to all scientific endeavors, guiding researchers in the pursuit of knowledge with integrity and responsibility. However, when research involves human subjects, extra ethical considerations and measures should be taken. Ensuring the well-being, dignity, and rights of participants must be prioritized, demanding rigorous standards to protect them from harm and exploitation.",
    "crumbs": [
      "Planning & Collecting",
      "Ethical Considerations for HS Research"
    ]
  },
  {
    "objectID": "02_ethics.html#what-defines-human-subjects-research",
    "href": "02_ethics.html#what-defines-human-subjects-research",
    "title": "Ethical Considerations for HS Research",
    "section": "What defines Human Subjects Research?",
    "text": "What defines Human Subjects Research?\nThe CFR 46 defines human subject as any living subject with whom a researcher (whether professional or student) engages directly to collect information or biospecimens, subsequently utilizing, studying, or analyzing these materials; or acquiring, utilizing, studying, analyzing, or generating identifiable private data or biospecimens.\nIn this context, intervention involves any physical procedures or subject/environment manipulations for data collection, interaction entails the communication or interpersonal contact between investigator and subject, and private information encompasses those provided for specific purposes within the context of the study but not intended for public disclosure (e.g., medical records), which can potentially link it to specific individuals directly or indirectly through coding systems, or when the characteristics allow others to re-identify individuals.\nCertain activities may not be considered human subjects research, including:\n\nClassroom projects and unfunded undergraduate theses: Involving data collection from living individuals for educational purposes, provided they stay within the classroom and are not intended for external use.\nQuality improvement/assurance activities and program evaluations: Conducted internally for measuring program effectiveness or improvement, such as teaching evaluations or curriculum evaluations.\nUse of de-identified or coded private information: Where the research team cannot readily ascertain the identities of individuals and certain conditions are met regarding the handling of coded information.\nCase reports: Limited to describing clinical features and outcomes of a single patient, without involving systematic investigation.\nFact-collecting interviews: Focused on things, products, or policies rather than on individuals’ opinions, behaviors, characteristics, or experiences.\nBiographies or autobiographies: Involving interviews with living individuals about their experiences, limited to that individual and not intended to be generalized beyond them.\n\nEven though the types of studies listed above are typically not considered human subject research, it is always important to check with your campus Institutional Review Board (IRB) or Research Integrity Office for a formal determination on whether your study qualifies as human subject research or not.\n\nJokes aside, IRB boards are tasked with guaranteeing the full protection of the rights and welfare research participants through ethical oversight and by monitoring compliance with regulations. Most importantly, you should NOT start a project or even a pilot study before obtaining a formal determination from the IRB or Research Integrity Office. This is crucial to ensure you are in compliance with all relevant regulations and ethical guidelines for research involving human participants.\n\n\n\n\n\n\nRequesting a Formal Determination\n\n\n\nAt UCSB, researchers should fill out this checklist and follow the submission instructions outlined in the form. Ensure to check specific guidelines and policies at your institution.\n\n\nIf it’s determined that the study constitutes human subject research, then there will be additional steps we will cover shortly. But before let’s open a parenthesis to understand the purpose of IRBs.\n\nWhy IRBs are not only important, but also required?\nInstitutional review boards (IRBs) are federally mandated to review research involving human subjects, ensuring proposed protocols meet ethical guidelines before enrollment. IRBs emerged as means to prevent unethical behavior and adverse risks to participants and to ensure the protection the welfare of individuals participating in research.\nBefore the establishment of IRBs there was no formal oversight of the ethical conduct of research with regard to human subjects, which could unwary expose participants to physical and psychological risks. Also vulnerable groups and economically or educationally disadvantaged groups could be targeted and keen to exploitation. Some infamous examples were the Milgram’s Obedience/Authority study (1961), the Stanford Prison Experiment (1971) and the Tuskegee syphilis study (1931 to 1972). The common thread in these studies was a blatant disregard for the ethical principles of respect for persons, beneficence, and justice in human subjects research. Participants were exploited, deceived, and subjected to significant harm, all in the name of research. These egregious violations of research ethics led to the development of key guidelines and regulations, such as the Belmont Report published and the Common Rule (last updated in 2018), to protect human research participants.\nThe history of IRBs in the U.S it traces back to 1974 with the signing of the National Research Act bill, which createdt he National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research.\nThe publication of the Belmont Report in 1979, provided the moral and ethical framework for human subjects research. It was through the Belmont Report that the basic principles of beneficence, respect of persons, and justice in the context of informed consent, assessment of risks ad benefits and selection of human subjects were first formally outlined, which later led to the development of regulations, such as the Common Rule (originally signed in 1991 and last updated in 2018), to protect human research participants in the United States.\nWe encourage you to watch the video below for a better introduction on what IRBs are and how they serve to protect human subjects:\n\nSource: U.S. Department of Health and Human Services (2018)\n\n\nTypes of IRB Review\nIf your research qualifies as human subject you must go through a IRB review. The type of IRB review required depends on the level of risk to participants and the specific characteristics of the research project. Researchers cannot make the determination themselves, and must submit their project to the IRB for the appropriate level of review.\n\nExempt: Exempt studies include research in educational settings, surveys/interviews of public behavior, and analysis of pre-existing data/specimens. These studies are not monitored by the IRB, but the ethical principles of human subjects research still apply.\nExpedite Review: An expedited review does not mean a faster turnaround, but rather that the project can be reviewed by a single IRB member rather than the full board. A research project that can potentially pose minimal risk to participants is classified under this category.\nFull Review: This is required for studies involving controlled substances, devices, or requiring human biological specimen. Studies that anticipate any procedures that might cause physical harm or any significant psychological distress. It also applies to research on highly sensitive topics (e.g., sexual abuse, suicide, etc.) or with vulnerable populations (e.g., prisoners, illegal immigrants).\n\nIt is never too much to emphasize that researchers cannot self-determine which category their research belongs to and all research with human subjects should be reviewed by the local IRB. For more information, consult the Office of Research’s For Researchers in Human Subjects page. Also, researchers are required to complete mandatory IRB training (https://orahs.research.ucsb.edu) before engaging in research with human participants.\n\n\nThe Role of Consent Forms\nAll research with human participants, included exempted, should ask from participants to confirm their agreement. Signed consent forms or waivers, however, are only requested for expedited and full review studies.\nThe informed consent process is a central component of the ethical conduct of research with human subjects overseen by an Institutional Review Board (IRB). It ensures individuals have an informed choice about whether to participate in a research study. In the United States, the requirements for obtaining informed consent from research participants are stipulated by several regulations and policies.\nIt is beyond of the scope of this course to go through the steps of developing or analyzing consent forms. Such forms are highly contextual to the research and the nature of the study. Nonetheless, here is an example from UCSB Office of Research for illustrative purposes, and essentially, we advise researchers to incorporate the items below:\n\nA declaration indicating that the study constitutes research, accompanied by an elucidation of the research’s essence and objectives.\nA description of the procedures to be employed and an estimation of the duration of the subject’s involvement.\nA declaration outlining the degree, if any, to which the confidentiality of records identifying the subjects will be upheld.\nA statement regarding the potential removal of identifiers from identifiable private information or biospecimens, enabling their utilization in future research without necessitating additional informed consent. Or, a statement asserting that information or biospecimens collected in this research, even post-identifier removal, will not be utilized for subsequent research endeavors.\nA description of any conceivable discomforts and risks that could reasonably be anticipated, or exceptionally, a mention that there are no foreseeable risks.\nAn explanation of any benefits, either to the subject or to society, that could reasonably be expected from the research (note that remuneration or class credit is not deemed a benefit).\nAn invitation to address any questions/concerns regarding the study.\nAn affirmation that participation is voluntary, emphasizing that refusal to participate will incur no penalty or forfeiture of entitled benefits, and that the subject retains the right to discontinue participation at any point without consequence.\nAn explication of whom to contact for information about the research, inclusive of the investigator’s name, telephone number, and email address.\nContact information provided for clarification regarding the rights of research subjects.\n\n\nConsent for Photographs or Recordings\nConsent forms for studies involving audio, video recordings or photographs, should include:\n\nA description of which recording technology will be used and with what purpose.\nThe type and extent of identifiable information that will be captured and retained; and how the recorded data will be stored, secured, used, and destroyed.\nA clear statement requesting confirmation and accounting for the use of excerpts in academic and teaching materials.\n\nFor this last, we recommend you add tiered options, as described below:\n\n\n\n\n\n\n\nExample 1: Recording\nI consent to this interview being audio recorded for transcription purposes. Y____ N ____\n\n\nExample 2: Use/Reuse\nI consent to de-identified excerpts of my interview to be used in academic publications. Y____ N ____\nI consent to de-identified excerpts of my interview to be used in academic publications in classroom activities. Y____ N ____\n\n\nExample 3: Sharing\nI consent audio recordings to be shared with select research teams. Y____ N ____\nI consent audio recordings to be shared on a protected repository available to verified researchers. Y____ N ____\nI consent audio recordings to be shared on a repository with unvetted public access. Y____ N____\n\n\n\n\n\n\n\n\n\n🤔 If I ask, will participants decline?\n\n\n\n\n\nThis is one possible outcome. However, previous research (VandeVusse, Mueller & Karcher, 2021) indicates that even in projects addressing stigmatized and sensitive topics like abortion, most participants are generally willing to consent to data recording and sharing de-identified data for future use. Nevertheless, the authors recommend exercising caution with the language used in informed consent forms to ensure that participants fully understand the terms and provisions.\n\n\n\n\n\nConsent for Data Sharing and Re-use\nMany funding agencies and publishers now require researchers to share their data to promote future reuse and application, including data from qualitative research. Consequently, it is crucial for investigators to carefully address these requirements when developing consent forms that allow for data sharing and reuse. Researchers should clearly outline their plans for managing, using, and sharing data, ensuring that participants fully understand these processes.\nConsent forms should specify how data will be shared and published, detailing the types of data to be shared and the methods of dissemination. They should also include explicit provisions for potential future reuse.\nImportantly, raw unprocessed recordings or unidentified transcripts typically are not shareable and won’t be accepted by a data repository. If private or sensitive identifying information is collected, it must be de-identified before sharing. Additionally, if data will be shared without access controls, the consent form should clearly state that the data may be reused by other researchers for purposes beyond those initially described in the study.\nIn practice, here is how a statement accounting for sharing and potential reuse could look like the following:\n\n\n✍️ “Upon the completion of the project, we plan to archive de-identified interview transcripts, ensuring they remain unlinked to individuals, at [data repository name and link]. These datasets will be openly accessible for reuse by repository users for research or educational purposes, which may differ from those outlined in this study.”\n\n\n\n\n\n\n\n💭 Discussion: Why is it important to state in the consent form that archived data might be used in future research?\n\n\n\n\n\nAnswer: Including this information in the permissive informed consent is crucial because it enables participants to make a fully informed decision, understanding that their data might be used in ways not originally anticipated. This transparency helps participants grasp the full scope of what they are agreeing to and fosters trust in the research process by demonstrating the researchers’ commitment to honesty about potential future uses of the data.\n\n\n\nThis handout provides a compilation of helpful tips:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/U4D8-UYFR).\n\n\n\n\n\n\nImportant\n\n\n\nAdditional Considerations:\n\nAlignment with repository access policies: Ensure consent language supports options for data deposition (controlled or public) supported by your chosen data repository.\nModular Consent: You may also consider creating flexible consent language to adjust future data sharing options and obtain separate signatures for different uses.\nDocumentation: Keep consent documentation with the data to inform future users of participant agreements.\nDon’t worry to memorize all this now! We will recap some of these considerations later on data sharing and archiving episode.\n\n\n\n\n\n\n\n\n\n🏋️‍♀️ Exercise: Review a Consent Form\n\n\n\n\n\nNow that we have learned about the importance of consent forms and some recommendations to craft these documents, in pairs, let’s access the archive for the two projects below:\nKlein, M., 2022. Interview transcripts of addiction therapists and recovering drug service users. Bath: University of Bath Research Data Archive. Available from: https://doi.org/10.15125/BATH-01096.\nVandeVusse, Alicia; Mueller, Jennifer. 2021. “Data for: Qualitative Data Sharing: Participant Understanding, Motivation, and Consent”. Qualitative Data Repository.https://doi.org/10.5064/F6YYA3O3. QDR Main Collection. V2\nLet’s review the project ethical documentation. Please check their consent forms and reflect on the following:\n\nWhat are the strengths of this consent form? What elements contribute positively to ethical and legal clarity?\nIs there any additional information that could have been included to enhance clarity or comprehensiveness?\n\n\n\n\n\nRecommended/Cited Sources:\nCychosz M, Romeo R, Soderstrom M, Scaff C, Ganek H, Cristia A, Casillas M, de Barbaro K, Bang JY, Weisleder A. Longform recordings of everyday life: Ethics for best practices. Behav Res Methods. 2020 Oct;52(5):1951-1969. doi: 10.3758/s13428-020-01365-9. PMID: 32103465; PMCID: PMC7483614.\nICPSR (2024). Recommended Informed Consent Language for Data Sharing: https://perma.cc/Q4JE-8G9F\nUIUC (2022). Representing Data Sharing in Informed Consent: Guidance for Researchers at the University of Illinois at Urbana-Champaign: https://perma.cc/5MQG-FKJB\nVandeVusse, A., Mueller, J., & Karcher, S. (2022). Qualitative data sharing: Participant understanding, motivation, and consent. Qualitative Health Research, 32(1), 182-191. doi: 10.1177/10497323211054058.",
    "crumbs": [
      "Planning & Collecting",
      "Ethical Considerations for HS Research"
    ]
  },
  {
    "objectID": "11_reusing.html",
    "href": "11_reusing.html",
    "title": "Reusing QHS Data",
    "section": "",
    "text": "We have seen the importance of sharing well-documented data and cross-reference it to related research deliverables to maximize their reuse value. Research data can be reused in various ways as outlined in the handout below:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/U4D8-UYFR).\nThe UK Data Archive highlights a the main ways qualitative data tend to be reused:\n\nDescription: Previous research can be utilized to describe the attributes, attitudes, and behaviors of individuals, societies, groups, or organizations during the original project period.\nComparative Research: This approach enables comparisons over time or among different social groups or regions.\nReanalysis: Involves posing new questions to the data and interpreting it in ways not addressed by the original researchers. This could include exploring different themes or topics. The richness and contextual detail of the raw data enhance the potential for fresh insights, without attempting to undermine previous analyses.\nResearch Design and Methodological Advancement: This approach focuses on designing new studies or developing methodologies and research tools. By examining sampling methods, data collection strategies, and topic guides, researchers can gain valuable insights. While researchers often publish sections on methods, their fieldwork diaries and analytic notes can provide deeper context about the research’s development.\nLearning and Teaching: Both classic and contemporary studies serve as valuable case materials for teaching research methods and substantive topics across various social science disciplines.\n\nWhen considering various approaches to data reuse, researchers should ask the following questions:\n\nWhat are the purposes of reuse?\nDo you fully understand the licenses or any restrictions applied to your data of interest?\nAre the new questions similar to or significantly different from those posed by the original researchers?\nDo you have a full understanding of the study limitations?\nSome of these answers will also help to inform a Data Request for controlled access data.\n\n\nCitation & Attribution\nData Citation\n&lt;iframe width=“25%” height=“450” src=“ ”\nSource: UCSB Library Data Literacy Series ([perma.cc/VHW9-9EPQ](https://perma.cc/VHW9-9EPQ))\nReuse/Consent\n&lt;iframe width=“25%” height=“450” src=“ ”",
    "crumbs": [
      "Reusing",
      "Reusing"
    ]
  }
]