[
  {
    "objectID": "09_qualcoder.html",
    "href": "09_qualcoder.html",
    "title": "QualCoder",
    "section": "",
    "text": "QualCoder is client-based open-source and free multi-platform tool designed to streamline qualitative data analysis. With its user-friendly interface and comprehensive features, QualCoder empowers researchers and analysts to efficiently organize, code, and analyze qualitative data, ranging from text documents to multimedia files. By providing tools for coding, categorizing, and retrieving data. QualCoder facilitates in-depth exploration and interpretation, enables researchers to uncover meaningful insights and patterns, and extracts rich and nuanced information from their qualitative data.\nWith QualCoder, you can code text, images, audio and video, and write journal notes and memos. You can categorize codes into a tree-like hierarchical categorization scheme. Coding for audio and video requires the VLC media player. Coder comparison reports can be generated for text coding using Cohen’s Kappa statistic for interrater/intercoder reliability, which measures the level of agreement between coders. A graph displaying codes and categories can be generated to visualize the coding hierarchy. Most reports can be exported as html, open document text (ODT) or as plain text files.\nDocumentation:\n\nWiki - https://github.com/ccbogel/QualCoder/wiki\nGitHub Repository - https://github.com/ccbogel/QualCoder\nGitHub Repository (QualCoder AI - beta) - https://github.com/kaixxx/QualCoder/tree/ai_integration\n\nYou can follow installation and running instructions for your operating system. Please note, however, that Mac users will need to follow a multi-step process requiring additional components and the terminal. We are available to consult with individuals or groups that require assistance in that process.\nTo save time, for this workshop, we will skip the installation process and grant learners access to a virtual machine with QualCoder pre-installed. We use the latest available version of the software (currently 3.5) for the coding and reporting activities.\n\n\nTo run QualCoder in a Mac you will need to type from any directory: qualcoder or,\nFrom the QualCoder-Master directory: python3 -m qualcoder or python3 qualcoder/__main__.py\n\n\n\n\n\n\nRunning Qualcoder\n\n\n\nDo not close the terminal or command prompt running in the background; otherwise, you will exit QualCoder. If you minimize the QualCoder window and get lost, you may click the Python Launch icon.\n\n\n\n\n\nWe will cover some basic settings and terms (attributes, including how to create a project and how to engage with coding. If you are following this content asynchronously, we strongly advise you to watch the following video for additional features:\n\nOther helpful tutorials can be found here: https://github.com/ccbogel/QualCoder/wiki/15-QualCoder-videos\n\n\nIf you are an UCSB-affiliate attending one of the in-person offerings of this workshop, you will have the opportunity to engage with QualCoder via a virtual machine (VM). DUO Multifactor Authentication is required to login and use the VM, so make sure to have your phone handy.\nWe will be using this lab VM dreamlab-datastats, please follow the instructions below according to your operating system:\nMac users\nWindow users\nOnce you follow those instructions, since we will be using a Windows virtual machine, you may click on the QualCoder logo to launch it.\n\n\n\nIn QualCoder, projects serve as containers for organizing multiple files, cases, and attributes. Each project allows you to manage and analyze a specific qualitative data set, keeping everything organized. You can create different projects for distinct research questions or datasets, enabling a streamlined workflow. This structure helps you delve deeper into your analysis without mixing different data sets.\nUnderstanding the roles of files, attributes, cases, and their relationships is key to most QDA software. In QualCoder, files refer to the individual documents or data sources you import for analysis. You may have more than one file associated with a unique case. Cases are specific units of analysis, often linked to files, that allow you to organize and explore your data with more detail. Together, they create a structured environment for qualitative analysis, allowing you to delve into themes and patterns effectively. In our project example, the cases will consist of individual interviewees, but we could also focus on their niche if we were to analyze at that level.\nFor example, let’s imagine that Sarah also collected social media video posts from content creators and digital influencers to analyze behaviors beyond participants’ self-reports by also checking how they have been engaging with their followers. In this case, she can add video clips to QualCoder and associate them with the same research participant. This structure will depend on the nature of your project, allowing for flexibility in how data is organized and analyzed to best capture the insights you seek.\nAttributes are metadata or characteristics that can be assigned to files and cases, helping to categorize or describe them. Examples related to our project example could be demographic pieces of information (e.g., gender, age group), years active, niche, total followers assigned to individuals represented as cases, or characteristics attributed to a file such as the type of source (e.g., interview, social media), total numbers of views, shares and comments, platform (e.g., IG, Youtube, TikTok). You may use memos to record more detailed information about attributes. Think of it as an essential aid to documentation that will provide better contextualization so researchers, including your future self, can better interpret and understand decisions made during the research process.\nAs a heads-up, QualCoder was not designed to analyze attributes statistically. For that, you can export attributes as CSV files and use R or another open dedicated application. Also, the only current options for attributes are numeric and character, so dates, types, or integers are not supported. We will see how we can export attributes later.\nYou can utilize Journals to practice data journaling and document ideas and reflections throughout the coding process, making them searchable with regex expressions—a topic we’ll explore later. Journals are particularly useful for bottom-up approaches, requiring multiple iterations to refine your themes and coding scheme. Plus, you can enhance your journaling content with markdown formatting!\n\n\n\n\nLet’s now set Sarah’s project in QualCoder, bring the files into the project, and perform some housekeeping and organization before engaging with actual coding.\n\n\nProject &gt; Create New Project\nSelect the directory in which you want to save your qda project and name it Influencers.\n\n\n\nWe’ve created a project but don’t have any files yet, so let’s load them now. Choose and manage files, and then, the second icon as noted below (Import File Into Project Folder), select the desired folder and files.\n\nNote that the file ID will be assigned according to the import order, so if you want that to match your file naming convention, you should order it accordingly. If you need to fix it, click the red X on the right and delete the files, then reorder them accordingly and import them again into QualCoder.\nYou may double-click on the file or choose the “eye icon” to open its content. Don’t worry about the information at the bottom of the window for now; we will cover that later.\n\n\n\nCreate Cases\nNow, let’s create a case for each interviewee and link the transcript to them accordingly. We will use the same pseudonym assigned to each research participant described in the file.\nManage &gt; Manage Cases &gt; Pencil (Add Case) &gt; Add your Case Name\n\nLink Files\nNow, let’s link the corresponding file to the case we created. Click on the file cell to manage files for the case. This will prompt another window, as shown below:\n\nNow, create cases and link the files for each of the interviews. You should have one for Linda, Margot, Gina, Otto, Ben, and Alex. Note that you can also import CSV files with cases. This can be handy, especially for projects with several cases.\nDescribe Attributes\nEach transcript has some characteristics associated with interviewees. It would be helpful to describe these as attributes for further analysis. We will only add two attributes: age group and niche. There are two different ways to create attributes: via manage &gt; manage attributes or in the manage file window, you may select the blue plus sign. The latter might be more convenient in this case, considering we have only a few attributes and cases.\nWe will assign age groups (25-29, 30-35, or 35-40) to each case and add their respective niches.\n\nYou can easily edit attribute values directly in the interface. However, to delete or modify an existing attribute column, please navigate to Manage &gt; Manage Attributes\nWe won’t need to add memos for now, but they can provide additional context when attributes aren’t immediately apparent.\nWe now have a well-organized project in QualCoder, so let’s jump into some coding!\n\nRecommended/Cited Sources:\nCheung, K. K. C., & Tai, K. W. H. (2021). The use of intercoder reliability in qualitative interview data analysis in science education. Research in Science & Technological Education, 41(3), 1155–1175. https://doi.org/10.1080/02635143.2021.1993179",
    "crumbs": [
      "Analyzing & Documenting",
      "QualCoder"
    ]
  },
  {
    "objectID": "09_qualcoder.html#what-is-qualcoder",
    "href": "09_qualcoder.html#what-is-qualcoder",
    "title": "QualCoder",
    "section": "",
    "text": "QualCoder is client-based open-source and free multi-platform tool designed to streamline qualitative data analysis. With its user-friendly interface and comprehensive features, QualCoder empowers researchers and analysts to efficiently organize, code, and analyze qualitative data, ranging from text documents to multimedia files. By providing tools for coding, categorizing, and retrieving data. QualCoder facilitates in-depth exploration and interpretation, enables researchers to uncover meaningful insights and patterns, and extracts rich and nuanced information from their qualitative data.\nWith QualCoder, you can code text, images, audio and video, and write journal notes and memos. You can categorize codes into a tree-like hierarchical categorization scheme. Coding for audio and video requires the VLC media player. Coder comparison reports can be generated for text coding using Cohen’s Kappa statistic for interrater/intercoder reliability, which measures the level of agreement between coders. A graph displaying codes and categories can be generated to visualize the coding hierarchy. Most reports can be exported as html, open document text (ODT) or as plain text files.\nDocumentation:\n\nWiki - https://github.com/ccbogel/QualCoder/wiki\nGitHub Repository - https://github.com/ccbogel/QualCoder\nGitHub Repository (QualCoder AI - beta) - https://github.com/kaixxx/QualCoder/tree/ai_integration\n\nYou can follow installation and running instructions for your operating system. Please note, however, that Mac users will need to follow a multi-step process requiring additional components and the terminal. We are available to consult with individuals or groups that require assistance in that process.\nTo save time, for this workshop, we will skip the installation process and grant learners access to a virtual machine with QualCoder pre-installed. We use the latest available version of the software (currently 3.5) for the coding and reporting activities.\n\n\nTo run QualCoder in a Mac you will need to type from any directory: qualcoder or,\nFrom the QualCoder-Master directory: python3 -m qualcoder or python3 qualcoder/__main__.py\n\n\n\n\n\n\nRunning Qualcoder\n\n\n\nDo not close the terminal or command prompt running in the background; otherwise, you will exit QualCoder. If you minimize the QualCoder window and get lost, you may click the Python Launch icon.\n\n\n\n\n\nWe will cover some basic settings and terms (attributes, including how to create a project and how to engage with coding. If you are following this content asynchronously, we strongly advise you to watch the following video for additional features:\n\nOther helpful tutorials can be found here: https://github.com/ccbogel/QualCoder/wiki/15-QualCoder-videos\n\n\nIf you are an UCSB-affiliate attending one of the in-person offerings of this workshop, you will have the opportunity to engage with QualCoder via a virtual machine (VM). DUO Multifactor Authentication is required to login and use the VM, so make sure to have your phone handy.\nWe will be using this lab VM dreamlab-datastats, please follow the instructions below according to your operating system:\nMac users\nWindow users\nOnce you follow those instructions, since we will be using a Windows virtual machine, you may click on the QualCoder logo to launch it.\n\n\n\nIn QualCoder, projects serve as containers for organizing multiple files, cases, and attributes. Each project allows you to manage and analyze a specific qualitative data set, keeping everything organized. You can create different projects for distinct research questions or datasets, enabling a streamlined workflow. This structure helps you delve deeper into your analysis without mixing different data sets.\nUnderstanding the roles of files, attributes, cases, and their relationships is key to most QDA software. In QualCoder, files refer to the individual documents or data sources you import for analysis. You may have more than one file associated with a unique case. Cases are specific units of analysis, often linked to files, that allow you to organize and explore your data with more detail. Together, they create a structured environment for qualitative analysis, allowing you to delve into themes and patterns effectively. In our project example, the cases will consist of individual interviewees, but we could also focus on their niche if we were to analyze at that level.\nFor example, let’s imagine that Sarah also collected social media video posts from content creators and digital influencers to analyze behaviors beyond participants’ self-reports by also checking how they have been engaging with their followers. In this case, she can add video clips to QualCoder and associate them with the same research participant. This structure will depend on the nature of your project, allowing for flexibility in how data is organized and analyzed to best capture the insights you seek.\nAttributes are metadata or characteristics that can be assigned to files and cases, helping to categorize or describe them. Examples related to our project example could be demographic pieces of information (e.g., gender, age group), years active, niche, total followers assigned to individuals represented as cases, or characteristics attributed to a file such as the type of source (e.g., interview, social media), total numbers of views, shares and comments, platform (e.g., IG, Youtube, TikTok). You may use memos to record more detailed information about attributes. Think of it as an essential aid to documentation that will provide better contextualization so researchers, including your future self, can better interpret and understand decisions made during the research process.\nAs a heads-up, QualCoder was not designed to analyze attributes statistically. For that, you can export attributes as CSV files and use R or another open dedicated application. Also, the only current options for attributes are numeric and character, so dates, types, or integers are not supported. We will see how we can export attributes later.\nYou can utilize Journals to practice data journaling and document ideas and reflections throughout the coding process, making them searchable with regex expressions—a topic we’ll explore later. Journals are particularly useful for bottom-up approaches, requiring multiple iterations to refine your themes and coding scheme. Plus, you can enhance your journaling content with markdown formatting!\n\n\n\n\nLet’s now set Sarah’s project in QualCoder, bring the files into the project, and perform some housekeeping and organization before engaging with actual coding.\n\n\nProject &gt; Create New Project\nSelect the directory in which you want to save your qda project and name it Influencers.\n\n\n\nWe’ve created a project but don’t have any files yet, so let’s load them now. Choose and manage files, and then, the second icon as noted below (Import File Into Project Folder), select the desired folder and files.\n\nNote that the file ID will be assigned according to the import order, so if you want that to match your file naming convention, you should order it accordingly. If you need to fix it, click the red X on the right and delete the files, then reorder them accordingly and import them again into QualCoder.\nYou may double-click on the file or choose the “eye icon” to open its content. Don’t worry about the information at the bottom of the window for now; we will cover that later.\n\n\n\nCreate Cases\nNow, let’s create a case for each interviewee and link the transcript to them accordingly. We will use the same pseudonym assigned to each research participant described in the file.\nManage &gt; Manage Cases &gt; Pencil (Add Case) &gt; Add your Case Name\n\nLink Files\nNow, let’s link the corresponding file to the case we created. Click on the file cell to manage files for the case. This will prompt another window, as shown below:\n\nNow, create cases and link the files for each of the interviews. You should have one for Linda, Margot, Gina, Otto, Ben, and Alex. Note that you can also import CSV files with cases. This can be handy, especially for projects with several cases.\nDescribe Attributes\nEach transcript has some characteristics associated with interviewees. It would be helpful to describe these as attributes for further analysis. We will only add two attributes: age group and niche. There are two different ways to create attributes: via manage &gt; manage attributes or in the manage file window, you may select the blue plus sign. The latter might be more convenient in this case, considering we have only a few attributes and cases.\nWe will assign age groups (25-29, 30-35, or 35-40) to each case and add their respective niches.\n\nYou can easily edit attribute values directly in the interface. However, to delete or modify an existing attribute column, please navigate to Manage &gt; Manage Attributes\nWe won’t need to add memos for now, but they can provide additional context when attributes aren’t immediately apparent.\nWe now have a well-organized project in QualCoder, so let’s jump into some coding!\n\nRecommended/Cited Sources:\nCheung, K. K. C., & Tai, K. W. H. (2021). The use of intercoder reliability in qualitative interview data analysis in science education. Research in Science & Technological Education, 41(3), 1155–1175. https://doi.org/10.1080/02635143.2021.1993179",
    "crumbs": [
      "Analyzing & Documenting",
      "QualCoder"
    ]
  },
  {
    "objectID": "03_ethics.html",
    "href": "03_ethics.html",
    "title": "Ethical Considerations for HS Research",
    "section": "",
    "text": "Ethical practices are central to all scientific endeavors, guiding researchers in pursuing knowledge with integrity and responsibility. However, extra ethical considerations and measures should be taken when research involves human subjects. Ensuring participants’ well-being, dignity, and rights must be prioritized, demanding rigorous standards to protect them from harm and exploitation.",
    "crumbs": [
      "Planning & Collecting",
      "Ethical Considerations for HS Research"
    ]
  },
  {
    "objectID": "03_ethics.html#what-defines-human-subjects-research",
    "href": "03_ethics.html#what-defines-human-subjects-research",
    "title": "Ethical Considerations for HS Research",
    "section": "What defines Human Subjects Research?",
    "text": "What defines Human Subjects Research?\nThe CFR 46 defines human subject as any living subject with whom a researcher (whether professional or student) engages directly to collect information or biospecimens, subsequently utilizing, studying, or analyzing these materials; or acquiring, utilizing, studying, analyzing, or generating identifiable private data or biospecimens.\nIn this context, intervention involves any physical procedures or subject/environment manipulations for data collection, interaction entails the communication or interpersonal contact between investigator and subject, and private information encompasses those provided for specific purposes within the context of the study but not intended for public disclosure (e.g., medical records), which can potentially link it to particular individuals directly or indirectly through coding systems, or when the characteristics allow others to re-identify individuals.\nCertain activities may not be considered human subjects research, including:\n\nClassroom projects and unfunded undergraduate theses: Involving data collection from living individuals for educational purposes, provided they stay within the classroom and are not intended for external use.\nQuality improvement/assurance activities and program evaluations: Conducted internally to measure program effectiveness or improvement, such as teaching or curriculum evaluations.\nUse of de-identified or coded private information: Where the research team cannot readily ascertain the identities of individuals and certain conditions are met regarding the handling of coded information.\nCase reports: Limited to describing clinical features and outcomes of a single patient without involving systematic investigation.\nFact-collecting interviews: Focused on things, products, or policies rather than individuals’ opinions, behaviors, characteristics, or experiences.\nBiographies or autobiographies: Involving interviews with living individuals about their experiences, limited to that individual and not intended to be generalized beyond them.\n\nEven though the types of studies listed above are typically not considered human subject research, it is always important to check with your campus Institutional Review Board (IRB) or Research Integrity Office for a formal determination on whether your study qualifies as human subject research.\n\nJokes aside, IRB boards are tasked with guaranteeing the complete protection of the rights and welfare of research participants through ethical oversight and by monitoring compliance with regulations. Most importantly, you should NOT start a project or pilot study before obtaining a formal determination from the IRB or Research Integrity Office. This is crucial to ensure you are in compliance with all relevant regulations and ethical guidelines for research involving human participants.\n\n\n\n\n\n\nRequesting a Formal Determination\n\n\n\n\n\nAt UCSB, researchers should fill out this checklist and follow the submission instructions outlined in the form. Ensure to check specific guidelines and policies at your institution.\n\n\n\nIf it’s determined that the study constitutes human subject research, we will cover additional steps shortly. But first, let’s understand the purpose of IRBs.\n\nWhy are IRBs not only important but also required?\nInstitutional review boards (IRBs) are federally mandated to review research involving human subjects, ensuring proposed protocols meet ethical guidelines before enrollment. IRBs emerged as a means to prevent unethical behavior and adverse risks to participants and to ensure the protection of the welfare of individuals participating in research.\nBefore the establishment of IRBs there was no formal oversight of the ethical conduct of research with regard to human subjects, which could unwary expose participants to physical and psychological risks. Also, vulnerable groups and economically or educationally disadvantaged groups could be targeted and keen to exploitation. Some infamous examples were the Milgram’s Obedience/Authority study (1961), the Stanford Prison Experiment (1971) and the Tuskegee syphilis study (1931 to 1972). The common thread in these studies was a blatant disregard for the ethical principles of respect for persons, beneficence, and justice in human subjects research. Participants were exploited, deceived, and subjected to significant harm, all in the name of research.\nThese egregious violations of research ethics led to the development of key guidelines and regulations, such as the Belmont Report and the Common Rule (last updated in 2018), to protect human research participants.\nThe history of IRBs in the U.S. traces back to 1974, when the National Research Act was signed, creating the National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research.The publication of the Belmont Report in 1979 provided the moral and ethical framework for human subjects research. It was through the Belmont Report that the basic principles of beneficence, respect of persons, and justice in the context of informed consent, assessment of risks and benefits and selection of human subjects were first formally outlined, which later led to the development of regulations, such as the Common Rule (signed initially in 1991 and last updated in 2018), to protect human research participants in the United States.\nWe encourage you to watch the video below for a better introduction on what IRBs are and how they serve to protect human subjects:\n\nSource: U.S. Department of Health and Human Services (2018)\n\n\nTypes of IRB Review\nIf your research qualifies as human subject you must go through a IRB review. The type of IRB review required depends on the level of risk to participants and the specific characteristics of the research project. Researchers cannot make the determination themselves, and must submit their project to the IRB for the appropriate level of review.\n\nExempt: Exempt studies include research in educational settings, surveys/interviews of public behavior, and analysis of pre-existing data/specimens. These studies are not monitored by the IRB, but the ethical principles of human subjects research still apply.\nExpedite Review: An expedited review does not mean a faster turnaround, but rather that the project can be reviewed by a single IRB member rather than the full board. A research project that can potentially pose minimal risk to participants is classified under this category.\nFull Review: This is required for studies involving controlled substances, devices, or requiring human biological specimens. Studies that anticipate any procedures that might cause physical harm or any significant psychological distress. It also applies to research on highly sensitive topics (e.g., sexual abuse, suicide, etc.) or with vulnerable populations (e.g., prisoners, illegal immigrants).\n\nIt is never too much to emphasize that researchers cannot self-determine which category their research belongs to, and all research with human subjects should be reviewed by the local IRB. For more information, consult the Office of Research’s For Researchers in Human Subjects page. Also, researchers are required to complete mandatory IRB training (https://orahs.research.ucsb.edu) before engaging in research with human participants.\n\n\nThe Role of Consent Forms\nAll research with human participants, including exempted, should ask participants to confirm their agreement. However, signed consent forms or waivers are only requested for expedited and full review studies.\nThe informed consent process is a central component of the ethical conduct of research with human subjects overseen by an Institutional Review Board (IRB). It ensures individuals have an informed choice about participating in a research study. In the United States, several regulations and policies stipulate the requirements for obtaining informed consent from research participants.\nIt is beyond the scope of this course to go through the steps of developing or analyzing consent forms. Such forms are highly contextual to the research and the nature of the study. Nonetheless, here is an example from the UCSB Office of Research for illustrative purposes, and essentially, we advise researchers to incorporate the items below:\n\nA declaration indicating that the study constitutes research, accompanied by elucidating the research’s essence and objectives.\nA description of the procedures to be employed and an estimation of the duration of the subject’s involvement.\nA declaration outlining the degree to which the confidentiality of records identifying the subjects will be upheld.\nA statement regarding the potential removal of identifiers from identifiable private information or biospecimens, enabling their utilization in future research without necessitating additional informed consent. Or a statement asserting that information or biospecimens collected in this research, even post-identifier removal, will not be utilized for subsequent research endeavors.\nA description of any conceivable discomforts and risks that could reasonably be anticipated, or exceptionally, a mention that there are no foreseeable risks.\nAn explanation of any benefits, either to the subject or to society, that could reasonably be expected from the research (note that remuneration or class credit is not deemed a benefit).\nAn invitation to address any questions/concerns regarding the study.\nAn affirmation that participation is voluntary, emphasizing that refusal to participate will incur no penalty or forfeiture of entitled benefits and that the subject retains the right to discontinue participation at any point without consequence.\nAn explication of whom to contact for research information, including the investigator’s name, telephone number, and email address.\nContact information provided for clarification regarding the rights of research subjects.\n\n\nConsent for Photographs or Recordings\nConsent forms for studies involving audio, video recordings, or photographs, should include the following:\n\nA description of which recording technology will be used and with what purpose.\nThe type and extent of identifiable information that will be captured and retained and how the recorded data will be stored, secured, used, and destroyed.\nA clear statement requesting confirmation and accounting for the use of excerpts in academic and teaching materials.\n\nFor this last, we recommend you add tiered options, as described below:\n\n\n\n\n\n\n\nExample 1: Recording\nI consent to this interview being audio-recorded for transcription purposes. Y____ N ____\n\n\nExample 2: Use/Reuse\nI consent to de-identified excerpts of my interview to be used in academic publications. Y____ N ____\nI consent to de-identified excerpts of my interview to be used in academic publications in classroom activities. Y____ N ____\n\n\nExample 3: Sharing\nI consent audio recordings to be shared with select research teams. Y____ N ____\nI consent audio recordings to be shared on a protected repository available to verified researchers. Y____ N ____\nI consent audio recordings to be shared on a repository with unvetted public access. Y____ N____\n\n\n\n\n\n\n\n\n\n🤔 If I ask, will participants decline?\n\n\n\n\n\nThis is one possible outcome. However, previous research (VandeVusse, Mueller & Karcher, 2021) indicates that even in projects addressing stigmatized and sensitive topics like abortion, most participants are generally willing to consent to data recording and sharing de-identified data for future use. Nevertheless, the authors recommend exercising caution with the language used in informed consent forms to ensure that participants fully understand the terms and provisions.\n\n\n\n\n\nConsent for Data Sharing and Re-use\nMany funding agencies and publishers now require researchers to share their data to promote future reuse and application, including data from qualitative research. Consequently, it is crucial for investigators to carefully address these requirements when developing consent forms that allow for data sharing and reuse. Researchers should clearly outline their plans for managing, using, and sharing data, ensuring participants fully understand these processes.\nConsent forms should specify how data will be shared and published, detailing the types of data to be shared and the methods of dissemination. They should also include explicit provisions for potential future reuse.\nImportantly, raw unprocessed recordings or unidentified transcripts typically are not shareable and won’t be accepted by a data repository. If private or sensitive identifying information is collected, it must be de-identified before sharing. Additionally, if data will be shared without access controls, the consent form should clearly state that the data may be reused by other researchers for purposes beyond those initially described in the study.\nIn practice, here is how a statement accounting for sharing and potential reuse could look like the following:\n\n\n✍️ “Upon the completion of the project, we plan to archive de-identified interview transcripts, ensuring they remain unlinked to individuals, at [data repository name and link]. These datasets will be openly accessible for reuse by repository users for research or educational purposes, which may differ from those outlined in this study.”\n\n\n\n\n\n\n\n💭 Discussion: Why is it important to state in the consent form that archived data might be used in future research?\n\n\n\n\n\nAnswer: Including this information in the permissive informed consent is crucial because it enables participants to make a fully informed decision, understanding that their data might be used in ways not originally anticipated. This transparency helps participants grasp the full scope of what they are agreeing to and fosters trust in the research process by demonstrating the researchers’ commitment to honesty about potential future uses of the data.\n\n\n\nThis handout provides a compilation of helpful tips:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/U4D8-UYFR).\n\n\n\n\n\n\nAdditional Considerations\n\n\n\n\n\n\nAlignment with repository access policies: Ensure consent language supports options for data deposition (controlled or public) supported by the repository of your choice.\nModular Consent: You may also consider creating flexible consent language to adjust future data sharing options and obtain separate signatures for different uses.\nDocumentation: Keep consent documentation with the data to inform future users of participant agreements.\nDon’t worry to memorize all this now! We will recap some of these considerations later on data sharing and archiving episode.\n\n\n\n\n\n\n\n\n\n\n🏋️‍♀️ Exercise: Review a Consent Form\n\n\n\n\n\nNow that we have learned about the importance of consent forms and some recommendations to craft these documents, in pairs, let’s access the archive for the below:\nVandeVusse, Alicia; Mueller, Jennifer. 2021. “Data for: Qualitative Data Sharing: Participant Understanding, Motivation, and Consent”. Qualitative Data Repository.https://doi.org/10.5064/F6YYA3O3. QDR Main Collection. V2\nLet’s review the project ethical documentation. Please check the consent form and reflect on the following:\n\nWhat are the strengths of this consent form? What elements contribute positively to ethical and legal clarity?\nIs there any additional information that could have been included to enhance clarity or comprehensiveness?\n\n\n\n\n\nRecommended/Cited Sources:\nCychosz M, Romeo R, Soderstrom M, Scaff C, Ganek H, Cristia A, Casillas M, de Barbaro K, Bang JY, Weisleder A. Longform recordings of everyday life: Ethics for best practices. Behav Res Methods. 2020 Oct;52(5):1951-1969. doi: 10.3758/s13428-020-01365-9. PMID: 32103465; PMCID: PMC7483614.\nICPSR (2024). Recommended Informed Consent Language for Data Sharing: https://perma.cc/Q4JE-8G9F\nUIUC (2022). Representing Data Sharing in Informed Consent: Guidance for Researchers at the University of Illinois at Urbana-Champaign: https://perma.cc/5MQG-FKJB\nVandeVusse, A., Mueller, J., & Karcher, S. (2022). Qualitative data sharing: Participant understanding, motivation, and consent. Qualitative Health Research, 32(1), 182-191. doi: 10.1177/10497323211054058.",
    "crumbs": [
      "Planning & Collecting",
      "Ethical Considerations for HS Research"
    ]
  },
  {
    "objectID": "06_deidentification.html",
    "href": "06_deidentification.html",
    "title": "Data De-identification",
    "section": "",
    "text": "Disclosing sensitive information without proper safeguards can lead to significant harm and jeopardize not only the well-being of participants but also the integrity and trustworthiness of the research process.\nThe richness of the data obtained through interviews presents additional ethical and analytical challenges, particularly regarding the need for more nuanced and intricate analysis, concerning the protection of participant’ identity, and the mitigation of the risk of re-identification.\nQualitative data can be more difficult to de-identify compared to quantitative data due to its typically unstructured nature. Another challenge is that qualitative data includes unique personal narratives and contextual details that can inadvertently reveal identities, requiring careful and nuanced de-identification strategies to protect participants’ privacy effectively. The process of de-identification or anonymization can also alter or diminish data value, especially when the significance lies in capturing personal experiences or narratives. Researchers should consider using de-identification alongside other methods, such as limiting access to the data or obtaining explicit consent from participants to share some or all of their personal information. Because extensive de-identification might sometimes obscure important details needed for deep analysis, striking a balance between removing identifiers and preserving the essential context of the qualitative data is key.\nIn a nutshell, data de-identification entails the process of removing direct and indirect identifiers from a dataset, while maintaining enough information to preserve its value and usability to future research. In this episode, we cover common strategies for de-identifying interview data and recommendations for handling sensitive information.",
    "crumbs": [
      "Analyzing & Documenting",
      "Data De-identification"
    ]
  },
  {
    "objectID": "06_deidentification.html#direct-vs-indirect-identifiers",
    "href": "06_deidentification.html#direct-vs-indirect-identifiers",
    "title": "Data De-identification",
    "section": "Direct vs Indirect Identifiers",
    "text": "Direct vs Indirect Identifiers\nA person’s identity can be disclosed from direct identifiers, unique to an individual or indirect identifiers which, when linked with other available information, could identify someone.\nDirect identifiers are pieces of information that can immediately identify an individual on their own, such as a social security number, full name, email address, home address, or phone number. In contrast, indirect identifiers are data points that do not uniquely identify someone by themselves but can do so when combined with other information. Examples of indirect identifiers include job title, gender, and ethnicity. The risk associated with indirect identifiers can vary depending on the context and the availability of additional data. Therefore, it is crucial to understand which data points are relevant within the context of the study and to consider both direct and indirect identifiers to effectively protect privacy and minimize the risk of identification.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/LM2L-K8DN).\nInitially, researchers might change names and disguise locations, but effectively managing identifying details in qualitative data requires a nuanced approach. Anonymity and privacy exist on a spectrum, so researchers must balance the risk of identification with the needs of their research.\nWhen data isn’t fully anonymized, including cases of potential indirect identification, obtaining explicit consent is essential before public release. This consent should be secured either at the time of data collection or prior to publication.\nOne significant challenge arises with indirect identification, particularly in small groups where participants know each other. This issue can be exacerbated by existing power dynamics, such as those between a team and its leader. Therefore, researchers must transparently communicate the risks of identity disclosure to participants and respect their wishes.\nTo mitigate these risks, techniques such as using multiple pseudonyms, rephrasing quotes, and breaking known links can be effective. These methods help protect participant identities while allowing for restricted sharing of the dataset.\nHowever, there may be situations where these strategies are not feasible. In such cases, researchers need to carefully consider how to obtain consent for disclosing an individual’s identity without inadvertently revealing the identities of others. Ultimately, the outcomes of these processes should be documented in the data management plan and ethics application, ensuring that all considerations are addressed.\n\n\n\n\n\n\n💭 Discussion: What could be potential direct and indirect identifiers in the context of Sarah’s research?\n\n\n\n\n\nAnswer: Besides names, social media usernames or handles are considered direct identifiers, while patterns in use of custom tags and symbols, locations, bios and profile descriptions and catchphrases combined might re-identify users, especially in niche activities.\nConsider the infamous Baby Reindeer Netflix series lawsuit, in which a simple keyword search on Twitter swiftly uncovered the identity of the real person behind the pseudonym used in the series.",
    "crumbs": [
      "Analyzing & Documenting",
      "Data De-identification"
    ]
  },
  {
    "objectID": "06_deidentification.html#data-de-identification-essentials",
    "href": "06_deidentification.html#data-de-identification-essentials",
    "title": "Data De-identification",
    "section": "Data De-identification Essentials",
    "text": "Data De-identification Essentials\nThe existing literature provides limited guidance on de-identifying qualitative data, with most researchers relying on ad hoc strategies. Perhaps the most robust and up-to-date methodological framework for handling sensitive narrative data is found in Campbell and co-authors (2023) multiphased process inspired by common qualitative coding techniques.\nIn the first phase, the process involves consultations with a range of stakeholders and subject-matter experts to identify risks related to re-identifiability and concerns about data sharing. The second phase outlines an iterative approach to identifying potentially identifiable information and developing tailored remediation strategies through group review and consensus. The final phase includes multiple methods for evaluating the effectiveness of the de-identification efforts, ensuring that the remediated transcripts adequately protect participants’ privacy. If your project involves working with vulnerable or protected groups, or handling sensitive data, we strongly recommend reviewing and applying this framework to your research.\n\n\n\nQualitative Data De-identification Overview. Source: Campbell et al. (2023)\n\n\nThese three phases can be broken into tasks, actions and examples:\n\n\n\n\n\n\n\n\n\nPhase goal\nTasks\nActions\nExamples\n\n\n\n\nPhase 1:\nDevelop a process to distinguish potentially identifiable data\nCreate a coding framework\nConsult with stakeholders and look for strategies followed by similar projects\nRegulatory guidance (HIPPAA, IRB, relevant professional and research associations)\n\n\n\n\n\nSubject-matter experts familiar with the population studied\n\n\n\n\n\nPublicly available records that may contain same/similar information as the research-interview transcripts\n\n\n\n\nDraft a codebook\nScan for named entities (e.g., names, places, dates)\n\n\n\n\n\nList potential identifiable topics\n\n\n\n\n\nGuidance for evaluating ambiguous information others or public records might have and that combined could jeopardize privacy and confidentiality\n\n\nPhase 2:\nRemediate potentially identifiable data\nEstablish a coding team*\nHire and train coders\nInclude coders with varying levels of familiarity with the data\n\n\n\nReview transcripts and propose re mediation plans\nHighlight each data point and create an audit trail containing proposed edits\nDraft blurred text\n\n\n\n\n\nBracket redacted text\n\n\n\n\n\nReview proposed remediation plans and discuss as a team\n\n\n\nImplement remediation plans\nEdit and redact text\nInsert blurred text, remove redacted text, and insert summaries in the event of important long redactions\n\n\n\nProvide support to coding team if there will be repeated exposure to traumatic con tent*\nCheck in with team members regarding their experiences of vicarious trauma\nProvide information and support regarding the emotional impact of repeated exposure to traumatic content and offer supportive resources\n\n\nPhase 3:\nAssess the validity of the de-identification analyses\nSelect validity standards\nAssess credibility (i.e., confidence in the accuracy of the findings)\nUse of prolonged engagement, persistent engagement, and member checks to assess accuracy of the findings\n\n\n\n\nAssess dependability (i.e., the findings are consistent and could be repeated)\nUse of codebooks, memos, and audit trails to document analyses\n\n\n\n\nAssess transferability (i.e., the findings have applicability in other con texts)\nProvide audiences with sufficient detail about the project so they can assess whether conclusions are transferable to other settings\n\n\n\n\nAssess confirmability (i.e., the findings reflect the participants’ views, not the researchers’ biases)\nConsider how re searchers’ positionalities affected the processes and findings and when necessary, recenter the participants’ views\n\n\n\n*Not applicable to small-scale studies. Source: Adapted from: Campbell et al. (2023).\nNow, let’s focus on some practical ways to remediate potentially identifiable data in interview transcripts. General recommendations include:\n\nEstablish uniform de-identification rules at the start of your project and apply them consistently throughout, especially when working with a team.\nThoroughly review data to pinpoint any details that could lead to individual identification.\nAssign an unique identifier to each participant to replace their name.\nReplace real names (people, companies) with pseudonyms.\nGeneralize location details and dates (e.g., North Carolina → [Southeast], 1977 → [1975-1980])\nGeneralize meaning of detailed variables (e.g., specific professional position → occupation or area of expertise) \nRemove or redact sensitive text or entire sections as needed.\nAvoid blanking out or replacing items without any indication. The use of brackets indicates that something has been changed, modified, or deleted from its original form.\nMaintain a master log of all replacements, aggregations, or removals made and keep it in a secure location separate from the de-identified data files.\n\nIn a single excerpt, you can integrate multiple de-identification techniques. The example below illustrates how an excerpt has been de-identified, with the modifications clearly indicated, while still retaining essential information for future analysis.\n\n\n\n\n\n\n\n🏋️‍♀️ Exercise: Handling Identifiable Information\n\n\n\n\n\n\n\n\n\n\n\nHow many potential identifiers can you spot? How would you mitigate re-identification risk?\n\n\n\n\n\nIdentifiable Excerpt: I collaborated with EcoFashion Co., a company based in North Carolina, to launch a sustainable clothing line bearing my name, Chole Adams. Given that this line was closely associated with my personal brand, I insisted on strict terms for fair compensation and transparency in sourcing. Unfortunately, the CEO at the time, Mitchel, did not adhere to these commitments. It was later exposed in a Netflix documentary named Harmful Fashion, that some of the items were produced in Cambodia under very unfair labor practices, a fact Mitchel had failed to disclose to me.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are seven potential identifiers that if combined with public information could potentially give away the interviewee’s identity.\nDe-identified Excerpt: I collaborated with [Company A] based in [the Southeastern U.S.] to launch a sustainable clothing line bearing my name [name redacted]. Given that this line was closely associated with my personal brand, I insisted on strict terms for fair compensation and transparency in sourcing. Unfortunately, the CEO at the time, [name redacted], did not adhere to these commitments. It was later exposed in a [Streaming Service name redacted] documentary named [Documentary Title redacted], that some items were produced in [Southeast Asia] under very unfair labor practices, which the CEO had failed to disclose to me.\n\n\n\n\n\n\n\n\n\nThis handout provides a compilation with some helpful tips:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/LM2L-K8DN).\nHaving promised to keep participants’ identities confidential, Sarah have assigned pseudonyms to interviewees. However, she is uncertain how to handle indirect identifiers. We will help her to address these issues confidently.\n\n\n\n\n\n\n🏋️‍♀Exercise: Helping Sarah De-identifying Transcripts\n\n\n\n\n\n\n\n\n\n\n\nLet’s open the files for interviewee_01 and interviewee_03 (Sara-Project/Data/Transcripts). To make things easier, we have highlighted some potential sensitive pieces of information. What strategies can we implement to minimize the risk of interviewees’ identification?\nThe Deid-Transcripts folder contains a possible solution.\n\n\n\n\n\n\nAdopting best practices for de-identifying responses from your human participants can significantly enhance both the ease and reliability of this process. It’s crucial to incorporate de-identification considerations early in your planning phase as part of your overall data management strategy. Decisions on which data to collect, what to exclude, and how to inform participants about de-identification will profoundly impact how you can use and share the data in the future as we will explore in future episodes.\n\nRecommended/Cited Sources:\nCampbell R, Javorka M, Engleton J, Fishwick K, Gregory K, Goodman-Williams R. Open-Science Guidance for Qualitative Research: An Empirically Validated Approach for De-Identifying Sensitive Narrative Data. Advances in Methods and Practices in Psychological Science. 2023;6(4). doi:10.1177/25152459231205832\nMyers CA, Long SE, Polasek FO. Protecting participant privacy while maintaining content and context: Challenges in qualitative data De-identification and sharing. ProcAssoc Inf Sci Technol. 2020;57:e415. https://doi.org/10.1002/pra2.415",
    "crumbs": [
      "Analyzing & Documenting",
      "Data De-identification"
    ]
  },
  {
    "objectID": "05_example.html",
    "href": "05_example.html",
    "title": "Running Example",
    "section": "",
    "text": "From this point onward, we will be working with a fictional research project and a toy dataset containing interview transcripts generated with the assistance of ChatGPT 4.0 mini.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "05_example.html#meet-sarah",
    "href": "05_example.html#meet-sarah",
    "title": "Running Example",
    "section": "Meet Sarah!",
    "text": "Meet Sarah!\n\nImage Generated using openart.ai\nIn the remaining episodes, we’ll be guiding Sarah, a graduate student in the Communication department, as she embarks on a pilot study with content creators/digital influencers for her dissertation research. Sarah’s goal is to understand how content creators/influencers, with verified profiles and a large community of followers, perceive their impact on consumerism, their role in shaping their audience shopping behaviors, and their ethical responsibilities when endorsing products and services.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "05_example.html#example-research-project",
    "href": "05_example.html#example-research-project",
    "title": "Running Example",
    "section": "Example Research Project",
    "text": "Example Research Project\nHer main research question is: How do content creators/digital influencers view their role in shaping their followers’ consumer behavior, and what ethical dilemmas do they face when promoting products?\nGiven the novelty of this research topic and the limited academic literature available, Sarah hopes that the insights gained from this small-scale qualitative exploratory study will help identify key variables for a larger survey study with a representative sample of content creators/digital influencers across the U.S.\nSarah has previous experience with quantitative methods but is very new to qualitative research and could use our help for better handling the data. Having already conducted six short structured interviews with subjects from top revenue niches (i.e., Home Decor and DYI, Travel & Adventure, Fashion & Style, Health & Wellness, Finance & Investment, Beauty & Skincare) and planning to conduct a dozen more, Sarah is eager to begin engaging with the data she has collected so far and deciding how to best organize and interpret it. We’ll be walking her through this process, providing the necessary guidance and support for effective and responsible data management.\nInterviews were conducted over Zoom and audio recorded with participants’ consent. The interview included four main questions, which were consistent across all interviews:\nQ1. Please tell me a little about your work as a content creator/digital influencer how it started, and how you have established yourself in your current niche.\nQ2. In what ways do you believe content creators/digital influencers shape consumer behavior? Could you share any examples?\nQ3. What strategies would you say content creators/digital influencers typically use to increase sales of sponsored products and services? Which ones have you used? What worked and what did not work for you? Why?\nQ4. In your view, what are the essential ethical responsibilities that content creators and digital influencers should uphold? Can you share any personal experiences that illustrate these responsibilities in action?\nEach interview generated approximately 15 minutes of audio recording and were manually transcribed by Sarah. Sarah decided to keep the transcription true to the recordings and seek assistance to mitigate any risk of identification later, as we will see in the next episode.\nNow, let’s make sure we all have the project files and that we take a quick pick on what is included there: Link to Project Folder. Make sure to also save this file in your computer.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "05_example.html#reviewing-documents",
    "href": "05_example.html#reviewing-documents",
    "title": "Running Example",
    "section": "Reviewing Documents",
    "text": "Reviewing Documents\nOpen the Sarah’s Project folder. Before we look at the transcripts, skim through the consent form and identify any opportunities for improvement. What details would you advise Sarah to add?\n\n\n\n\n\n\n💭 Discussion\n\n\n\n\n\n\nDo pilot studies, such as Sarah’s, require IRB review?\n\nAnswer: Yes! A pilot study is a preliminary investigation, often involving 10 or fewer participants, aimed at testing the feasibility of a research design and refining data collection methods or instruments. Despite its exploratory nature and potential lack of inclusion in final results or publications, a pilot study with human subjects requires IRB review as it constitutes human subjects research. If the pilot study refines tools or procedures for a larger study, it remains an integral part of the research process. It’s crucial to inform participants that they are part of a pilot study and clarify whether their data will be used for refinement purposes or included in the main study.\n\nWhat if Sarah accepts our advise and make some modifications to the informed consent? What about the participants who have already signed the original consent form?\n\nAnswer: All amendments (i.e., changes/modifications) to approved research must be submitted to the IRB for review and approval before they are implemented. If the amendments involve changes to how participants’ data will be used or how their involvement is described, inform the participants of these changes. This might involve contacting them to obtain updated consent or simply providing them with new information depending on the nature of the amendments.\n\n\n\nNow, let’s open some transcripts. Sarah did well by not listing actual interviewees’ names. She organized all transcripts in a dedicated data folder and adopted a file convention name before sharing it with us. Way to go, Sarah! When working on your own projects, ensure to do the same.",
    "crumbs": [
      "Planning & Collecting",
      "Running Example"
    ]
  },
  {
    "objectID": "02_dmp.html",
    "href": "02_dmp.html",
    "title": "Data Management Planning",
    "section": "",
    "text": "A Data Management Plan (DMP) is a strategic document that outlines how you will handle data throughout a project’s life cycle. It serves as a roadmap for managing, storing, and sharing data effectively, ensuring that your data is well-organized, accessible, and secure. By documenting your data management strategies, a DMP helps streamline data workflows, facilitates compliance with institutional and funding agency requirements, and enhances the efficiency and integrity of research.\nThe importance of a DMP can be highlighted in several ways:\n\nOrganization and efficiency: A well-structured DMP ensures that data is collected, stored, and analyzed consistently and organized. This reduces the risk of data loss, duplication, or mismanagement and helps team members quickly find and use the data they need.\nResource allocation: The plan helps allocate resources effectively, including budgeting for data management tools and personnel. This foresight can prevent unexpected costs and ensure data management tasks are adequately resourced.\nData security and privacy: The DMP outlines measures for safeguarding sensitive or confidential data, ensuring compliance with privacy regulations and ethical guidelines. This includes specifying access controls, encryption methods, and protocols for data anonymization.\nData sharing and reproducibility: By detailing how data will be shared and made available, the DMP supports transparency and collaboration. This includes specifying data formats, metadata standards, and repositories for public or restricted access, enhancing research findings’ reproducibility.\nLong-Term preservation: The DMP addresses strategies for long-term data preservation, including backup procedures, archival formats, and storage solutions. This ensures that valuable data remains accessible and usable beyond the immediate project duration.\n\nMost funders require researchers to submit a Data Management Plan (DMP) and their research proposal as a prerequisite for consideration. However, developing a DMP offers substantial benefits beyond meeting this requirement. It serves as a crucial tool for all researchers, aiding them in anticipating resource needs, exploring available services, and strategically planning for data management throughout the project’s life cycle. For example, a researcher preparing to conduct interviews can use the DMP to identify and utilize institutionally supported transcription services and secure storage solutions.\nBy anticipating these needs early on, the researcher can allocate appropriate budget and resources, ensuring these essential services are available when required. Additionally, the DMP helps the researcher create a structured plan for managing qualitative data, such as organizing interview transcripts, coding data, and maintaining a detailed audit trail of data analysis decisions.\nBroadly, the DMP should cover the following topics:\n\nThe types of data you expect to collect,\nHow those data will be documented and organized,\nHow the data will be stored and kept secure, and\nHow will the data be shared (or why not) and stored for the long term?\n\nThe ultimate goal is that researchers will make more informed decisions on how to produce and share data satisfying the FAIR principles, which essentially aim to ensure that the data is:\n\nFindable: It is published on a stable location and indexed\nAcessible: It can be easily retrieved\nInteroperable: It can be read by humans and machines\nReusable: It has clear usage licenses and good enough documentation for interpretation\n\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/CT8P-D5MK).\nResearchers studying Indigenous communities should pay special attention to an additional set of principles, named CARE, an acronym for a set of purpose-oriented principles for Indigenous Data Governance, which aims to help advance Indigenous innovation, sovereignty, and self-determination.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/3ZHR-6JAG).",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "02_dmp.html#what-are-data-management-plans-dmps-and-why-should-you-care",
    "href": "02_dmp.html#what-are-data-management-plans-dmps-and-why-should-you-care",
    "title": "Data Management Planning",
    "section": "",
    "text": "A Data Management Plan (DMP) is a strategic document that outlines how you will handle data throughout a project’s life cycle. It serves as a roadmap for managing, storing, and sharing data effectively, ensuring that your data is well-organized, accessible, and secure. By documenting your data management strategies, a DMP helps streamline data workflows, facilitates compliance with institutional and funding agency requirements, and enhances the efficiency and integrity of research.\nThe importance of a DMP can be highlighted in several ways:\n\nOrganization and efficiency: A well-structured DMP ensures that data is collected, stored, and analyzed consistently and organized. This reduces the risk of data loss, duplication, or mismanagement and helps team members quickly find and use the data they need.\nResource allocation: The plan helps allocate resources effectively, including budgeting for data management tools and personnel. This foresight can prevent unexpected costs and ensure data management tasks are adequately resourced.\nData security and privacy: The DMP outlines measures for safeguarding sensitive or confidential data, ensuring compliance with privacy regulations and ethical guidelines. This includes specifying access controls, encryption methods, and protocols for data anonymization.\nData sharing and reproducibility: By detailing how data will be shared and made available, the DMP supports transparency and collaboration. This includes specifying data formats, metadata standards, and repositories for public or restricted access, enhancing research findings’ reproducibility.\nLong-Term preservation: The DMP addresses strategies for long-term data preservation, including backup procedures, archival formats, and storage solutions. This ensures that valuable data remains accessible and usable beyond the immediate project duration.\n\nMost funders require researchers to submit a Data Management Plan (DMP) and their research proposal as a prerequisite for consideration. However, developing a DMP offers substantial benefits beyond meeting this requirement. It serves as a crucial tool for all researchers, aiding them in anticipating resource needs, exploring available services, and strategically planning for data management throughout the project’s life cycle. For example, a researcher preparing to conduct interviews can use the DMP to identify and utilize institutionally supported transcription services and secure storage solutions.\nBy anticipating these needs early on, the researcher can allocate appropriate budget and resources, ensuring these essential services are available when required. Additionally, the DMP helps the researcher create a structured plan for managing qualitative data, such as organizing interview transcripts, coding data, and maintaining a detailed audit trail of data analysis decisions.\nBroadly, the DMP should cover the following topics:\n\nThe types of data you expect to collect,\nHow those data will be documented and organized,\nHow the data will be stored and kept secure, and\nHow will the data be shared (or why not) and stored for the long term?\n\nThe ultimate goal is that researchers will make more informed decisions on how to produce and share data satisfying the FAIR principles, which essentially aim to ensure that the data is:\n\nFindable: It is published on a stable location and indexed\nAcessible: It can be easily retrieved\nInteroperable: It can be read by humans and machines\nReusable: It has clear usage licenses and good enough documentation for interpretation\n\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/CT8P-D5MK).\nResearchers studying Indigenous communities should pay special attention to an additional set of principles, named CARE, an acronym for a set of purpose-oriented principles for Indigenous Data Governance, which aims to help advance Indigenous innovation, sovereignty, and self-determination.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/3ZHR-6JAG).",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "02_dmp.html#crafting-a-dmp-for-qualitative-research",
    "href": "02_dmp.html#crafting-a-dmp-for-qualitative-research",
    "title": "Data Management Planning",
    "section": "Crafting a DMP for Qualitative Research",
    "text": "Crafting a DMP for Qualitative Research\nFortunately, creating a DMP is simplified with the help of appropriate templates. The DMPTool is an excellent resource for researchers to develop customized data management plans tailored to specific disciplines and funding agencies. The handout below outlines is more information about it:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/3HFE-6X7U).\nTo get started with the DMPTool, follow these steps:\n\nVisit DMPTool in your web browser.\nOn the DMPTool homepage, enter your institutional email address on the right side and click “Continue.”\nThe tool will automatically recognize your affiliation. Click the “Sign in with Institution (SSO)” button to be redirected to the EID sign-in page and complete the sign-in process.\n\nIt’s important to highlight that the DMPTool is free for everyone. Non-UCSB affiliates can also be added as collaborators and reviewers if desired. We encourage you to log in with your UCSB account to access tailored guidance and resources to help you craft your plan.\nThe DMPTool offers a wide range of templates tailored to specific funder requirements and assists you in structuring your Data Management Plan (DMP) according to these templates. It guides you through the process with targeted questions, ensuring that all necessary components are addressed. Once completed, the tool generates a well-organized and professionally formatted DMP.\nThe Qualitative Data Repository (QDR) has developed a specialized data management checklist for qualitative researchers to ensure that their Data Management Plans (DMPs) address all critical aspects. The checklist includes topic-specific examples from qualitative research where applicable, and offers practical tips based on our extensive experience advising researchers on their DMPs. You can use this checklist proactively as a guide while drafting your DMP, or as a tool to review and confirm that an existing DMP comprehensively covers all necessary elements. Using the DMPTool in tandem with QDR’s checklist can provide additional clarity and help you navigate the various topics effectively. This combination ensures that your DMP is comprehensive, meets all requirements, and is easy to understand.\nFor future reference, you may find valuable insights by consulting the list of the 11 winning DMPs from the DMPTool Qualitative Data Competition. These exemplary DMPs showcase best practices and innovative approaches in managing qualitative data, offering a valuable benchmark for developing your own plan.",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "02_dmp.html#update-as-you-progress",
    "href": "02_dmp.html#update-as-you-progress",
    "title": "Data Management Planning",
    "section": "Update as You Progress",
    "text": "Update as You Progress\nDMPs aren’t meant to collect dust; they should be treated as living documents that evolve alongside your project. To ensure they stay relevant and accurate, it’s crucial to implement version control. Please update the DMP regularly to reflect any changes in your project and track these revisions to maintain a clear history of modifications. This way, you can easily reference past versions, understand the evolution of your project, and ensure that everyone involved is working with the most current information.\nEthical considerations are paramount in any research, but they are significant in qualitative research, where interactions and data collection are often more personal and complex. It’s vital to address these considerations early in the research process and incorporate them into the Data Management Plan (DMP). For qualitative research, the DMP should be particularly detailed in outlining these ethical aspects. This includes obtaining informed consent from participants, ensuring confidentiality and privacy, managing potential biases, and being transparent about data use. By embedding these considerations into a well-defined DMP, you create a framework that not only supports responsible and respectful research practices but also anticipates the resources and services needed for compliance.\nWhen working with data from human subjects, ethical considerations are particularly complex due to their direct impact on individuals’ privacy, rights, and well-being. In the next episode, we will delve deeper into the ethical dimensions of qualitative research, highlighting key considerations and essential steps researchers should take when engaging with human subject data.\n\nRecommended/Cited Sources:\nQualitative Data Repository (2017). Data Management Checklist. https://qdr.syr.edu/drupal_data/public/QDR%20-%20Data%20Management%20Checklist.pdf",
    "crumbs": [
      "Planning & Collecting",
      "Data Management Planning"
    ]
  },
  {
    "objectID": "12_reusing.html",
    "href": "12_reusing.html",
    "title": "Reusing QHS Data",
    "section": "",
    "text": "Sharing well-documented data and connecting it to related research is crucial for maximizing reuse potential. With the tips we’ve provided and the support of QualCoder for exporting codebooks and reports, Sarah can easily share her project with the research community. This ensures that her work is valuable within her field and accessible to other disciplines. By adopting this approach, she enhances the reuse value of her data and fosters collaboration across various research areas!\nResearch data can be reused in various ways, as outlined in the handout below:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/U4D8-UYFR).\nThe UK Data Archive highlights more specific applications of qualitative data secondary analysis:\n\nDescription: Previous research can be utilized to describe the attributes, attitudes, and behaviors of individuals, societies, groups, or organizations during the original project period.\nComparative Research: This approach enables comparisons over time or among different social groups or regions.\nReanalysis: Involves posing new questions to the data and interpreting it in ways not addressed by the original researchers. This could include exploring different themes or topics. The richness and contextual detail of the raw data enhance the potential for fresh insights, without attempting to undermine previous analyses.\nResearch Design and Methodological Advancement: This approach focuses on designing new studies or developing methodologies and research tools. Researchers can gain valuable insights by examining sampling methods, data collection strategies, and topic guides. While researchers often publish sections on methods, their fieldwork diaries and analytic notes can provide deeper context about the research’s development.\nLearning and Teaching: Both classic and contemporary studies serve as valuable case materials for teaching research methods and substantive topics across various social science disciplines.\n\nWhen considering various approaches to data reuse, researchers should ask the following questions:\n\nWhat are the purposes of reuse?\nDo you fully understand the licenses or any restrictions applied to your data of interest?\nAre the new questions similar to or significantly different from those posed by the original researchers?\nDo you fully understand the study’s limitations?\n\nSome of these answers will also help to inform a Data Request for controlled access data.\n\nResponsibilities\nWe have talked about the responsibilities of researchers sharing data, but what are the responsibilities of those reusing pre-existing research data?\nResearchers interested in reusing QHS data need to pay special attention to the following:\n1. Proper Attribution: Acknowledge the original creators of the data by giving them credit, thereby respecting their contributions and intellectual property rights.\n2. Compliance with Licensing: Familiarize yourself with and follow the terms of any licenses associated with the data, ensuring your usage aligns with the specified conditions.\n3. Verification of Data Quality: Evaluate the reliability and quality of the data before using it to ensure it meets your intended purpose.\n4. Transparency in Use: Document how you used the data, including any modifications, to maintain transparency and facilitate replicability.\n5. Ethical Considerations: Be aware of ethical issues, such as privacy concerns and potential biases in the data, and take measures to address any negative impacts.\n6. Data Security: Safeguard the data, especially if it contains sensitive information, to prevent unauthorized access or misuse.\n7. Sharing Findings: Share your results and insights about utilizing the data, contributing to the community and promoting further research.\n\n\nCitation & Attribution\nData citation and attribution are vital, especially when it comes to recognizing the hard work of researchers like Sarah in her digital influencers project. Since she invested significant effort in gathering and analyzing her data, it’s essential that she receives proper attribution to acknowledge her contributions to the topic. This not only upholds academic integrity but also enhances the credibility and visibility of her research.\nThe good news is that most data repositories offer a recommended citation alongside a persistent identifier, such as a DOI, simplifying referencing datasets. If a citation is not available, we recommend following the guidelines outlined in this handout:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/VHW9-9EPQ)",
    "crumbs": [
      "Reusing",
      "QHD Secondary Analysis"
    ]
  },
  {
    "objectID": "07_analysis.html",
    "href": "07_analysis.html",
    "title": "Analyzing Qualitative Data",
    "section": "",
    "text": "Qualitative data can be explored and re-analyzed to uncover hidden meanings and deeply unfold historical, social, and cultural contexts and their entanglement with human subjects’ attitudes, behaviors, and opinions. A crucial contextual issue regarding qualitative data concerns the ‘traces’ left by different perspectives on the material. Texts, whether from interviews, social media, or embedded in an artifact, are not just produced under certain material conditions rooted within socio-cultural contexts, but they are also produced to do something and become part of the context to be understood.",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "07_analysis.html#the-role-of-contextuality",
    "href": "07_analysis.html#the-role-of-contextuality",
    "title": "Analyzing Qualitative Data",
    "section": "",
    "text": "Qualitative data can be explored and re-analyzed to uncover hidden meanings and deeply unfold historical, social, and cultural contexts and their entanglement with human subjects’ attitudes, behaviors, and opinions. A crucial contextual issue regarding qualitative data concerns the ‘traces’ left by different perspectives on the material. Texts, whether from interviews, social media, or embedded in an artifact, are not just produced under certain material conditions rooted within socio-cultural contexts, but they are also produced to do something and become part of the context to be understood.",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "07_analysis.html#a-few-words-about-secondary-analysis",
    "href": "07_analysis.html#a-few-words-about-secondary-analysis",
    "title": "Analyzing Qualitative Data",
    "section": "A Few Words about Secondary Analysis",
    "text": "A Few Words about Secondary Analysis\nArchived qualitative data offers opportunities for reanalysis, reinterpretation, and comparison with existing and newly collected data sources. While many steps in secondary analysis parallel those of primary data analysis—such as data processing, analysis, and quality control—unique challenges arise, particularly in aligning the archived data with the specific objectives of the new study and assessing the data’s value in that context.\nThe success of secondary analysis is also heavily dependent on the quality and comprehensiveness of the accompanying documentation, which must provide detailed context about the original data collection, including the methodology, sampling, and any potential biases. Without this, the data may be misinterpreted, leading to inaccurate conclusions or limiting its applicability to new research questions. In future hands-on activities of this course, we will reuse pre-existing data and assess the documentation provided. Later, we will also explore recommendations on what type of metadata and documentation should be preserved and archived along with the data.",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "07_analysis.html#understanding-qualitative-data-analysis-methods",
    "href": "07_analysis.html#understanding-qualitative-data-analysis-methods",
    "title": "Analyzing Qualitative Data",
    "section": "Understanding Qualitative Data Analysis Methods",
    "text": "Understanding Qualitative Data Analysis Methods\n\nCoding & Themes\nPicture this: a treasure trove of data brimming with stories, emotions, and insights waiting to be uncovered. That’s the realm of qualitative data analysis — a journey through the complexities of human attitudes, behaviors, and perceptions to unearth the gems that lie within.\nQualitative analysis is about finding patterns, unfolding both explicit and implicit attitudes, behaviors, and beliefs, understanding meanings, and making sense of findings concerning the research questions at hand.\nThis process involves identifying and assigning “codes,” which are specific labels applied to small pieces of data, such as text fragments, attributes of objects or photographs, or key topics in video recordings. These codes act as foundational building blocks that, when further developed and refined, can combine to form “themes.” Themes are broader concepts that emerge from grouping related codes, offering a deeper interpretation of the data. It is an outcome of coding, categorization, and analytic reflection. In other words, codes provide detailed elements that contribute to the development of broader themes. Coding is the initial step of analyzing data by assigning labels to relevant sections while theming involves interpreting and grouping codes to identify recurring patterns and meanings.\n\nLet’s imagine you’ve conducted a study on students’ perceptions of online learning. The responses you collected highlight various challenges participants face, such as:\n\nChallenges for active participation in discussions\nDifficulties staying focused and motivated\nFeeling of isolation and lack of social interactions with other students and instructors\nInternet connectivity and bandwidth issues\nLearning Management System (software-related) problems\n\nAssigning Codes\nTo analyze these responses effectively, you can assign short codes to capture the key aspects of the student’s experiences:\n\nEngagement: Refers to the challenges students face in actively participating in discussions.\nFocus & Motivation: Represents students’ difficulties in maintaining concentration and motivation.\nIsolation: Reflects feelings of loneliness or disconnection from peers and instructors in the online learning environment.\nTechnical Issues: Encompasses problems related to internet connectivity, bandwidth limitations, and software malfunctions.\n\nDeveloping Themes\nOnce you have your codes, you can group them into broader themes that provide deeper insights into the students’ experiences:\n\nBarriers to Active Participation: This theme includes the code “Engagement” and “Isolation” and highlights the obstacles preventing students from joining discussions effectively.\nChallenges in Learning Environment: This theme combines “Focus and Motivation” with “Technical Issues,” reflecting how psychological and technical difficulties impact students’ learning experiences.\n\n\n\n\n\n\n\nCan we start with themes?\n\n\n\n\n\nEvery research project begins with some foundational knowledge, whether drawn from existing literature or from the researchers’ assumptions developed while exploring their topic and creating data collection instruments. So, researchers often have general categories in mind. However, these initial categories are unlikely to be fully refined or precise at the outset. This is where multiple iterations and deeper engagement with the data become essential.\n\n\n\n\n\nApproaches to Data Analysis\nResearchers use various methods to extract codes and develop themes from qualitative data, such as thematic analysis, content analysis, comparative analysis, and discourse analysis, to unearth and understand the complexities of human experiences, attitudes, behaviors, and perceptions captured in qualitative data. The goal is to generate rich and nuanced understandings of phenomena rather than producing numerical summaries or generalizable findings. These methods have been used to inform theory development, policy, and practice across disciplines and research domains and applied to various data sources, including written documents, social media posts, news articles, advertisements, photographs, videos, and interviews.\n\nThematic analysis: perhaps the most common method for qualitative data analysis, it allows researchers to spot overarching topics and main themes in the data that uncover recurring ideas, topics, or concepts.\nContent analysis: digs deeper into themes to see how often they appear. It’s like zooming in to see the details. Content analysis involves systematically analyzing and interpreting the content of textual, visual, or audio materials to uncover patterns, trends, or meanings. It focuses on the specific elements within the content, such as words, phrases, images, or themes, rather than the broader themes or concepts.\nComparative analysis: delves into the intricate web of causal relationships between events and outcomes across diverse cases. By scrutinizing the nuances and variations, it focuses on causal relationships between events and outcomes in different cases.\n\nDiscourse analysis helps us understand how language reflects different ideas and cultures. It focuses on spoken or written conversational language.\n\nSentiment analysis: is a branch of discourse or content analysis particularly interested in determining whether the emotional tone of a message or discourse (speech or written) is positive, negative, or neutral or exploring a broader spectrum of sentiments. Depending on the corpus of interest, it can also be heavily computational and quantitative-oriented.\n\n\nThematic Analysis\nOur practical exercises will focus on the most common approach; thematic analysis. So, here is a recommended workflow we suggest you follow for this method:\n\n\n\nSource: Sendze (2019) adapted from Braun, V., & Clarke, V. (2006).\n\n\nWhether you’re new to research or a pro, qualitative analysis is an active process of reflexivity in which your subjective experience and pre-knowledge about the phenomenon of interest inform your process of making sense of the data and finding patterns and relationships. It is all about close familiarization with the data, categorizing, discovering, reviewing, and iterating until you find meaningful insights to inform your research questions before you can draw conclusions and articulate your findings.\nData can be arranged and coded following two types of approaches: a more inductive (bottom-up) approach, where insights and themes emerge more organically and directly from the data without prior assumptions, or even a combination of both as researchers engage with and learn more details from the data. Or a deductive (top-down) approach, where researchers apply preexisting theoretical frameworks or concepts to the data.\n\n\n\nInductive vs. Deductive Coding\n\n\nAdapted from: https://delvetool.com/blog/deductiveinductive\nThe top-down approach begins with a specific theoretical framework or research question. Data collection and analysis are guided by preconceived hypotheses, focusing on confirming or refuting these hypotheses. While it provides more direction to the research process, it may limit alternative perspectives or emergent themes not accounted for in the initial framework. In contrast, the bottom-up approach is exploratory and highly flexible as there are no predetermined categories or hypotheses; it has its basis in grounded theory, meaning that themes emerge directly from the data through open coding and analysis, allowing a wide range of topics and potential unexpected insights to be extracted from the data.\nIt’s important to remember that, although it’s helpful to understand the differences between these two approaches, they can actually work well together. Combining them allows you to take advantage of each method’s strengths, leading to a richer and more nuanced analysis that incorporates real-world data and theoretical insights.\nCheck our handout for more information on these two approaches:\nThis handout provides a compilation with some helpful tips:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/4L6T-4ND5).\n\n\n\n🧠 Knowledge Checking\nBased on our discussion about bottom-up versus top-down approaches to coding, please answer the following questions:\n\nWhat is the primary focus of top-down coding?\n\n\n\n\n ✓Applying pre-existing categories or frameworks to the data.\n\n\n ✗Identifying themes that emerge directly from the data.\n\n\n ✗Allowing codes to develop organically without prior assumptions.\n\n\n ✗Analyzing data without an structured approach.\n\n\n\n\n\n\nWhich of the following best describes bottom-up coding?\n\n\n\n\n ✗It begins with hypotheses that guide the analysis.\n\n\n ✗It utilizes existing literature to create codes.\n\n\n ✗It follows a structured approach to analyze the data.\n\n\n ✓It starts with the raw data and develops codes from specific observations.\n\n\n\n\n\n\nWhich of the following scenarios is most suitable for using top-down coding?\n\n\n\n\n ✗A study aiming to discover new patterns in data.\n\n\n ✓A study testing a specific theory or framework.\n\n\n ✗Explore a new topic, not yet well-established in the literature.\n\n\n\n\n\n\n\n\n\n\n\nA Note about Saturation\n\n\n\n\n\nHow many interviews are enough? Determining how many qualitative interviews are sufficient is a complex question that often yields nuanced answers.\nData saturation is frequently cited as a measure of quality in qualitative research and used to justify purposive small samples. It is primarily reliant on the point at which little new information is obtained, known as thematic saturation. In theory, thematic saturation occurs when interviews or observations start lacking variability to reveal only recurring themes, indicating that further data collection is unlikely to yield new insights. When researchers observe a plateau in these findings, it often signals that they may have reached an adequate sample size.\nHowever, assessing saturation is not straightforward and is much left to interpretation, as it involves subjective judgments about rigor, precision, and confidence. Often, researchers may claim to have achieved saturation to meet certain criteria without adequately explaining what it entails in their specific context or how they reached that conclusion.\nThis lack of justification can undermine the transparency and credibility of qualitative studies. Data saturation is often insufficiently examined and can be problematic and contradictory when applied broadly to qualitative research. It might be more appropriate to reserve the concept of data saturation for grounded theory, where there is a clear framework for its application.\nWhile there is no consensus on minimal sample sizes for qualitative research. There is a general recommendation of 6–30 interviews for most qualitative research projects.\n\n\n\n\n\n\n\n\n\nTime to Practice! 💪🏼\n\n\n\n\n\nOptional\nNow that we’ve gained some knowledge about coding, let’s explore how we can put these concepts into practice. In pairs, access the worksheet and follow the instructions.\n\n\n\nWe will get more into this in later episodes, where we will have a chance to perform coding assisted by a QDA open and free software.\n\nRecommended/Cited Sources:\nGuest, G., Namey, E., & Chen, M. (2020). A simple method to assess and report thematic saturation in qualitative research. PloS one, 15(5), e0232076. https://doi.org/10.1371/journal.pone.0232076\nvan Rijnsoever F. J. (2017). (I Can’t Get No) Saturation: A simulation and guidelines for sample sizes in qualitative research. PloS one, 12(7), e0181689. https://doi.org/10.1371/journal.pone.0181689",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding Analysis"
    ]
  },
  {
    "objectID": "00_about.html",
    "href": "00_about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "00_about.html#ways-we-can-help-you",
    "href": "00_about.html#ways-we-can-help-you",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data in a more effective and reproducible way through:\n\nConsultations\nLong-term engagements\nInstructional workshops.\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided resources, data management systems and curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), as well as sustainable and preservable, and that researchers receive scholarly credit for sharing and publishing their data.\nWe recommend you explore the Research Computing and Data website (https://rcd.ucsb.edu) maintained by our department for campus-wide tools, recommendations, events, communities and learning resources.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "04_data-interviews.html",
    "href": "04_data-interviews.html",
    "title": "Interview Data",
    "section": "",
    "text": "Interviews entail direct exchanges between investigators and participants, facilitating a thorough exploration of subjects’ experiences, perspectives, and opinions. Through the use of open-ended questions and probing techniques, researchers can uncover richer and more nuanced insights by delving deeply into firsthand narratives that illuminate the research phenomenon. This depth of understanding is often unattainable through other research methodologies.",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "04_data-interviews.html#interview-techniques",
    "href": "04_data-interviews.html#interview-techniques",
    "title": "Interview Data",
    "section": "Interview Techniques",
    "text": "Interview Techniques\nThere are three primary types of interview techniques:\n\nStructured: In this approach, the interviewer follows a set of predefined questions in a specific order. Although this format offers less flexibility, it ensures that all respondents receive the same questions in the same manner and order, leading to more consistent data collection and easier comparison of responses.\nSemi-structured: This method focuses on exploring a few issues in moderate detail, allowing researchers to expand their understanding to some extent. Semi-structured interviews offer the advantage of maintaining objectivity while enabling participants to share their perspectives and opinions. Researchers typically develop an interview guide with targeted open questions to steer the conversation.\nIn-depth/unstructured: This approach aims to delve into a person’s subjective experiences and feelings regarding a specific topic. Such interviews are commonly employed to explore emotive subjects. Researchers craft an interview guide with selected open questions, but participants have more influence over the direction of the conversation compared to semi-structured interviews. In-depth interviews prioritize participants’ lived experiences and are frequently utilized in phenomenology studies.\n\nIt is beyond the scope of this course to cover interview techniques and the actual process of data collection, best approaches to choose one technique over another or strategies for developing interview protocols and conducting interviews. For that we recommend Gubrium & Holstein’s (2001) handbook listed below.",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "04_data-interviews.html#transcription-considerations",
    "href": "04_data-interviews.html#transcription-considerations",
    "title": "Interview Data",
    "section": "Transcription Considerations",
    "text": "Transcription Considerations\nTranscription is an essential process that converts spoken data into written text, capturing the nuances of participants’ responses during interviews or discussions. This detailed record is crucial for researchers as it provides a complete and accurate account of the data, enabling thorough analysis. In academic research, two most common types of transcription are verbatim and intelligent:\n\nVerbatim Transcription: This method includes every utterance, such as filler words or what some call response tokens (“um,” “uh,” “you know”), involuntary vocalizations such as coughing, sneezing, burping, sniffing, laughing, and even non-verbal cues (gestures and face expression) when the interview is film recorded, ensuring that every detail of the conversation is translated into text. This approach preserves the exact way participants express themselves, which can be important for analyzing language use and speech patterns.\nIntelligent Transcription: This approach focuses on the core content of the conversation by omitting filler words and redundant elements. It presents the dialogue in a more readable format while retaining the essential meaning, making it easier to interpret and analyze the main points of the discussion.\n\nTo assist this process, researchers can leverage dedicated transcription programs such as Express Scribe that allows easy control of playback speed and navigation through audio segments or even use automatic transcription services such as Otter.ai. Regardless of the chosen tool, we advise researchers to be cautious about technologies that automatically upload recordings for transcription. These recordings will likely contain sensitive and personal information that could be compromised if not handled properly. Therefore, we suggest you:\n\nVerify if the transcription company encrypts your data during transmission and at rest. Most services do, but if you find one that doesn’t, it’s wise to consider another provider.\nPrioritize security and privacy when evaluating new technologies. Check the transcription service’s privacy policy and terms of use to understand who can access your data, including potential access by employees or third parties.\nDo not let your project data linger online. After you download your transcripts and store them in a secure location, please be sure to delete them from the transcription service’s website. This helps protect your information from unauthorized access in case the service experiences a breach.\n\nTranscription represents verbal narratives in written text, which can affect how data are conceptualized. Instead of being viewed as a behind-the-scenes task, this process allows in-depth reflection and honoring the research process and participant’s voice. (Oliver, Serovich & Mason, 2005).\nWhile automated transcription and third-party services can expedite data processing, researchers might prefer to transcribe interviews themselves. Depending on the project’s scope, manual transcription can be an effective way to ensure confidentiality and privacy, maintain better control over data quality, and become more familiar with the content. This hands-on approach allows researchers to observe subtleties in language, tone, and expression that may be missed in automated transcriptions, leading to deeper insights into participants’ thoughts and feelings.\n\nWe won’t have time to cover the transcription process in detail, but here is a good guide with things to remember when you are transcribing audio recording.",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "04_data-interviews.html#what-to-do-with-recordings-after-transcription",
    "href": "04_data-interviews.html#what-to-do-with-recordings-after-transcription",
    "title": "Interview Data",
    "section": "What to Do with Recordings After Transcription?",
    "text": "What to Do with Recordings After Transcription?\nWhat’s the next step once your transcriptions are complete and the data is in text form? Should you keep those audio recordings or dispose of them? While some research teams opt to delete the recordings right away, others suggest keeping them for potential future use or inspectionability.\nAudio recordings are considered highly identifiable private information. Accents, speech patterns, and vocal signatures can reveal participants’ identities. Therefore, in addition to obtaining formal consent from participants before recording, it’s crucial to disclose how the recordings will be handled, who will have access to them, and the procedures for their eventual destruction.\nThe question of whether to destroy audio or video recordings remains debated. Some advocate for retaining these recordings to ensure that the original data can be reviewed in case of misconduct allegations and for historical value. Still, special consideration must be given to studies involving vulnerable populations, and ethical guidelines should be followed to balance transparency with privacy. Also, considerations should be made about costs and technical requirements for retaining data indefinitely and maintaining its persistence. While there is no one-size-fits-all approach to data retention, researchers should consult with local IRB for recommended practices and follow promises made to research participants in agreed protocols and informed consent.\n\nRecommended/Cited Sources:\nBailey, J. (2008). First steps in qualitative data analysis: transcribing, Family Practice, Volume 25, Issue 2, April 2008, Pages 127–131, https://doi.org/10.1093/fampra/cmn003\nGubrium, J. F., & Holstein, J. A. (Eds.) (2001). Handbook of interview research. SAGE Publications, Inc., https://doi.org/10.4135/9781412973588\nMcCrae, N., & Murray, J. (2008). When to delete recorded qualitative research data. Research Ethics, 4(2), 76-77. https://doi.org/10.1177/17470161080040021\nOliver, D. G., Serovich, J. M., & Mason, T. L. (2005). Constraints and Opportunities with Interview Transcription: Towards Reflection in Qualitative Research. Social forces; a scientific medium of social study and interpretation, 84(2), 1273–1289. https://doi.org/10.1353/sof.2006.0023\nResnik, D. B., Antes, A., & Mozersky, J. (2024). Should Researchers Destroy Audio or Video Recordings?. Ethics & Human Research, 46(2), 30-35, https://doi.org/10.1002/eahr.500205",
    "crumbs": [
      "Planning & Collecting",
      "Interview Data"
    ]
  },
  {
    "objectID": "08_tools.html",
    "href": "08_tools.html",
    "title": "Computer-aided Qualitative Data Analysis (CAQDA) tools",
    "section": "",
    "text": "Qualitative data analysis tools vary in terms of features, usability, and costs and researchers should choose the tool that best fits their needs and preferences. It is essential for researchers to have a good understanding of qualitative research principles and methodologies, to then choose the most appropriate tool for their projects.\nThese tools may range from analog methods, such as post-its notes and manual annotation techniques for categorization and color-coding or highlighting, to more advanced computer-aided qualitative data analysis (CAQDA) software that allows for similar strategies in a more automated way and with the aid of advanced features.\n\n\n\nPost-it Coding - Source: Musings (2013)\n\n\nNot rarely, researchers use MS Word or Google docs to annotate and analyze qualitative data. While this process may satisfy some small-scale projects’ needs, there are some disadvantages associated with this process. Can you guess what are those?\n\n\n\nMS Word Coding Example - Source: hdl.handle.net/2139/39482\n\n\n\n\n\n\n\n\nDiscussion: What are the pitfalls of these approaches to coding?\n\n\n\n\n\nWhile the above mentioned approaches may suffice for small-scale projects or initial exploratory tasks, we caution against their use in more research-intensive endeavors. Why? They tend to be considerably more time-consuming and labor-intensive, particularly when managing extensive datasets or coordinating multiple coders. Implementing changes can prove challenging, especially post-coding completion and categorization of data into themes or categories. Revising codes or themes often requires extensive data re-analysis and re-organization, resulting in inefficiencies and project delays. Additionally, ensuring coding consistency and reliability across diverse coders can pose significant challenges, potentially leading to conflicts or discrepancies in interpretation. Collaboration may inadvertently lead to overlapping efforts or disagreements. Furthermore, these methods lack the capability for advanced analysis and visualizations, restricting the depth of insights that can be gleaned from the data.\n\n\n\nWe vouch for the use of Computer-aided Qualitative Data Analysis (CAQDA) Tools given their capability to reduce ambiguities and provide visibility into all instances of a code. Such tools facilitate the seamless updating, merging, and splitting of codes, categories, and themes, ensuring these changes reflect across the board and allow multiple collaborators to concurrently engage in analysis, fostering efficient teamwork. Below we describe some advantages:\n\nCoding and annotation: these tools offer features to code data segments, identifying themes and patterns, with options for flexible coding schemes and annotations as you progress.\nCollaboration and teamwork: these tools facilitate collaborative research efforts by enabling multiple users to work on projects simultaneously, promoting communication and data sharing.\nData management: most CAQDA tools have the ability to import, structure, and manage different types of qualitative data (text, image, videos) in a variety of formats, simplifying data access and retrieval. They also support the export of codebooks and other reports that are important to allow for interpretation and transparency of results.\nData retrieval and querying: such tools allow for search for specific data segments based on coding or other criteria, aiding in the retrieval of pertinent information and a more efficient identification of specific relevant excerpts and quotes to be included in your reports.\nData visualization: most CAQDA tools provide embedded features to generate charts, diagrams, and graphs to assist the visual exploration and interpretation of the data.\nIntegration with other software: such tools integrate with various qualitative research applications, such as transcription or survey tools, to enhance the research workflow.\n\nChoosing a tool will depend on the specific needs of your project, including the types of data you are analyzing, the complexity of the analysis, and whether collaboration is a key requirement.\nThe UCSB Library offers limited NVivo and MaxQDA licenses to campus affiliates via the DREAMLab. These two programs together with Atlas.TI are the leading and most widely recognized proprietary solutions in the market. They are well-known for their comprehensive features, flexibility, and ability to handle complex qualitative research projects.\nSo why not focus primary on those well-established proprietary solutions? 1. Proprietary tools already have comprehensive documentation and online tutorials available. 2. UCSB can only support a limited number of seats per quarter due to the high licensing costs. Additionally, some advanced collaboration and synchronization are not supported for our institutional license. 3. The learning curve to use these tools can be steep for some researchers, especially since some features may be less relevant and underutilized. 4. While we can help you navigate and get started with UCSB-licensed proprietary tools if you choose so, we want promote of more democratic, open-source and free alternatives to help researchers organize, code and analyze qualitative data.\nBelow we compare the top most popular open-source free tool: Qualcoder and Taguette.\nComparison of Qualcoder vs. Taguette for Qualitative Data Analysis\n\n\n\n\n\n\n\n\nFeature\nQualcoder\nTaguette\n\n\n\n\nAccess\nDesktop-based qualitative data analysis tool\nWeb-based qualitative data analysis tool\n\n\nData Formats\nText, audio, video, images, PDFs\nPrimarily text (e.g., plain text, PDFs, Word)\n\n\nCoding\nManual coding, automatic coding, hierarchical coding, AI-based functionality*\nManual coding, tagging\n\n\nMemoing\nCreate and link memos to codes and data segments\nCreate and attach notes to codes and text segments\n\n\nAnalysis\nCode frequency tables, text search, code co-occurrence analysis\nBasic query and search functions\n\n\nVisualizations\nCode clouds, coding matrices\nLimited visualization options\n\n\nCollaboration\nDesigned for single-user projects and. manual file sharing. Some options for working together in a team.\nReal-time sharing and editing capabilities\n\n\nStrengths\nComprehensive coding and memoing tools, supports various data formats, active development community\nEasy to use, excellent for collaborative work, accessible from any device\n\n\nLimitations\nLimited collaboration features, less intuitive for new users, limited advanced visualizations\nLimited to text-based data, basic analysis tools, fewer advanced features\n\n\n\n(*) QualCoder AI (beta) experimental version of QualCoder with AI-enhanced functionality (using GPT-4). See: https://github.com/kaixxx/QualCoder/tree/ai_integration\nDespite of its limited collaboration capabilities (as it will be further described), for this offer of the course we will be using QualCoder. This choice was motivated by the fact that this tool offer more robust coding features than Taguette, and that are more similar to advanced options offered by commercial QDA tools.\nIf you are interested in learning more about Taguette, here is a hands-on workshop to help you get started: Open Qualitative Research.",
    "crumbs": [
      "Analyzing & Documenting",
      "Computer-Aided Analysis Tools"
    ]
  },
  {
    "objectID": "08_tools.html#qualitative-data-analysis-tools",
    "href": "08_tools.html#qualitative-data-analysis-tools",
    "title": "Computer-aided Qualitative Data Analysis (CAQDA) tools",
    "section": "",
    "text": "Qualitative data analysis tools vary in terms of features, usability, and costs and researchers should choose the tool that best fits their needs and preferences. It is essential for researchers to have a good understanding of qualitative research principles and methodologies, to then choose the most appropriate tool for their projects.\nThese tools may range from analog methods, such as post-its notes and manual annotation techniques for categorization and color-coding or highlighting, to more advanced computer-aided qualitative data analysis (CAQDA) software that allows for similar strategies in a more automated way and with the aid of advanced features.\n\n\n\nPost-it Coding - Source: Musings (2013)\n\n\nNot rarely, researchers use MS Word or Google docs to annotate and analyze qualitative data. While this process may satisfy some small-scale projects’ needs, there are some disadvantages associated with this process. Can you guess what are those?\n\n\n\nMS Word Coding Example - Source: hdl.handle.net/2139/39482\n\n\n\n\n\n\n\n\nDiscussion: What are the pitfalls of these approaches to coding?\n\n\n\n\n\nWhile the above mentioned approaches may suffice for small-scale projects or initial exploratory tasks, we caution against their use in more research-intensive endeavors. Why? They tend to be considerably more time-consuming and labor-intensive, particularly when managing extensive datasets or coordinating multiple coders. Implementing changes can prove challenging, especially post-coding completion and categorization of data into themes or categories. Revising codes or themes often requires extensive data re-analysis and re-organization, resulting in inefficiencies and project delays. Additionally, ensuring coding consistency and reliability across diverse coders can pose significant challenges, potentially leading to conflicts or discrepancies in interpretation. Collaboration may inadvertently lead to overlapping efforts or disagreements. Furthermore, these methods lack the capability for advanced analysis and visualizations, restricting the depth of insights that can be gleaned from the data.\n\n\n\nWe vouch for the use of Computer-aided Qualitative Data Analysis (CAQDA) Tools given their capability to reduce ambiguities and provide visibility into all instances of a code. Such tools facilitate the seamless updating, merging, and splitting of codes, categories, and themes, ensuring these changes reflect across the board and allow multiple collaborators to concurrently engage in analysis, fostering efficient teamwork. Below we describe some advantages:\n\nCoding and annotation: these tools offer features to code data segments, identifying themes and patterns, with options for flexible coding schemes and annotations as you progress.\nCollaboration and teamwork: these tools facilitate collaborative research efforts by enabling multiple users to work on projects simultaneously, promoting communication and data sharing.\nData management: most CAQDA tools have the ability to import, structure, and manage different types of qualitative data (text, image, videos) in a variety of formats, simplifying data access and retrieval. They also support the export of codebooks and other reports that are important to allow for interpretation and transparency of results.\nData retrieval and querying: such tools allow for search for specific data segments based on coding or other criteria, aiding in the retrieval of pertinent information and a more efficient identification of specific relevant excerpts and quotes to be included in your reports.\nData visualization: most CAQDA tools provide embedded features to generate charts, diagrams, and graphs to assist the visual exploration and interpretation of the data.\nIntegration with other software: such tools integrate with various qualitative research applications, such as transcription or survey tools, to enhance the research workflow.\n\nChoosing a tool will depend on the specific needs of your project, including the types of data you are analyzing, the complexity of the analysis, and whether collaboration is a key requirement.\nThe UCSB Library offers limited NVivo and MaxQDA licenses to campus affiliates via the DREAMLab. These two programs together with Atlas.TI are the leading and most widely recognized proprietary solutions in the market. They are well-known for their comprehensive features, flexibility, and ability to handle complex qualitative research projects.\nSo why not focus primary on those well-established proprietary solutions? 1. Proprietary tools already have comprehensive documentation and online tutorials available. 2. UCSB can only support a limited number of seats per quarter due to the high licensing costs. Additionally, some advanced collaboration and synchronization are not supported for our institutional license. 3. The learning curve to use these tools can be steep for some researchers, especially since some features may be less relevant and underutilized. 4. While we can help you navigate and get started with UCSB-licensed proprietary tools if you choose so, we want promote of more democratic, open-source and free alternatives to help researchers organize, code and analyze qualitative data.\nBelow we compare the top most popular open-source free tool: Qualcoder and Taguette.\nComparison of Qualcoder vs. Taguette for Qualitative Data Analysis\n\n\n\n\n\n\n\n\nFeature\nQualcoder\nTaguette\n\n\n\n\nAccess\nDesktop-based qualitative data analysis tool\nWeb-based qualitative data analysis tool\n\n\nData Formats\nText, audio, video, images, PDFs\nPrimarily text (e.g., plain text, PDFs, Word)\n\n\nCoding\nManual coding, automatic coding, hierarchical coding, AI-based functionality*\nManual coding, tagging\n\n\nMemoing\nCreate and link memos to codes and data segments\nCreate and attach notes to codes and text segments\n\n\nAnalysis\nCode frequency tables, text search, code co-occurrence analysis\nBasic query and search functions\n\n\nVisualizations\nCode clouds, coding matrices\nLimited visualization options\n\n\nCollaboration\nDesigned for single-user projects and. manual file sharing. Some options for working together in a team.\nReal-time sharing and editing capabilities\n\n\nStrengths\nComprehensive coding and memoing tools, supports various data formats, active development community\nEasy to use, excellent for collaborative work, accessible from any device\n\n\nLimitations\nLimited collaboration features, less intuitive for new users, limited advanced visualizations\nLimited to text-based data, basic analysis tools, fewer advanced features\n\n\n\n(*) QualCoder AI (beta) experimental version of QualCoder with AI-enhanced functionality (using GPT-4). See: https://github.com/kaixxx/QualCoder/tree/ai_integration\nDespite of its limited collaboration capabilities (as it will be further described), for this offer of the course we will be using QualCoder. This choice was motivated by the fact that this tool offer more robust coding features than Taguette, and that are more similar to advanced options offered by commercial QDA tools.\nIf you are interested in learning more about Taguette, here is a hands-on workshop to help you get started: Open Qualitative Research.",
    "crumbs": [
      "Analyzing & Documenting",
      "Computer-Aided Analysis Tools"
    ]
  },
  {
    "objectID": "11_sharing.html",
    "href": "11_sharing.html",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "",
    "text": "Sharing qualitative data benefits both the scholarly community and researchers in several ways:\n\nFostering Public Trust: Transparency enhances public confidence in research outcomes, vital for securing funding and support for future projects. It allows for verification of claims, reinforcing trust in the research.\nDynamic Research Environment: While qualitative research invites diverse interpretations, sharing data fosters improved research quality through collaborative critique and examination.\nEnabling New Research: Access to shared data inspires innovative analyses, maximizing the scientific value of existing studies.\nMore Effective Use of Resources: Data sharing reduces costs related to new data collection, promoting efficient resource utilization, and minimizing the burden on frequently targeted communities.\nSkill Development for Trainees: It offers students valuable opportunities to learn coding and analysis techniques, enhancing their educational experience.\nReceiving Credit: Sharing data ensures proper attribution, allowing researchers to gain recognition for their work.\nOpportunities for Collaboration: Open data fosters partnerships among researchers, leading to new insights and advancements.",
    "crumbs": [
      "Sharing & Archiving",
      "What and Where to Share?"
    ]
  },
  {
    "objectID": "11_sharing.html#why-sharing-qualitative-data",
    "href": "11_sharing.html#why-sharing-qualitative-data",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "",
    "text": "Sharing qualitative data benefits both the scholarly community and researchers in several ways:\n\nFostering Public Trust: Transparency enhances public confidence in research outcomes, vital for securing funding and support for future projects. It allows for verification of claims, reinforcing trust in the research.\nDynamic Research Environment: While qualitative research invites diverse interpretations, sharing data fosters improved research quality through collaborative critique and examination.\nEnabling New Research: Access to shared data inspires innovative analyses, maximizing the scientific value of existing studies.\nMore Effective Use of Resources: Data sharing reduces costs related to new data collection, promoting efficient resource utilization, and minimizing the burden on frequently targeted communities.\nSkill Development for Trainees: It offers students valuable opportunities to learn coding and analysis techniques, enhancing their educational experience.\nReceiving Credit: Sharing data ensures proper attribution, allowing researchers to gain recognition for their work.\nOpportunities for Collaboration: Open data fosters partnerships among researchers, leading to new insights and advancements.",
    "crumbs": [
      "Sharing & Archiving",
      "What and Where to Share?"
    ]
  },
  {
    "objectID": "11_sharing.html#sharing-with-caring",
    "href": "11_sharing.html#sharing-with-caring",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "Sharing with Caring",
    "text": "Sharing with Caring\nWhen sharing data, researchers should make their best effort to provide complete and good quality documentation to support reuse.\nBefore we dive into what researchers should share and where. Let’s explore something together.\n\n\n\n\n\n\n💭 Discussion: Comparing Data Deposits\n\n\n\n\n\nPlease open the links to the two data deposits below:\nTaherzadeh, O., 2016, “Interview Transcripts”, Interview Transcripts, https://doi.org/10.7910/DVN/4C9KFK/XRREIY, Harvard Dataverse, V1\nKlein, M., 2022. Interview transcripts of addiction therapists and recovering drug service users. Bath: University of Bath Research Data Archive. Available from: https://doi.org/10.15125/BATH-01096.\nCan you spot any differences? Supposing those were both topics related to your research, how likely would you be to reuse one dataset versus another? Why?\n\nContext and Documentation\n\nTaherzadeh (2016): This deposit lacks detailed contextual information about the study, such as the sample, interview questions, study goals, or informed consent details. It is a standalone collection of transcripts.\nKlein (2022): This deposit provides clearer context, the objectives of the research and questions asked, and links to the associated dissertation.\n\nReuse Value\nTaherzadeh (2016) Dataset: Low\n\nThe absence of context and supporting documentation makes it challenging to assess the dataset’s validity, reliability, and relevance to other research. Without knowing the background or how the data was collected, it’s difficult to justify its use in further studies.\n\nKlein (2022): Higher\n\nThe dataset seems to come with comprehensive documentation, including context about the participants and the study goals. This information facilitates a better understanding of how to apply the data effectively in new research, making it much more reusable.",
    "crumbs": [
      "Sharing & Archiving",
      "What and Where to Share?"
    ]
  },
  {
    "objectID": "11_sharing.html#considerations-on-what-to-share",
    "href": "11_sharing.html#considerations-on-what-to-share",
    "title": "Sharing and Archiving Qualitative Data",
    "section": "Considerations on What to Share",
    "text": "Considerations on What to Share\nRemember when we discussed the importance of outlining data-sharing plans in Data Management Plans (DMPs)? At this stage, Sarah could greatly benefit from having a clear strategy for archiving and storing her data. As we discussed earlier, understanding the available options and having at least a rough plan for what will be shared, along with strategies to facilitate the process, is very important. We provided Sarah with recommendations on what to document, and we hope this guidance will empower her to share her research deliverables confidently while adhering to key principles of open practices.\nAlso, it is important to recap the importance of balancing the value of open sharing against the risks of harm associated with the identification of participants, communities, and research sites. The good news is that there are more options in between data being closed and open!\nDepending on your project needs and what was agreed in the informed consent, we recommend you to consider evaluating access control options, they will help you determine which data repository will be most suitable for storing and preserving your project data.\n\nAccess Control Questions\nAccess controls fall into three main categories:\n\nWho can access your data? Access may be limited to qualified researchers, often requiring proof of interest through research proposal, or it may require pre-approval from an Institutional Review Board (IRB) for general requests.\nHow can others access your data? Secure internet connections, along with agreements regarding data storage and destruction, might be required for downloading data. Researchers may sometimes need to access data in person on a secure, offline computer. Hybrid solutions, like ICPSR’s “virtual enclave,” allow remote viewing without data leaving the server.\nWhen can others access your data? Embargoes can temporarily restrict access to protect human participants, often allowing researchers to publish findings before broader access. These embargoes can also facilitate long-term data availability, with set dates for lifting restrictions, as seen in historical archives.\n\n\n\nSharing Levels\n\nOpenly available: data (typically de-identified) shared with no restrictions.\n\nExample: Cunningham, Una; De Brún, Aoife; Mayumi, Willgerodt et al. (2021). Appendices interview formats [Dataset]. Dryad. https://doi.org/10.5061/dryad.q83bk3jg8\n\nSubject to Embargo: a temporary restriction on sharing or publishing data. It means that the data can’t be made public for a set period, usually to protect sensitive information allow for further analysis, or wait for a specific event, such as a formal publication before releasing it.\n\nExample: Ibitoye, Mobolaji; OlaOlorun, Funmilola; Casterline, John B.. 2025. “Demand for Modern Contraception in Sub-Saharan Africa: New Methods, New Evidence”. Qualitative Data Repository. https://doi.org/10.5064/F600CMLO. QDR Main Collection. V1\n\nClosed Access/Metadata Record Only (sensitive data/no consent): a summary and description of a dataset without containing the actual data itself that provides essential information about the dataset’s provenance, structure, and context.\n\nDepending on the research case, access can be provided through a Data Use Agreement (DUA) and involve a data enclave for safe access. These requirements will also depend on IRB and consent form agreements.\n\nData Use Agreement (DUA) required: a contract that outlines the terms and conditions for a recipient to use data from a data owner. It’s specific to a project or study and can include limitations on use, data safeguarding obligations, and privacy rights. Some supplementary files (i.e., codebooks, data collection instrument, selected processed data to reproduce specific figures or support some findings).\n\nExample: Steeves, Vicky; Peltzman, Shira; Kim, Julia; Griesinger, Peggy; Blumenthal, Karl-Rainer. 2020. “Data for:”What’s Wrong with Digital Stewardship: Evaluating the Organization of Digital Preservation Programs from Practitioners’ Perspectives”. Qualitative Data Repository. https://doi.org/10.5064/F6DJRPLK.\n\n\n\n\n\n\n💭 Discussion: What is the value of sharing a metadata record only?\n\n\n\n\n\nA metadata-only record for research data that isn’t openly available enables readers to evaluate whether they want to request access quickly. While a well-crafted Data Availability Statement in journal papers serves a similar purpose, a metadata-only record in a suitable repository offers the benefit of being discoverable through data-focused searches, along with the ability to provide more detailed descriptions through rich, linked, and interoperable metadata.\n\n\n\n\n\nA Note About DAS\nData Availability Statements (DAS) are crucial for the credibility of manuscripts and other published research. They provide interested readers—and sometimes automated algorithms—access to the underlying data supporting your claims, allowing them to verify those assertions or use the data for further research. We suggest following some best practices for crafting statements that are both effective and clear while also complying with funders’ and journal policies’ requirements.\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/3ZHR-6JAG)\n\n\nApplying Access Controls\nImplementing access controls involves a trade-off: while stricter controls reduce misuse risk, they can hinder beneficial access. Though powerful, they should not unnecessarily complicate access to low-risk data. As the principal steward of your data, you ultimately decide on access controls. However, it’s advisable to involve repository staff in this process, as they can highlight potential challenges, ensuring that your data remains accessible and ethically shared in the long run.\n\nSharing de-identified transcripts openly while placing recordings under more stringent access controls.\nDo keep a list of de-identification rules for yourself and your team should you collaborate. This list serves as necessary documentation when you share your data. See, for example, the protocol Thad Dunning and Edward Camp used to de-identify data deposited with the Qualitative Data Repository. This document is separate from the key that links de-identified entries to the individuals or entities interviewed, which should not be included when sharing your data.\nDo check the document properties of files, which may contain identifiers such as original file names identifying interview respondents.\nFinally, do try to strike a balance between keeping your participants’ information confidential and unnecessarily reducing the analytic value of the data by removing too much information. If you are having difficulties striking that balance, you could ask another subject-matter expert for assistance; some repository personnel or data librarians can also provide abstract rules that you can follow.\n\n\n\nWhat data?\nICPSR’s Guide for Sharing Qualitative Data outlines examples of qualitative data sources that may be archived for secondary analysis:\n• Interview methods, including those captured through notes, audio, and video\n\nIn-depth and/or unstructured interviews\nSemi-structured interviews\nFocus group interviews\n\n• Diary studies that are unstructured or use semi-structured writing prompts\n• Observational studies that generate field notes and other text and information\n\nNaturalistic observation of real-world environments (e.g., classrooms, workplaces, healthcare facilities, courtrooms, public spaces)\nParticipant observation, where the researcher becomes an active part of the setting to collect information (e.g., online gaming, community policing, nightclub culture)\nStructured observation is where the research has predefined objectives and a systemic approach to collecting information. This would include case studies.\n\n• Text from available sources\n\nMeeting minutes\nOfficial records Medical records\nNews sources and social media\nExcerpts of copyrighted materials (e.g., literature, film, music)\n\n• Survey methods or questionnaires with substantial open-ended comments\n\n\nOpen formats\nWhy should we prioritize open file formats in our research? Imagine sharing your groundbreaking findings and ensuring that anyone, anywhere, can access and build upon your work without running into compatibility issues. Open formats, offer exactly that—freedom from proprietary software constraints. By choosing open formats, you enhance collaboration and transparency and make your research more sustainable for others and your future self.\nThere is a diversity of open formats available across different types of media that can be of great use to qualitative data researchers, including audio, video, image, and text. Refer to the handout below for some examples:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/W4FL-JDFT)\n\n\nWhere Should You Share Your Project Data?\nThe decision of where to archive data is crucial for ensuring its accessibility, integrity, and long-term preservation. Selecting a stable, certified repository not only safeguards the data against loss or corruption but also enhances its credibility and usability within the research community. Unlike sharing via email, personal communication, or unsecured websites—methods that can lead to data loss, miscommunication, and lack of traceability—certified repositories provide a structured and secure environment for data management.\nSuch repositories adhere to rigorous standards for data storage and access, ensuring that shared data remains discoverable, citable, and protected over time. By thoughtfully choosing the right repository, researchers can maximize the impact of their work, facilitate reproducibility, and contribute to the advancement of knowledge across various fields.\nBeyond support to access controls when required, choosing a repository to archive QHS data, should take into account several factors laid out in the handout below:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/WLF7-WTUC).\n\n\nPreparing Your Data for Submission\nThere are a few required and recommended files that are important to be added to your project package submission.\nRequired:\n\nProcessed de-identified data (e.g., transcripts);\nCodebook;\nCoded Data (supporting excerpts);\nREADME File: an overview of your project, including data sources, their relationships and a brief description of the methods. Here is a customizable README template;\nData Collection Instruments: A sample of instruments used for data collection, such as surveys or interview guides;\nCodebook: the coding framework used, including definitions of codes and categories;\nREADME File: an overview of your project, including data sources, their relationships and a brief description of the methods. Here is a customizable README template.\n\nRecommended:\n\nInformed consent statement(s), if applicable;\nIRB protocol, if applicable;\nStudy protocol or procedures manual, if applicable.\n\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/E7BA-BBYE).\n\n\nLicensing Your Data\nResearch data itself is generally not copyrightable because it consists of facts, figures, and raw information that cannot be considered original creative expression. Copyright protects the unique expression of ideas, such as written texts, artwork, and music, rather than the underlying data or factual content.\nMost data repositories adhere to open licenses such as CC0 (Creative Commons Zero) or CC BY (Creative Commons Attribution) to encourage broad accessibility and reuse of data. These licenses promote the free sharing of knowledge, allowing researchers and practitioners to utilize, modify, and redistribute data without significant restrictions, ultimately fostering collaboration and innovation within the scientific community.\nHowever, researchers may choose to assign different licenses to other creative deliverables and supplementary materials associated with their projects, such as reports, presentations, or multimedia content. For example, Sarah might opt for a CC BY-NC (Attribution-NonCommercial) license for a infographic she created to represent the ethical approaches in social media influencing market, to restrict its use for commercial purpose. This flexibility allows Sarah and the research community at large to balance openness with the need to protect specific aspects of their intellectual property while still contributing to the collective body of knowledge.\nThe handout below provides more insights about licenses, including the Creative Commons family:\n\n\nSource: UCSB Library Data Literacy Series (perma.cc/ET6F-N84X).\n\nRecommended/Cited Sources:\nCampbell R, Javorka M, Engleton J, Fishwick K, Gregory K, Goodman-Williams R. Open-Science Guidance for Qualitative Research: An Empirically Validated Approach for De-Identifying Sensitive Narrative Data. Advances in Methods and Practices in Psychological Science. 2023;6(4). doi:10.1177/25152459231205832\nMyers CA, Long SE, Polasek FO. Protecting participant privacy while maintaining content and context: Challenges in qualitative data De-identification and sharing. ProcAssoc Inf Sci Technol. 2020;57:e415. https://doi.org/10.1002/pra2.415\nDuBois, J. M., Strait, M., & Walsh, H. (2018). Is it time to share qualitative research data?Qualitative Psychology, 5(3), 380–393. https://doi.org/10.1037/qup0000076",
    "crumbs": [
      "Sharing & Archiving",
      "What and Where to Share?"
    ]
  },
  {
    "objectID": "01_qual-data.html",
    "href": "01_qual-data.html",
    "title": "Qualitative Data",
    "section": "",
    "text": "Quantiative vs. Qualitative Data\nQualitative data refers to any non-numeric forms of data that are typically textual, audio, images, or video recordings, which can be transcribed or described. This data type includes field notes, interviews, social media content (e.g., posts, comments), archival documents, and more.\nQualitative data delves into the “why” and “how” of research questions, aiming to understand underlying motivations, behaviors, and experiences. It can complement quantitative data in mixed methods studies by providing deeper insights, context, and understanding of complex research phenomena.\nDespite of its advantages, handling and analyzing qualitative data requires specialized skills, tools, and techniques to overcome the following challenges:\nQualitative researchers should be aware of the above limitations and plan accordingly in order to identify ways to improve the transparency and reusability of their work.",
    "crumbs": [
      "Planning & Collecting",
      "Qualitative Data"
    ]
  },
  {
    "objectID": "01_qual-data.html#scientific-rigor-in-qualitative-research",
    "href": "01_qual-data.html#scientific-rigor-in-qualitative-research",
    "title": "Qualitative Data",
    "section": "Scientific Rigor in Qualitative Research",
    "text": "Scientific Rigor in Qualitative Research\nScientific rigor manifests differently in qualitative research. Unlike quantitative research, which produces numerical results, qualitative research yields findings. These findings consist of insights, themes, and patterns that emerge from analyzing non-numerical data. They offer a descriptive and interpretive understanding of the studied phenomenon and play a crucial role in theory-building and contextual understanding, laying the groundwork for exploring phenomena in depth and inform practical applications.\nA key challenge in qualitative research is establishing the credibility of these findings, as they lack the objective metrics used in quantitative research, such as validity and reliability checks. Also, instead of aiming for generalizability to a broader population, qualitative research provides in-depth insights into specific contexts or groups, enhancing our understanding of the subject matter within those contexts.\nQualitative researchers use the concept of “trustworthiness” as a foundational scientific rigor principle, which is built on four key pillars:\n\nCredibility: the confidence in the truthfulness of the findings.\nTransferability: the extent to which the findings are applicable to other contexts.\nDependability: the consistency of the findings and their potential for replication.\nConfirmability: the degree to which the findings are shaped by the respondents rather than influenced by researcher bias or personal interests.\n\nLater in this course, we’ll cover some practical recommendations to increase the trustworthiness of your qualitative research findings.",
    "crumbs": [
      "Planning & Collecting",
      "Qualitative Data"
    ]
  },
  {
    "objectID": "01_qual-data.html#data-types-in-qualitative-research",
    "href": "01_qual-data.html#data-types-in-qualitative-research",
    "title": "Qualitative Data",
    "section": "Data Types in Qualitative Research",
    "text": "Data Types in Qualitative Research\n Qualitative data encompasses a variety of types, including written text, recordings, imagery, and cultural artifacts. Written text can be found in diaries, personal notes, letters, emails, news articles, official documents, policies, medical records, meeting minutes, and other organizational documents. Recordings consist of audio and video materials, along with their transcriptions. Imagery includes photographs and artwork, while cultural artifacts cover literature, film, music, and products or prototypes that reflect the experiences of research participants and their communities.\nThe sources of qualitative data can be broadly categorized into two groups: direct interaction with human participants and existing materials generated independently of a research protocol. Direct interaction may involve interviews or focus groups, while existing materials can include social media posts, community-produced content, and various organizational records. These diverse sources allow researchers to explore and analyze a wide range of topics.\nMethods of collecting qualitative data involve several approaches. Direct interaction methods, such as interviews and discussions, provide firsthand insights. Content analysis of existing materials—like social media, literature, and news articles—allows researchers to examine public discourse and cultural narratives. Additionally, observing artifacts in context, such as in museums or cultural events, can reveal deeper meanings. Reviewing secondary sources, including official reports and news articles, further enriches the understanding of the phenomenon under study. Together, these methods and sources yield valuable insights into attitudes, behaviors, and cultural norms.\n\n\n\n\n\n\n💭 Discussion: Can you think of any examples where an everyday artifact can be used in research to provide valuable insights about a specific group?\n\n\n\n\n\nA collection of protest signs from a social movement could be one. These signs might initially appear to be simple expressions of demands or slogans. However, analyzing the language, imagery, and symbolism used can uncover underlying values, emotions, and the collective identity of the movement. For instance, the choice of words, the use of humor or anger, and the symbols depicted can reveal insights into the group’s attitudes towards authority, their sense of justice, and the cultural or references that have historically resonate with them. This deeper analysis helps us understand the broader social and political context of the movement.\n\n\n\nQualitative data can be collected through various research methods. Defining the best approach to obtain qualitative data depends on several factors, including the research question(s), the dispersion of research subjects, the nature of the data needed, and the study context. Below, are the most common methods:\n\nDocument analyses: Examination of written or recorded materials to gain insights into past events, attitudes, and experiences.\nOpen-ended digital surveys: Online questionnaires with open-ended questions to collect qualitative data at scale to capture participants’ opinions, experiences, and suggestions.\nFocus Groups: Moderated discussions among a small group of participants representing the targeted population or the community of interest, allowing researchers to observe interactions and gather collective insights on a specific topic.\nObservations: Close monitoring and recording of behaviors, interactions, and phenomena in natural physical or online setting and the context in which those are generated. Observations might be naturalistic - when researchers observe people in their natural setting without interference -, or participatory - when the investigator becomes an active member of the group being observed.\nField Work: Researchers produce annotations based on their observations, reflections, and impressions during fieldwork, capturing nuances and contextual details that other methods may not capture.\nCase Studies: In-depth examination of a single individual, group, or organization, using multiple data sources to explore complex phenomena within real-life contexts.\nEthnography: Immersive, long-term engagement with a particular community or culture, aiming to understand its social dynamics, rituals, and practices through participant observation and interaction. It may also be conducted online to explore communities and cultures formed and active through computer-mediated social interaction.\nOne-to-one Interviews: Direct conversations between the researcher and participants, allowing in-depth exploration of thoughts, experiences, and perspectives. Interviews can be structured, semi-structured, or unstructured and be conducted utilizing various means (phone, online, in-person).\n\nThis course will use qualitative research data from direct interactions between researchers and human participants through interviews. Before we delve into the specifics, it’s crucial to recognize the significance of a data management plan in guiding your research process effectively.\nRecommended/Cited Sources:\nCreswell, J. W., & Creswell, J. W. (2007). Qualitative inquiry & research design: choosing among five approaches (Second edition.). Sage Publications. (UCSB Library Catalog)",
    "crumbs": [
      "Planning & Collecting",
      "Qualitative Data"
    ]
  },
  {
    "objectID": "10_qualcoder-coding.html",
    "href": "10_qualcoder-coding.html",
    "title": "Coding and Documenting with Qualcoder",
    "section": "",
    "text": "Now that we’re more familiar with QualCoder’s interface and the basics of our project, let’s walk Sarah through the coding analysis of the transcripts using the software’s features and some best practices. QualCoder offers a versatile platform for coding not just text, but also images (including PDFs), video, and audio. While Sarah’s project focuses on textual data, feel free to explore these features for your own work later on!\nTo make the most of our time, we’ll focus our coding examples and activities on the interviewees’ responses to Question 4:\nWhat do you see as the key ethical responsibilities of content creators and digital influencers? How should these ethics apply to endorsements and recommendations? Can you share any personal examples?\nWe’ll be using a combination of top-down and bottom-up approaches to demonstrate some important coding features in QualCoder. For now, don’t worry about the various icons on the top bar—we’ll explore those shortly. Instead, let’s concentrate on the lower right-hand window.\n\n\nAfter we presented the distinction between inductive and deductive coding approaches, Sarah mentioned that her preliminary literature review on the topic revealed some broad topics that could guide her initial coding framework.\nShe found that accountability, authenticity and transparency are ethical responsibilities frequently associated with the the work of content creators and social media influencers. She’s eager to explore these broad categories while keeping them flexible, allowing for refinement, including branching and or combination of existing codes, and the incorporation of new codes as they emerge. Therefore, we will first create these initial three categories to get her started with some coding using the tool.\nIt’s important to keep in mind that this process typically requires multiple iterations, which we might not be able to afford in a short episode. While we want to demonstrate the overall process to Sarah, she will likely need to keep adjusting the codes until she comes up with a more solid list of unambiguous ones.\nSo, we will start by representing these three main categories in QualCoder. To do that, let’s do a ctrl+click on the panel. This will prompt a window with the most important actions for you to engage with the development and organization of your codebook.\nClick add a new category, and enter “Accountability”. Complete the same process for the other two: authenticity and transparency. You may also add a memo with some explanation about their meanings.\n\nWith QualCoder we can create as many levels we want (as long as they are not ambiguous) and also drag categories and codes to reorganize them. We’ll see more about it later.\nWe now have three initial categories to work with. Assuming we’re familiar with the transcripts, we’ll explore excerpts related to these categories and consider the possibility of developing new codes. It’s important to remember that while we can’t directly assign categories to excerpts, they serve as a useful guide for exploring the data. This approach will assist us in identifying relevant themes later on, as we’ve seen in previous episodes.\nOk, let’s open the Interviewee_01 transcripts. look at the answer to Q4, let’s read the first paragraph. In these few sentences we can identify a few relevant keywords: trust, reliability, respect, genuine/genuinely. There could be multiple ways to go about it, but let’s say we want to code this section as “Genuine Added-Value” and relate this to the “Authenticity” category.\n\nWe can then drag the new code and arrange it under Authenticity. You will notice that a color will be automatically assigned to the code. You may change colors, rename, add memos, move or edit codes using the menu options.\nNote that the code now shows that there is one excerpt associated with the code. If you continue assigning more excerpts to a giving code, they will be reflected in that number, but at the file level in this view. You may check all excerpts assigned to a code across files by selecting Show coded files.\n\nYour turn! Code the second paragraph in relation to the category “Transparency” and code it as “Honest Reviews”.\n\nAlright, we have two new codes assigned to this interview, let’s move to other interviews and see if we can find excerpts related to this existing codes. I trust you have found some right away. After assigning honest reviews to another interview excerpt, let’s check the list of coded files.\n\n\n\n\nGreat! Now that we understand how to create categories, assign and organize codes, and view excerpts related to those codes, let’s move our attention now to the top bar we mentioned earlier to understand other coding options we have to assist our coding tasks.\n\nYou may use the search box to perform queries using Regex functions and locate specific terms within and across multiple documents. This can be extremely handy to speed up the process of getting to specific passages of the data. Once you perform a search you, unfortunately you won’t see the total of matching results. QualCoder will highlight those and you may use the back and forward arrows to navigate results.\nAnother useful feature is the ability to create file annotations for selected excerpts and coded sections, all conveniently displayed in one window. This allows you to highlight points that need clarification and passages you may want to revisit, especially if they have the potential to inspire new codes. You can use this tool to streamline your workflow and enhance your analysis. Note that the passage will be highlighted in bold so you can more easily spot what needs your attention. Similar approach can be followed with already coded passages, but typically those memos will reflect decisions behind the coding of that passage.\n\nAlright, here are a few more features to support Sarah’s coding efforts. It’s clear that this approach to coding is much more effective than using post-its or word processors, wouldn’t you agree?\n\n\nQualCoder also offers auto-coding text features represented in the icons below:\n\nYou can from left to right:\n• auto-code exact text\n• auto-code sentences based on a text fragment in the current file\n• auto-code sentences based on a text fragment for all files\n• auto-code the current file using start and end text marks. You can use ‘\\n’ for a line ending character\nNote that automatic coding is case-sensitive and requires a defined end for each sentence, with a default setting being a period followed by a space.\nTo auto-code exact text matches, you can assign multiple text sections using the pipe ‘|’ symbol. For instance, both “politics” and “politicians” can be assigned to the same code simultaneously (only for exact text matches).\nAdditionally, there is an undo option to revert recent auto-coding actions; however, this option will be lost if the project is closed and then reopened.\nThe auto-code exact text button includes a right-click menu with additional options. By default, it auto-codes all text, but you can also choose to auto-code only the first instance or specific matching text. When using the auto-code exact text feature, you can select which files to apply it to, making it a useful option if you want to identify a single instance across multiple files.\nFor example, let’s say Sarah Identified another code under the “Authenticity” category named “Genuine Engagement” to represent content/creators connections with their community of followers. In this case, after we create the code and select it, by choosing the first wand, we can auto-code all mentions of “genuine connection” as “Genuine Engagement”.\n\nWe can select as many files we would like to apply the auto-code to the chosen piece of text.\n\nNote that the text is highlighted and coded accordingly.\n\n\n\n\n\n\n\nNote\n\n\n\nFirst, it may lead to a loss of context and limit the scope of analysis, potentially overlooking important themes. Second, the subjective nature of code selection can result in inconsistencies, and complex ideas may become oversimplified. Third, this approach can complicate comparisons across different data sets, since it is unlikely terms and expressions derived from various sources will match.\n\n\n\n\n\n\n\n\nAuto-coding Limitations\n\n\n\n\n\nAuto-coding features, which can be also referred as in vivo, verbatim coding, literal coding use exact phrases or terms from participants’ responses, enabling researchers to highlight key themes and concepts directly from the data. While this method can be effective for preliminary and first cycle coding, researchers should be aware of its potential drawbacks as the primary method. This approach can result in a loss of insight, as context may be overlooked. Additionally, auto-coding often oversimplifies the data, focusing on specific terms rather than capturing broader themes.\n\n\n\n\n\n\n\nYou may also edit text using the blue pencil (proceed with care as this are irreversible) and export your coded files with the list of codes in html, odt or txt. Caution in clicking the red X, that will erase all your coding efforts for that particular file.\n\n\n\nAfter reviewing the basics with Sarah, she gained confidence in exploring the data using the tool independently. She initiated a new project over the weekend, concentrating on Q4, where she experimented with the tool and developed categories and codes. When she returned, she had some preliminary coding ready, allowing us to guide her through the reporting and graphing features in QualCoder. Although she hasn’t finished the analysis yet, Sarah feels like she is in a good place to navigate the reporting options within the tool with us.\nBefore we proceed, please access and download the influencers_q4.qda in our shared drive, save it in your desktop and open it in QualCoder.\nQualCoder offers a few options for users to explore, query, visualize the data and export those reports and outputs. It can provide code counting summaries in a variety of formats, word clouds, plotting in bars and pie charts, heat and tree maps. It also supports intercoder reliability testing for datasets coded, independently, by multiple people. Let’s see show Sarah some of the options she might want to consider for her analysis.\nLet’s click reports to see options in QualCoder. We will start with coding reports:\n\nOnce we select coding reports, we will be presented with a few additional filtering options. Since Sarah was the solo coder, we will only have one default coder for this project. Let’s leave it like that for now, and we will return to it a little later. If we click {x}, we will open the attribute selection parameters, where we can refine our report to include only responses from male influencers, for example. However, beforehand, we need to select at least one code from our list. Let’s choose the codes under the “authenticity and transparency” category.\n\nOnce completed, click the play button to see the results. You may also export the results in your chosen format after naming the file.\nWe can create a matrix by selecting files and desired codes. Let’s select all files and all codes within the authenticity and transparency category. Select the top categories in the drop-down menu and click the play button. Great! We have a matrix, but if we check the transposed matrix, we will have a better view of the relevant excerpts within the selected category and across documents.\n\nThe `code summary` option allows the inspection of files and a detailed word summary for each code. Check it yourself!\nWell, there are more reports to experiment and not all of them will be relevant for this project. So let’s now focus on the visualization options within the tool.\nThe view graph option will allow you to create visual based on existing selected codes by clicking on the bule plus sign. You may move things around and apply additional customization, but we want to keep it simple. Assuming we are happy with it, we can save the graph by clicking the downward arrow, and adding a name and description to it.\n\nLet’s move on to the charts section, where you’ll encounter several options. We typically recommend researchers avoid pie charts, as they are not effective for communicating information with three or more slices; human eyes struggle to distinguish curves. Instead, let’s explore more effective alternatives.\n\nTake time to explore the options available. Which ones would make more sense for you visualize Sarah’s coding?\n\n\n\nQualCoder was designed for individual use, allowing one person to work on a project at a time. However, a second coder can access the same project on the same computer, or the project folder can be easily transferred to another computer. To add a new coder, navigate to the project settings, enter the coder’s name, and click “Apply.”\n\nAdd yourself as Sarah’s helper. To do this, return to the coding text feature and code one or two excerpts using the existing codes, ensuring that your name is listed as the coder.\nFor a quick refresher on the steps: go to the coding section, then select “text,” and choose one of the interviews or files. Once you’ve completed the coding, head back to the reporting tab. Select the coders and click the play button to view the results. This will generate a table similar to the one shown below:\n\nTo interpret the table, you can click the question mark icon for a detailed explanation of what each column represents:\n\nAgree %: Calculated across all text files as the (total dual coded plus the total uncoded)/total characters\nA and B %: Calculated as the total dual coded characters/total characters\nNot A Not B %: The characters not coded by either coder/total characters\nDisagree %: Is 100% minus the total agreement percent.\nAgree coded only %: Is the dual coded characters divided by the dual coded and single coded characters\nKappa: The statistic or coefficient also know as Cohen’s kappa used to measure inter-rater reliability as described by McHugh (2012). This coefficient essentially assesses the degree of agreement and how well two independent coders classified/coded the same content into thematic groups.\n\nYou might be wondering how this can be useful to you, right? Well, let’s imagine you plan to publish your study. Several journals and peer reviewers might require that information, specially for disciplines that are more traditionally quantitative. Indicating that an independent coder agreed with your classification helps to support reliability, consistency and credibility of coding decisions that guided data analysis. But what are the thresholds?\nWhile there are some variations in the literature, a typical rule of thumb is that:\n\nKappa = 0.00 should be taken as representing no agreement\n0.00 ± 0.20 as “slight” agreement\n0.21 ± 0.40 as “fair” agreement\n0.41 ± 0.60 as “moderate” agreement\n0.61 ± 0.80 as “substantial” agreement\n0.81 ± 0.99 as “almost perfect” agreement.\nA kappa coefficient of 1 represents perfect agreement.\n\n\n\n\nWhen conducting qualitative research, it’s crucial to maintain thorough documentation to ensure transparency and ethical compliance. Documentation should include both project and file level documentation. QualCoder allows you to export a few important files to support interpretation and reusability. Here’s what we should recommend Sarah to save:\n\nCodebook: the coding framework used, including definitions of codes and categories.\nAll relevant materials generated within the tool, such as memos, notes, network diagrams, and classifications.\n\n\n\n\n\n\nREFI-QDA Codebook\n\n\n\nAs seen, a codebook in qualitative or mixed-methods research is a structured list of defined keywords or phrases that represent key themes, concepts, or topics of interest to the researcher. These keywords or phrases are commonly referred to as “codes.”\nIn qualitative data analysis (QDA) software, codes can be linked to specific segments of data where they appear. While the codebook serves as a comprehensive listing of codes, many QDA programs also allow for the organization of these codes in various ways, depending on the features of the software used.\nThe REFI-QDA Standard enables interoperability between Qualitative Data Analysis Software (QDAS or CAQDAS) programmes. It ensures that a codebook developed in one of the compatible software programs can be exported and imported into any other listed programs. QualCoder is currently experimenting with this format, and allows .\nIt’s important to note that because each program has unique functionalities regarding codes and their organization, users may encounter some modifications when importing a codebook into a different software environment.\nCheck which QDA software are REFI-QDA compatible: https://www.qdasoftware.org/refi-qda-codebook\n\n\n\nYou may export your project and codebook using the REFI-QDA standard under the Export &gt; Project option.\n\nAttributes are also exportable in csv file, as indicated below:\n\nSave these files using a consistent naming convention and ensure they are included in your data archiving package along with other important files, which will be discussed in more detail in the next episode.\n\nRecommended/Cited Sources:\nMcHugh, M. L. Interrater reliability: the kappa statistic. Biochem Med, 2012;22(3):276-82. https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding & Documenting with QualCoder"
    ]
  },
  {
    "objectID": "10_qualcoder-coding.html#practicing-with-the-case-example",
    "href": "10_qualcoder-coding.html#practicing-with-the-case-example",
    "title": "Coding and Documenting with Qualcoder",
    "section": "",
    "text": "Now that we’re more familiar with QualCoder’s interface and the basics of our project, let’s walk Sarah through the coding analysis of the transcripts using the software’s features and some best practices. QualCoder offers a versatile platform for coding not just text, but also images (including PDFs), video, and audio. While Sarah’s project focuses on textual data, feel free to explore these features for your own work later on!\nTo make the most of our time, we’ll focus our coding examples and activities on the interviewees’ responses to Question 4:\nWhat do you see as the key ethical responsibilities of content creators and digital influencers? How should these ethics apply to endorsements and recommendations? Can you share any personal examples?\nWe’ll be using a combination of top-down and bottom-up approaches to demonstrate some important coding features in QualCoder. For now, don’t worry about the various icons on the top bar—we’ll explore those shortly. Instead, let’s concentrate on the lower right-hand window.\n\n\nAfter we presented the distinction between inductive and deductive coding approaches, Sarah mentioned that her preliminary literature review on the topic revealed some broad topics that could guide her initial coding framework.\nShe found that accountability, authenticity and transparency are ethical responsibilities frequently associated with the the work of content creators and social media influencers. She’s eager to explore these broad categories while keeping them flexible, allowing for refinement, including branching and or combination of existing codes, and the incorporation of new codes as they emerge. Therefore, we will first create these initial three categories to get her started with some coding using the tool.\nIt’s important to keep in mind that this process typically requires multiple iterations, which we might not be able to afford in a short episode. While we want to demonstrate the overall process to Sarah, she will likely need to keep adjusting the codes until she comes up with a more solid list of unambiguous ones.\nSo, we will start by representing these three main categories in QualCoder. To do that, let’s do a ctrl+click on the panel. This will prompt a window with the most important actions for you to engage with the development and organization of your codebook.\nClick add a new category, and enter “Accountability”. Complete the same process for the other two: authenticity and transparency. You may also add a memo with some explanation about their meanings.\n\nWith QualCoder we can create as many levels we want (as long as they are not ambiguous) and also drag categories and codes to reorganize them. We’ll see more about it later.\nWe now have three initial categories to work with. Assuming we’re familiar with the transcripts, we’ll explore excerpts related to these categories and consider the possibility of developing new codes. It’s important to remember that while we can’t directly assign categories to excerpts, they serve as a useful guide for exploring the data. This approach will assist us in identifying relevant themes later on, as we’ve seen in previous episodes.\nOk, let’s open the Interviewee_01 transcripts. look at the answer to Q4, let’s read the first paragraph. In these few sentences we can identify a few relevant keywords: trust, reliability, respect, genuine/genuinely. There could be multiple ways to go about it, but let’s say we want to code this section as “Genuine Added-Value” and relate this to the “Authenticity” category.\n\nWe can then drag the new code and arrange it under Authenticity. You will notice that a color will be automatically assigned to the code. You may change colors, rename, add memos, move or edit codes using the menu options.\nNote that the code now shows that there is one excerpt associated with the code. If you continue assigning more excerpts to a giving code, they will be reflected in that number, but at the file level in this view. You may check all excerpts assigned to a code across files by selecting Show coded files.\n\nYour turn! Code the second paragraph in relation to the category “Transparency” and code it as “Honest Reviews”.\n\nAlright, we have two new codes assigned to this interview, let’s move to other interviews and see if we can find excerpts related to this existing codes. I trust you have found some right away. After assigning honest reviews to another interview excerpt, let’s check the list of coded files.\n\n\n\n\nGreat! Now that we understand how to create categories, assign and organize codes, and view excerpts related to those codes, let’s move our attention now to the top bar we mentioned earlier to understand other coding options we have to assist our coding tasks.\n\nYou may use the search box to perform queries using Regex functions and locate specific terms within and across multiple documents. This can be extremely handy to speed up the process of getting to specific passages of the data. Once you perform a search you, unfortunately you won’t see the total of matching results. QualCoder will highlight those and you may use the back and forward arrows to navigate results.\nAnother useful feature is the ability to create file annotations for selected excerpts and coded sections, all conveniently displayed in one window. This allows you to highlight points that need clarification and passages you may want to revisit, especially if they have the potential to inspire new codes. You can use this tool to streamline your workflow and enhance your analysis. Note that the passage will be highlighted in bold so you can more easily spot what needs your attention. Similar approach can be followed with already coded passages, but typically those memos will reflect decisions behind the coding of that passage.\n\nAlright, here are a few more features to support Sarah’s coding efforts. It’s clear that this approach to coding is much more effective than using post-its or word processors, wouldn’t you agree?\n\n\nQualCoder also offers auto-coding text features represented in the icons below:\n\nYou can from left to right:\n• auto-code exact text\n• auto-code sentences based on a text fragment in the current file\n• auto-code sentences based on a text fragment for all files\n• auto-code the current file using start and end text marks. You can use ‘\\n’ for a line ending character\nNote that automatic coding is case-sensitive and requires a defined end for each sentence, with a default setting being a period followed by a space.\nTo auto-code exact text matches, you can assign multiple text sections using the pipe ‘|’ symbol. For instance, both “politics” and “politicians” can be assigned to the same code simultaneously (only for exact text matches).\nAdditionally, there is an undo option to revert recent auto-coding actions; however, this option will be lost if the project is closed and then reopened.\nThe auto-code exact text button includes a right-click menu with additional options. By default, it auto-codes all text, but you can also choose to auto-code only the first instance or specific matching text. When using the auto-code exact text feature, you can select which files to apply it to, making it a useful option if you want to identify a single instance across multiple files.\nFor example, let’s say Sarah Identified another code under the “Authenticity” category named “Genuine Engagement” to represent content/creators connections with their community of followers. In this case, after we create the code and select it, by choosing the first wand, we can auto-code all mentions of “genuine connection” as “Genuine Engagement”.\n\nWe can select as many files we would like to apply the auto-code to the chosen piece of text.\n\nNote that the text is highlighted and coded accordingly.\n\n\n\n\n\n\n\nNote\n\n\n\nFirst, it may lead to a loss of context and limit the scope of analysis, potentially overlooking important themes. Second, the subjective nature of code selection can result in inconsistencies, and complex ideas may become oversimplified. Third, this approach can complicate comparisons across different data sets, since it is unlikely terms and expressions derived from various sources will match.\n\n\n\n\n\n\n\n\nAuto-coding Limitations\n\n\n\n\n\nAuto-coding features, which can be also referred as in vivo, verbatim coding, literal coding use exact phrases or terms from participants’ responses, enabling researchers to highlight key themes and concepts directly from the data. While this method can be effective for preliminary and first cycle coding, researchers should be aware of its potential drawbacks as the primary method. This approach can result in a loss of insight, as context may be overlooked. Additionally, auto-coding often oversimplifies the data, focusing on specific terms rather than capturing broader themes.\n\n\n\n\n\n\n\nYou may also edit text using the blue pencil (proceed with care as this are irreversible) and export your coded files with the list of codes in html, odt or txt. Caution in clicking the red X, that will erase all your coding efforts for that particular file.\n\n\n\nAfter reviewing the basics with Sarah, she gained confidence in exploring the data using the tool independently. She initiated a new project over the weekend, concentrating on Q4, where she experimented with the tool and developed categories and codes. When she returned, she had some preliminary coding ready, allowing us to guide her through the reporting and graphing features in QualCoder. Although she hasn’t finished the analysis yet, Sarah feels like she is in a good place to navigate the reporting options within the tool with us.\nBefore we proceed, please access and download the influencers_q4.qda in our shared drive, save it in your desktop and open it in QualCoder.\nQualCoder offers a few options for users to explore, query, visualize the data and export those reports and outputs. It can provide code counting summaries in a variety of formats, word clouds, plotting in bars and pie charts, heat and tree maps. It also supports intercoder reliability testing for datasets coded, independently, by multiple people. Let’s see show Sarah some of the options she might want to consider for her analysis.\nLet’s click reports to see options in QualCoder. We will start with coding reports:\n\nOnce we select coding reports, we will be presented with a few additional filtering options. Since Sarah was the solo coder, we will only have one default coder for this project. Let’s leave it like that for now, and we will return to it a little later. If we click {x}, we will open the attribute selection parameters, where we can refine our report to include only responses from male influencers, for example. However, beforehand, we need to select at least one code from our list. Let’s choose the codes under the “authenticity and transparency” category.\n\nOnce completed, click the play button to see the results. You may also export the results in your chosen format after naming the file.\nWe can create a matrix by selecting files and desired codes. Let’s select all files and all codes within the authenticity and transparency category. Select the top categories in the drop-down menu and click the play button. Great! We have a matrix, but if we check the transposed matrix, we will have a better view of the relevant excerpts within the selected category and across documents.\n\nThe `code summary` option allows the inspection of files and a detailed word summary for each code. Check it yourself!\nWell, there are more reports to experiment and not all of them will be relevant for this project. So let’s now focus on the visualization options within the tool.\nThe view graph option will allow you to create visual based on existing selected codes by clicking on the bule plus sign. You may move things around and apply additional customization, but we want to keep it simple. Assuming we are happy with it, we can save the graph by clicking the downward arrow, and adding a name and description to it.\n\nLet’s move on to the charts section, where you’ll encounter several options. We typically recommend researchers avoid pie charts, as they are not effective for communicating information with three or more slices; human eyes struggle to distinguish curves. Instead, let’s explore more effective alternatives.\n\nTake time to explore the options available. Which ones would make more sense for you visualize Sarah’s coding?\n\n\n\nQualCoder was designed for individual use, allowing one person to work on a project at a time. However, a second coder can access the same project on the same computer, or the project folder can be easily transferred to another computer. To add a new coder, navigate to the project settings, enter the coder’s name, and click “Apply.”\n\nAdd yourself as Sarah’s helper. To do this, return to the coding text feature and code one or two excerpts using the existing codes, ensuring that your name is listed as the coder.\nFor a quick refresher on the steps: go to the coding section, then select “text,” and choose one of the interviews or files. Once you’ve completed the coding, head back to the reporting tab. Select the coders and click the play button to view the results. This will generate a table similar to the one shown below:\n\nTo interpret the table, you can click the question mark icon for a detailed explanation of what each column represents:\n\nAgree %: Calculated across all text files as the (total dual coded plus the total uncoded)/total characters\nA and B %: Calculated as the total dual coded characters/total characters\nNot A Not B %: The characters not coded by either coder/total characters\nDisagree %: Is 100% minus the total agreement percent.\nAgree coded only %: Is the dual coded characters divided by the dual coded and single coded characters\nKappa: The statistic or coefficient also know as Cohen’s kappa used to measure inter-rater reliability as described by McHugh (2012). This coefficient essentially assesses the degree of agreement and how well two independent coders classified/coded the same content into thematic groups.\n\nYou might be wondering how this can be useful to you, right? Well, let’s imagine you plan to publish your study. Several journals and peer reviewers might require that information, specially for disciplines that are more traditionally quantitative. Indicating that an independent coder agreed with your classification helps to support reliability, consistency and credibility of coding decisions that guided data analysis. But what are the thresholds?\nWhile there are some variations in the literature, a typical rule of thumb is that:\n\nKappa = 0.00 should be taken as representing no agreement\n0.00 ± 0.20 as “slight” agreement\n0.21 ± 0.40 as “fair” agreement\n0.41 ± 0.60 as “moderate” agreement\n0.61 ± 0.80 as “substantial” agreement\n0.81 ± 0.99 as “almost perfect” agreement.\nA kappa coefficient of 1 represents perfect agreement.\n\n\n\n\nWhen conducting qualitative research, it’s crucial to maintain thorough documentation to ensure transparency and ethical compliance. Documentation should include both project and file level documentation. QualCoder allows you to export a few important files to support interpretation and reusability. Here’s what we should recommend Sarah to save:\n\nCodebook: the coding framework used, including definitions of codes and categories.\nAll relevant materials generated within the tool, such as memos, notes, network diagrams, and classifications.\n\n\n\n\n\n\nREFI-QDA Codebook\n\n\n\nAs seen, a codebook in qualitative or mixed-methods research is a structured list of defined keywords or phrases that represent key themes, concepts, or topics of interest to the researcher. These keywords or phrases are commonly referred to as “codes.”\nIn qualitative data analysis (QDA) software, codes can be linked to specific segments of data where they appear. While the codebook serves as a comprehensive listing of codes, many QDA programs also allow for the organization of these codes in various ways, depending on the features of the software used.\nThe REFI-QDA Standard enables interoperability between Qualitative Data Analysis Software (QDAS or CAQDAS) programmes. It ensures that a codebook developed in one of the compatible software programs can be exported and imported into any other listed programs. QualCoder is currently experimenting with this format, and allows .\nIt’s important to note that because each program has unique functionalities regarding codes and their organization, users may encounter some modifications when importing a codebook into a different software environment.\nCheck which QDA software are REFI-QDA compatible: https://www.qdasoftware.org/refi-qda-codebook\n\n\n\nYou may export your project and codebook using the REFI-QDA standard under the Export &gt; Project option.\n\nAttributes are also exportable in csv file, as indicated below:\n\nSave these files using a consistent naming convention and ensure they are included in your data archiving package along with other important files, which will be discussed in more detail in the next episode.\n\nRecommended/Cited Sources:\nMcHugh, M. L. Interrater reliability: the kappa statistic. Biochem Med, 2012;22(3):276-82. https://pmc.ncbi.nlm.nih.gov/articles/PMC3900052",
    "crumbs": [
      "Analyzing & Documenting",
      "Coding & Documenting with QualCoder"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Handling and Sharing Qualitative Data Responsibly and Effectively",
    "section": "",
    "text": "Qualitative data is vital in social sciences research as it delves deep into human behavior, attitudes, and interactions, offering a comprehensive perspective beyond what quantitative data can provide. It can offer valuable insights into the intricacies of social relationships and as means to develop hypotheses, theories, and models, guiding further inquiry and contributing to knowledge advancement. Often times qualitative data derives from researchers recruitment and engagement with human subjects posing additional challenges and considerations for proper data management.\nThis website aims to assist researchers to better and ethically manage qualitative data from studies involving human subjects throughout its life cycle, since planning to sharing and reusing. Episodes were developed in such way to help researchers:\n\nUnderstand the nuances and challenges of working with qualitative data;\nLearn about the ethical aspects around working with human subjects data and how to stay compliant with IRB regulations;\nCraft better consent forms to allow for data sharing and reuse;\nSelect and apply data de-identification techniques;\nGet started with tools to manage and analyze qualitative data;\nIdentify best approaches to share and receive credit for qualitative data.\n\nThis course is grounded in the research data life cycle depicted in the graphical handout below which focuses on approaches for researchers to handle human subject qualitative data more effectively & responsibly. We will use this framework to explore recommended data management practices applied to human subjects qualitative data throughout the following steps: 1) planning and collecting, 2) analyzing and documenting, 3) sharing/archiving, and 4) reusing.\n\n\nSource: UCSB Library Data Literacy Series. Click to expand: (perma.cc/D95L-54W8).\nResearchers and students are welcome to take advantage of this learning resource asynchronously at their own pace. We also encourage faculty to reuse or repurpose content to their research methods classes. If you’d like to us to host a hands-on in-person or remote workshop for your team, reach out to RDS for scheduling: rds@library.ucsb.edu.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Handling and Sharing Qualitative Data Responsibly and Effectively",
    "section": "",
    "text": "Qualitative data is vital in social sciences research as it delves deep into human behavior, attitudes, and interactions, offering a comprehensive perspective beyond what quantitative data can provide. It can offer valuable insights into the intricacies of social relationships and as means to develop hypotheses, theories, and models, guiding further inquiry and contributing to knowledge advancement. Often times qualitative data derives from researchers recruitment and engagement with human subjects posing additional challenges and considerations for proper data management.\nThis website aims to assist researchers to better and ethically manage qualitative data from studies involving human subjects throughout its life cycle, since planning to sharing and reusing. Episodes were developed in such way to help researchers:\n\nUnderstand the nuances and challenges of working with qualitative data;\nLearn about the ethical aspects around working with human subjects data and how to stay compliant with IRB regulations;\nCraft better consent forms to allow for data sharing and reuse;\nSelect and apply data de-identification techniques;\nGet started with tools to manage and analyze qualitative data;\nIdentify best approaches to share and receive credit for qualitative data.\n\nThis course is grounded in the research data life cycle depicted in the graphical handout below which focuses on approaches for researchers to handle human subject qualitative data more effectively & responsibly. We will use this framework to explore recommended data management practices applied to human subjects qualitative data throughout the following steps: 1) planning and collecting, 2) analyzing and documenting, 3) sharing/archiving, and 4) reusing.\n\n\nSource: UCSB Library Data Literacy Series. Click to expand: (perma.cc/D95L-54W8).\nResearchers and students are welcome to take advantage of this learning resource asynchronously at their own pace. We also encourage faculty to reuse or repurpose content to their research methods classes. If you’d like to us to host a hands-on in-person or remote workshop for your team, reach out to RDS for scheduling: rds@library.ucsb.edu.",
    "crumbs": [
      "Home"
    ]
  }
]